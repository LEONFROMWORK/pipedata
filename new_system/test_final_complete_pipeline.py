#!/usr/bin/env python3
"""
ì™„ì „í•œ Stack Overflow + Reddit 403 ìš°íšŒ íŒŒì´í”„ë¼ì¸ ìµœì¢… í…ŒìŠ¤íŠ¸
ì‹¤ì œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° AI ì²˜ë¦¬ê¹Œì§€ í¬í•¨í•œ ì™„ì „í•œ ë°ì´í„°ì…‹ ìƒì„±
"""
import asyncio
import json
import logging
import re
import sys
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any

# Add project root to path
sys.path.append(str(Path(__file__).parent))

from collectors.stackoverflow_collector import StackOverflowCollector
from collectors.reddit_collector import RedditCollector
from processors.image_processor import ImageProcessor
from core.cache import APICache, LocalCache

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

def extract_images_from_html(html_content: str) -> List[str]:
    """HTML ì»¨í…ì¸ ì—ì„œ ì´ë¯¸ì§€ URL ì¶”ì¶œ"""
    image_urls = []
    
    # <img src="..."> íŒ¨í„´
    img_pattern = r'<img[^>]+src=["\']([^"\']+)["\'][^>]*>'
    img_matches = re.findall(img_pattern, html_content, re.IGNORECASE)
    image_urls.extend(img_matches)
    
    # <a href="..."> íŒ¨í„´ (ì´ë¯¸ì§€ ë§í¬)
    link_pattern = r'<a[^>]+href=["\']([^"\']+\.(?:png|jpg|jpeg|gif|webp|svg)(?:\?[^"\']*)?)["\'][^>]*>'
    link_matches = re.findall(link_pattern, html_content, re.IGNORECASE)
    image_urls.extend(link_matches)
    
    # ë§ˆí¬ë‹¤ìš´ ìŠ¤íƒ€ì¼ ì´ë¯¸ì§€ ë§í¬ ![alt](url)
    markdown_pattern = r'!\[.*?\]\(([^)]+)\)'
    markdown_matches = re.findall(markdown_pattern, html_content)
    image_urls.extend(markdown_matches)
    
    # ì§ì ‘ URL íŒ¨í„´ (http://...image.png)
    direct_pattern = r'https?://[^\s<>"\']+\.(?:png|jpg|jpeg|gif|webp|svg)(?:\?[^\s<>"\']*)?'
    direct_matches = re.findall(direct_pattern, html_content, re.IGNORECASE)
    image_urls.extend(direct_matches)
    
    # ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
    unique_urls = list(set(image_urls))
    
    # ë¹ˆ URLì´ë‚˜ í”„ë¡œí•„ ì´ë¯¸ì§€ ì œì™¸
    filtered_urls = []
    for url in unique_urls:
        if url and 'gravatar.com' not in url and len(url.strip()) > 10:
            # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
            if url.startswith('//'):
                url = 'https:' + url
            elif url.startswith('/'):
                url = 'https://i.sstatic.net' + url
                
            filtered_urls.append(url.strip())
    
    return filtered_urls

def extract_images_from_reddit_data(reddit_data: List) -> List[str]:
    """Reddit ë°ì´í„°ì—ì„œ ì´ë¯¸ì§€ URL ì¶”ì¶œ"""
    all_images = []
    
    for item in reddit_data:
        # submissionì˜ ì´ë¯¸ì§€ë“¤
        if hasattr(item, 'submission'):
            submission = item.submission
            
            # selftextì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ
            if isinstance(submission, dict) and 'selftext' in submission:
                images = extract_images_from_html(submission['selftext'])
                all_images.extend(images)
            
            # solutionì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ
            if hasattr(item, 'solution') and isinstance(item.solution, dict):
                solution_body = item.solution.get('body', '')
                images = extract_images_from_html(solution_body)
                all_images.extend(images)
    
    return list(set(all_images))  # ì¤‘ë³µ ì œê±°

async def test_complete_pipeline():
    """ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    
    logger.info("ğŸš€ Complete Pipeline Test - Stack Overflow + Reddit + 403 Bypass")
    logger.info("=" * 80)
    
    # Cache ì´ˆê¸°í™”
    local_cache = LocalCache(db_path=Path("/tmp/complete_pipeline_test.db"))
    cache = APICache(local_cache)
    
    final_dataset = {
        "dataset_info": {
            "name": "Excel Q&A Dataset with 403 Bypass Complete",
            "version": "4.0-complete-pipeline",
            "description": "Complete Excel Q&A dataset with Stack Overflow + Reddit + 403 bypass + AI processing",
            "generated_at": datetime.now().isoformat()
        },
        "collection_results": {
            "stackoverflow": {"data": [], "images": []},
            "reddit": {"data": [], "images": []}
        },
        "image_processing": {
            "total_images": 0,
            "successful_downloads": 0,
            "successful_processing": 0,
            "results": []
        },
        "bypass_effectiveness": {
            "stackoverflow": {"tested": 0, "successful": 0, "rate": 0},
            "reddit": {"tested": 0, "successful": 0, "rate": 0},
            "overall": {"tested": 0, "successful": 0, "rate": 0}
        }
    }
    
    # Phase 1: Stack Overflow ìˆ˜ì§‘
    logger.info("\nğŸ“š Phase 1: Stack Overflow Collection")
    logger.info("-" * 60)
    
    try:
        so_collector = StackOverflowCollector(cache)
        so_data = await so_collector.collect_excel_questions(max_pages=2)  # ë” ë§ì€ ë°ì´í„°
        
        final_dataset["collection_results"]["stackoverflow"]["data"] = so_data
        logger.info(f"âœ… Stack Overflow: {len(so_data)}ê°œ ìˆ˜ì§‘")
        
        # ì´ë¯¸ì§€ ì¶”ì¶œ
        for item in so_data:
            question_body = item.get('body_markdown', '') + ' ' + item.get('body', '')
            question_images = extract_images_from_html(question_body)
            
            if 'accepted_answer' in item:
                answer_body = item['accepted_answer'].get('body', '')
                answer_images = extract_images_from_html(answer_body)
                question_images.extend(answer_images)
            
            final_dataset["collection_results"]["stackoverflow"]["images"].extend(question_images)
        
        # ì¤‘ë³µ ì œê±°
        final_dataset["collection_results"]["stackoverflow"]["images"] = list(set(
            final_dataset["collection_results"]["stackoverflow"]["images"]
        ))
        
        logger.info(f"   ğŸ“¸ Stack Overflow ì´ë¯¸ì§€: {len(final_dataset['collection_results']['stackoverflow']['images'])}ê°œ")
        
    except Exception as e:
        logger.error(f"âŒ Stack Overflow ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
    
    # Phase 2: Reddit ìˆ˜ì§‘
    logger.info("\nğŸŸ  Phase 2: Reddit Collection")
    logger.info("-" * 60)
    
    try:
        reddit_collector = RedditCollector(cache)
        reddit_data = await reddit_collector.collect_excel_discussions(max_submissions=10)  # ë” ë§ì€ ë°ì´í„°
        
        final_dataset["collection_results"]["reddit"]["data"] = [
            {
                "submission": item.submission,
                "solution": item.solution,
                "metadata": item.metadata
            } for item in reddit_data
        ]
        
        logger.info(f"âœ… Reddit: {len(reddit_data)}ê°œ ìˆ˜ì§‘")
        
        # Reddit ì´ë¯¸ì§€ ì¶”ì¶œ
        reddit_images = extract_images_from_reddit_data(reddit_data)
        final_dataset["collection_results"]["reddit"]["images"] = reddit_images
        
        logger.info(f"   ğŸ“¸ Reddit ì´ë¯¸ì§€: {len(reddit_images)}ê°œ")
        
    except Exception as e:
        logger.error(f"âŒ Reddit ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
    
    # Phase 3: ì´ë¯¸ì§€ 403 ìš°íšŒ ë° AI ì²˜ë¦¬
    logger.info("\nğŸ–¼ï¸  Phase 3: Image Processing with 403 Bypass")
    logger.info("-" * 60)
    
    all_images = (final_dataset["collection_results"]["stackoverflow"]["images"] + 
                 final_dataset["collection_results"]["reddit"]["images"])
    
    # í…ŒìŠ¤íŠ¸ìš© Reddit ì´ë¯¸ì§€ ì¶”ê°€
    test_reddit_images = [
        'https://preview.redd.it/76mukstfxhdf1.png'
    ]
    all_images.extend(test_reddit_images)
    
    # ì¤‘ë³µ ì œê±°
    unique_images = list(set(all_images))
    
    final_dataset["image_processing"]["total_images"] = len(unique_images)
    
    logger.info(f"   ğŸ¯ ì´ ì²˜ë¦¬í•  ì´ë¯¸ì§€: {len(unique_images)}ê°œ")
    
    if unique_images:
        processor = ImageProcessor(cache)
        
        for i, img_url in enumerate(unique_images, 1):
            logger.info(f"   [{i}/{len(unique_images)}] ì²˜ë¦¬ ì¤‘: {img_url[:60]}...")
            
            try:
                # 403 ìš°íšŒ + AI ì²˜ë¦¬
                result = await processor.process_image_url(img_url, ['excel'])
                
                # ì†ŒìŠ¤ íŒë³„
                is_reddit = 'redd.it' in img_url or 'reddit' in img_url
                is_stackoverflow = 'sstatic.net' in img_url
                source = 'reddit' if is_reddit else 'stackoverflow'
                
                if result and result.get('success'):
                    final_dataset["image_processing"]["successful_downloads"] += 1
                    
                    if result.get('extracted_content'):
                        final_dataset["image_processing"]["successful_processing"] += 1
                    
                    final_dataset["image_processing"]["results"].append({
                        'url': img_url,
                        'source': source,
                        'success': True,
                        'processing_tier': result.get('processing_tier', ''),
                        'content_length': len(result.get('extracted_content', '')),
                        'extracted_content': result.get('extracted_content', '')[:200] + '...' if len(result.get('extracted_content', '')) > 200 else result.get('extracted_content', '')
                    })
                    
                    logger.info(f"      âœ… ì„±ê³µ! {result.get('processing_tier', 'Unknown')}")
                else:
                    final_dataset["image_processing"]["results"].append({
                        'url': img_url,
                        'source': source,
                        'success': False,
                        'error': result.get('error', 'Unknown') if result else 'No result'
                    })
                    logger.error(f"      âŒ ì‹¤íŒ¨")
                
                # ì†ŒìŠ¤ë³„ í†µê³„ ì—…ë°ì´íŠ¸
                if is_stackoverflow:
                    final_dataset["bypass_effectiveness"]["stackoverflow"]["tested"] += 1
                    if result and result.get('success'):
                        final_dataset["bypass_effectiveness"]["stackoverflow"]["successful"] += 1
                elif is_reddit:
                    final_dataset["bypass_effectiveness"]["reddit"]["tested"] += 1
                    if result and result.get('success'):
                        final_dataset["bypass_effectiveness"]["reddit"]["successful"] += 1
                
            except Exception as e:
                logger.error(f"      âŒ ì˜ˆì™¸: {e}")
                final_dataset["image_processing"]["results"].append({
                    'url': img_url,
                    'source': source,
                    'success': False,
                    'error': str(e)
                })
    
    # Phase 4: ìµœì¢… í†µê³„ ê³„ì‚°
    logger.info("\nğŸ“Š Phase 4: Final Statistics")
    logger.info("-" * 60)
    
    # ì†ŒìŠ¤ë³„ ì„±ê³µë¥  ê³„ì‚°
    so_stats = final_dataset["bypass_effectiveness"]["stackoverflow"]
    if so_stats["tested"] > 0:
        so_stats["rate"] = (so_stats["successful"] / so_stats["tested"]) * 100
    
    reddit_stats = final_dataset["bypass_effectiveness"]["reddit"]
    if reddit_stats["tested"] > 0:
        reddit_stats["rate"] = (reddit_stats["successful"] / reddit_stats["tested"]) * 100
    
    # ì „ì²´ ì„±ê³µë¥ 
    total_tested = so_stats["tested"] + reddit_stats["tested"]
    total_successful = so_stats["successful"] + reddit_stats["successful"]
    
    final_dataset["bypass_effectiveness"]["overall"] = {
        "tested": total_tested,
        "successful": total_successful,
        "rate": (total_successful / total_tested * 100) if total_tested > 0 else 0
    }
    
    # ìµœì¢… ì—…ë°ì´íŠ¸
    final_dataset["dataset_info"].update({
        "total_qa_pairs": len(final_dataset["collection_results"]["stackoverflow"]["data"]) + len(final_dataset["collection_results"]["reddit"]["data"]),
        "total_images_found": final_dataset["image_processing"]["total_images"],
        "image_processing_success_rate": (final_dataset["image_processing"]["successful_processing"] / final_dataset["image_processing"]["total_images"] * 100) if final_dataset["image_processing"]["total_images"] > 0 else 0
    })
    
    # ê²°ê³¼ ì €ì¥
    output_path = "/Users/kevin/bigdata/data/output/complete_pipeline_dataset.json"
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(final_dataset, f, indent=2, ensure_ascii=False)
    
    # ìµœì¢… ë³´ê³ ì„œ
    logger.info("\n" + "ğŸ‰" * 30)
    logger.info("ğŸ“‹ COMPLETE PIPELINE RESULTS")
    logger.info("=" * 80)
    logger.info(f"ğŸ’ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: {output_path}")
    logger.info("")
    logger.info("ğŸ“Š ìˆ˜ì§‘ í†µê³„:")
    logger.info(f"   â€¢ ì´ Q&A ìŒ: {final_dataset['dataset_info']['total_qa_pairs']}ê°œ")
    logger.info(f"   â€¢ Stack Overflow: {len(final_dataset['collection_results']['stackoverflow']['data'])}ê°œ")
    logger.info(f"   â€¢ Reddit: {len(final_dataset['collection_results']['reddit']['data'])}ê°œ")
    logger.info("")
    logger.info("ğŸ–¼ï¸  ì´ë¯¸ì§€ ì²˜ë¦¬ í†µê³„:")
    logger.info(f"   â€¢ ì´ ì´ë¯¸ì§€ ë°œê²¬: {final_dataset['image_processing']['total_images']}ê°œ")
    logger.info(f"   â€¢ ë‹¤ìš´ë¡œë“œ ì„±ê³µ: {final_dataset['image_processing']['successful_downloads']}ê°œ")
    logger.info(f"   â€¢ AI ì²˜ë¦¬ ì„±ê³µ: {final_dataset['image_processing']['successful_processing']}ê°œ")
    logger.info(f"   â€¢ ì „ì²´ ì²˜ë¦¬ ì„±ê³µë¥ : {final_dataset['dataset_info']['image_processing_success_rate']:.1f}%")
    logger.info("")
    logger.info("ğŸ¯ 403 ìš°íšŒ ì„±ê³¼:")
    logger.info(f"   â€¢ ì „ì²´: {final_dataset['bypass_effectiveness']['overall']['rate']:.1f}% ({final_dataset['bypass_effectiveness']['overall']['successful']}/{final_dataset['bypass_effectiveness']['overall']['tested']})")
    logger.info(f"   â€¢ Stack Overflow: {final_dataset['bypass_effectiveness']['stackoverflow']['rate']:.1f}% ({final_dataset['bypass_effectiveness']['stackoverflow']['successful']}/{final_dataset['bypass_effectiveness']['stackoverflow']['tested']})")
    logger.info(f"   â€¢ Reddit: {final_dataset['bypass_effectiveness']['reddit']['rate']:.1f}% ({final_dataset['bypass_effectiveness']['reddit']['successful']}/{final_dataset['bypass_effectiveness']['reddit']['tested']})")
    
    if final_dataset['bypass_effectiveness']['overall']['rate'] >= 80:
        logger.info("ğŸ† 403 ìš°íšŒ ëª©í‘œ ë‹¬ì„±!")
    elif final_dataset['bypass_effectiveness']['overall']['rate'] >= 60:
        logger.info("âœ… 403 ìš°íšŒ ì–‘í˜¸í•œ ì„±ê³¼!")
    else:
        logger.info("âš ï¸  403 ìš°íšŒ ì¶”ê°€ ê°œì„  ê¶Œì¥")
    
    return final_dataset

if __name__ == "__main__":
    result = asyncio.run(test_complete_pipeline())