#!/usr/bin/env python3
"""
ëŒ€ê·œëª¨ Stack Overflow ë°ì´í„° ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸
- ìºì‹œ ì´ˆê¸°í™”
- ë” ë„“ì€ ê¸°ê°„ ì„¤ì •
- ë” ë§ì€ í˜ì´ì§€ ìˆ˜ì§‘
- ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§
"""
import asyncio
import json
import sqlite3
import sys
from pathlib import Path
from datetime import datetime, timedelta

sys.path.insert(0, str(Path(__file__).parent))

from config import Config
from core.cache import LocalCache, APICache
from collectors.fixed_stackoverflow_collector import FixedStackOverflowCollector

def clear_stackoverflow_cache():
    """Stack Overflow ê´€ë ¨ ìºì‹œ ëª¨ë‘ ì‚­ì œ"""
    print("ğŸ—‘ï¸ Stack Overflow ìºì‹œ ì´ˆê¸°í™”")
    
    try:
        with sqlite3.connect(Config.DATABASE_PATH) as conn:
            # ê¸°ì¡´ Stack Overflow ìºì‹œ ì‚­ì œ
            cursor = conn.execute("DELETE FROM cache WHERE key LIKE 'so_api:%' OR key LIKE 'fixed_%'")
            deleted_count = cursor.rowcount
            conn.commit()
            
            print(f"   ì‚­ì œëœ ìºì‹œ í•­ëª©: {deleted_count}ê°œ")
            
            # ì¤‘ë³µ ì¶”ì ê¸°ë„ ì´ˆê¸°í™”
            dedup_db = Path("/Users/kevin/bigdata/data/deduplication_tracker.db")
            if dedup_db.exists():
                with sqlite3.connect(dedup_db) as dedup_conn:
                    dedup_conn.execute("DELETE FROM stackoverflow_questions")
                    dedup_conn.commit()
                    print("   ì¤‘ë³µ ì¶”ì ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
                    
    except Exception as e:
        print(f"   ìºì‹œ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")

async def large_scale_collection():
    """ëŒ€ê·œëª¨ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸"""
    print("ğŸ“ˆ ëŒ€ê·œëª¨ Stack Overflow ë°ì´í„° ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    try:
        # ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
        local_cache = LocalCache(Config.DATABASE_PATH)
        api_cache = APICache(local_cache)
        collector = FixedStackOverflowCollector(api_cache)
        
        print("âœ… ìˆ˜ì§‘ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        
        # ë” ë„“ì€ ê¸°ê°„ ì„¤ì • (3ê°œì›”)
        from_date = datetime.now() - timedelta(days=90)
        print(f"ğŸ“… ìˆ˜ì§‘ ê¸°ê°„: {from_date.strftime('%Y-%m-%d')} ~ í˜„ì¬ (3ê°œì›”)")
        
        # ë” ë§ì€ í˜ì´ì§€ ìˆ˜ì§‘
        max_pages = 5
        print(f"ğŸ“„ ìˆ˜ì§‘ í˜ì´ì§€: ìµœëŒ€ {max_pages}í˜ì´ì§€")
        
        # ìˆ˜ì§‘ ì‹œì‘
        print("\nğŸš€ ìˆ˜ì§‘ ì‹œì‘...")
        start_time = datetime.now()
        
        qa_pairs = await collector.collect_excel_questions_fixed(
            from_date=from_date,
            max_pages=max_pages
        )
        
        end_time = datetime.now()
        duration = end_time - start_time
        
        print(f"\nâ±ï¸ ìˆ˜ì§‘ ì™„ë£Œ (ì†Œìš” ì‹œê°„: {duration.total_seconds():.1f}ì´ˆ)")
        print(f"ğŸ“Š ì´ ìˆ˜ì§‘ ê²°ê³¼: {len(qa_pairs)}ê°œ Q&A ìŒ")
        
        if not qa_pairs:
            print("âŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            await collector.close()
            return
        
        # ìƒì„¸ ë¶„ì„
        print(f"\nğŸ” ìƒì„¸ ë°ì´í„° ë¶„ì„:")
        
        # ì™„ì„±ë„ ë¶„ì„
        complete_pairs = sum(1 for pair in qa_pairs if pair.get('answer'))
        print(f"   ì™„ì „í•œ Q&A ìŒ: {complete_pairs}/{len(qa_pairs)} ({complete_pairs/len(qa_pairs)*100:.1f}%)")
        
        # ì ìˆ˜ ë¶„ì„
        question_scores = [pair['question'].get('score', 0) for pair in qa_pairs]
        answer_scores = [pair['answer'].get('score', 0) for pair in qa_pairs if pair.get('answer')]
        quality_scores = [pair.get('quality_score', 0) for pair in qa_pairs]
        
        print(f"   ì§ˆë¬¸ ì ìˆ˜ ë²”ìœ„: {min(question_scores)} ~ {max(question_scores)} (í‰ê· : {sum(question_scores)/len(question_scores):.1f})")
        if answer_scores:
            print(f"   ë‹µë³€ ì ìˆ˜ ë²”ìœ„: {min(answer_scores)} ~ {max(answer_scores)} (í‰ê· : {sum(answer_scores)/len(answer_scores):.1f})")
        print(f"   í’ˆì§ˆ ì ìˆ˜ ë²”ìœ„: {min(quality_scores)} ~ {max(quality_scores)} (í‰ê· : {sum(quality_scores)/len(quality_scores):.1f})")
        
        # íƒœê·¸ ë¶„ì„
        all_tags = []
        for pair in qa_pairs:
            all_tags.extend(pair['question'].get('tags', []))
        
        tag_counts = {}
        for tag in all_tags:
            tag_counts[tag] = tag_counts.get(tag, 0) + 1
        
        print(f"\nğŸ·ï¸ íƒœê·¸ ë¶„í¬ (ìƒìœ„ 10ê°œ):")
        for tag, count in sorted(tag_counts.items(), key=lambda x: x[1], reverse=True)[:10]:
            print(f"   {tag}: {count}íšŒ")
        
        # Excel í•¨ìˆ˜ ë¶„ì„
        print(f"\nğŸ”§ Excel í•¨ìˆ˜ ì–¸ê¸‰ ë¶„ì„:")
        excel_functions = ['VLOOKUP', 'INDEX', 'MATCH', 'SUMIF', 'COUNTIF', 'IF', 'PIVOT', 'XLOOKUP', 'LAMBDA']
        function_counts = {func: 0 for func in excel_functions}
        
        for pair in qa_pairs:
            question = pair['question']
            answer = pair.get('answer', {})
            
            # í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°
            q_text = (question.get('title', '') + ' ' + question.get('body_markdown', '')).upper()
            a_text = answer.get('body_markdown', answer.get('body', '')).upper()
            full_text = q_text + ' ' + a_text
            
            for func in excel_functions:
                if func in full_text:
                    function_counts[func] += 1
        
        for func, count in sorted(function_counts.items(), key=lambda x: x[1], reverse=True):
            if count > 0:
                print(f"   {func}: {count}íšŒ")
        
        # ê³ í’ˆì§ˆ Q&A ì„ ë³„ (í’ˆì§ˆ ì ìˆ˜ ìƒìœ„)
        high_quality = sorted(qa_pairs, key=lambda x: x.get('quality_score', 0), reverse=True)[:5]
        
        print(f"\nâ­ ê³ í’ˆì§ˆ Q&A (ìƒìœ„ 5ê°œ):")
        for i, pair in enumerate(high_quality, 1):
            question = pair['question']
            answer = pair.get('answer', {})
            
            print(f"\n   {i}. í’ˆì§ˆì ìˆ˜: {pair.get('quality_score', 0)}")
            print(f"      ì§ˆë¬¸: {question.get('title', 'N/A')[:100]}...")
            print(f"      Qì ìˆ˜: {question.get('score', 0)} | Aì ìˆ˜: {answer.get('score', 0) if answer else 'N/A'}")
            print(f"      íƒœê·¸: {', '.join(question.get('tags', []))}")
        
        # ë°ì´í„° ì €ì¥
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_file = Path(Config.OUTPUT_DIR) / f"large_scale_stackoverflow_{timestamp}.json"
        
        # JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
        save_data = []
        for pair in qa_pairs:
            save_item = {
                'question': pair['question'],
                'answer': pair.get('answer'),
                'quality_score': pair.get('quality_score', 0),
                'is_complete': bool(pair.get('answer')),
                'collected_at': datetime.now().isoformat()
            }
            save_data.append(save_item)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"\nğŸ’¾ ëŒ€ê·œëª¨ ë°ì´í„° ì €ì¥:")
        print(f"   íŒŒì¼: {output_file}")
        print(f"   í¬ê¸°: {output_file.stat().st_size:,} bytes")
        
        # ìš”ì•½ ë¦¬í¬íŠ¸
        summary_file = Path(Config.OUTPUT_DIR) / f"collection_report_{timestamp}.txt"
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write("Stack Overflow ëŒ€ê·œëª¨ ìˆ˜ì§‘ ë¦¬í¬íŠ¸\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"ìˆ˜ì§‘ ì¼ì‹œ: {start_time.strftime('%Y-%m-%d %H:%M:%S')} ~ {end_time.strftime('%H:%M:%S')}\n")
            f.write(f"ìˆ˜ì§‘ ê¸°ê°„: {from_date.strftime('%Y-%m-%d')} ~ í˜„ì¬ (90ì¼)\n")
            f.write(f"ì†Œìš” ì‹œê°„: {duration.total_seconds():.1f}ì´ˆ\n")
            f.write(f"ìˆ˜ì§‘ í˜ì´ì§€: {max_pages}í˜ì´ì§€\n\n")
            f.write(f"ì´ Q&A ìŒ: {len(qa_pairs)}ê°œ\n")
            f.write(f"ì™„ì „í•œ ìŒ: {complete_pairs}ê°œ ({complete_pairs/len(qa_pairs)*100:.1f}%)\n")
            f.write(f"í‰ê·  í’ˆì§ˆ ì ìˆ˜: {sum(quality_scores)/len(quality_scores):.1f}\n\n")
            f.write(f"ë°ì´í„° íŒŒì¼: {output_file.name}\n")
        
        print(f"   ë¦¬í¬íŠ¸: {summary_file}")
        
        # ìˆ˜ì§‘ í†µê³„
        stats = collector.get_collection_stats()
        print(f"\nğŸ“Š API ì‚¬ìš© í†µê³„:")
        print(f"   ì˜¤ëŠ˜ ì‚¬ìš©ëŸ‰: {stats['requests_today']}")
        print(f"   ë‚¨ì€ í• ë‹¹ëŸ‰: {stats['daily_quota_remaining']}")
        
        await collector.close()
        
        print(f"\nğŸ‰ ëŒ€ê·œëª¨ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print(f"   ìˆ˜ì§‘ëœ ê³ í’ˆì§ˆ Excel Q&A: {complete_pairs}ê°œ")
        
        return qa_pairs
        
    except Exception as e:
        print(f"âŒ ëŒ€ê·œëª¨ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ Stack Overflow ëŒ€ê·œëª¨ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸")
    print("=" * 70)
    
    # 1. ìºì‹œ ì´ˆê¸°í™”
    clear_stackoverflow_cache()
    
    # 2. ëŒ€ê·œëª¨ ìˆ˜ì§‘ ì‹¤í–‰
    result = asyncio.run(large_scale_collection())
    
    print(f"\nğŸ ì „ì²´ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

if __name__ == "__main__":
    main()