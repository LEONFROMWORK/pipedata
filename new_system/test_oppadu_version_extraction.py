#!/usr/bin/env python3
"""
ì˜¤ë¹ ë‘ ë²„ì „ ì •ë³´ ì¶”ì¶œ í…ŒìŠ¤íŠ¸
ì‹¤ì œ HTML êµ¬ì¡° ê¸°ë°˜ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
"""

import asyncio
import logging
from pathlib import Path
import sys
from bs4 import BeautifulSoup

# Add project root to path
sys.path.append(str(Path(__file__).parent))

from collectors.oppadu_crawler import OppaduCrawler
from core.cache import APICache, LocalCache

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

def test_html_parsing():
    """HTML íŒŒì‹± í…ŒìŠ¤íŠ¸"""
    
    # ì‹¤ì œ ì˜¤ë¹ ë‘ HTML êµ¬ì¡° ì‹œë®¬ë ˆì´ì…˜
    test_html = """
    <html>
    <head><title>Test Post</title></head>
    <body>
        <h1>í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ì œëª©</h1>
        
        <div class="post-options-display">
            <div class="options-container">
                <div class="option-item">
                    <span class="option-label">ì—‘ì…€ë²„ì „</span>
                    <span class="option-value">M365</span>
                </div>
                <div class="option-item">
                    <span class="option-label">OSë²„ì „</span>
                    <span class="option-value">ìœˆë„ìš°11</span>
                </div>
            </div>
        </div>
        
        <div class="post-content">
            <p>ì´ê²ƒì€ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ë‚´ìš©ì…ë‹ˆë‹¤. Excelì—ì„œ VLOOKUP í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤.</p>
            <p>ìˆ˜ì‹: =VLOOKUP(A1, ë°ì´í„°!A:B, 2, FALSE)</p>
        </div>
        
        <div class="answer-section">
            <div class="selected-answer-badge">ì±„íƒëœ ë‹µë³€</div>
            <div class="answer-content">
                <p>VLOOKUP í•¨ìˆ˜ ì‚¬ìš©ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:</p>
                <p>=VLOOKUP(ì°¾ì„ê°’, ë²”ìœ„, ì—´ë²ˆí˜¸, ì •í™•íˆì¼ì¹˜)</p>
                <p>ì˜ˆì‹œ: =VLOOKUP(A1, Sheet2!A:B, 2, 0)</p>
            </div>
        </div>
    </body>
    </html>
    """
    
    logger.info("ğŸ§ª HTML íŒŒì‹± í…ŒìŠ¤íŠ¸ ì‹œì‘")
    logger.info("=" * 40)
    
    try:
        # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
        soup = BeautifulSoup(test_html, 'html.parser')
        
        # OppaduCrawler ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        local_cache = LocalCache(db_path=Path("/tmp/test_oppadu.db"))
        cache = APICache(local_cache)
        crawler = OppaduCrawler(cache)
        
        # íŒŒì‹± í…ŒìŠ¤íŠ¸
        post_data = crawler._parse_post_detail(test_html, "http://test.com/post/123")
        
        if post_data:
            logger.info("âœ… íŒŒì‹± ì„±ê³µ!")
            logger.info(f"   ì œëª©: {post_data.get('title', 'N/A')}")
            logger.info(f"   Excel ë²„ì „: {post_data.get('metadata', {}).get('excel_version', 'N/A')}")
            logger.info(f"   OS ë²„ì „: {post_data.get('metadata', {}).get('os_version', 'N/A')}")
            
            question = post_data.get('question', {})
            answer = post_data.get('answer', {})
            
            logger.info(f"   ì§ˆë¬¸ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(question.get('text', ''))} ë¬¸ì")
            logger.info(f"   ë‹µë³€ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(answer.get('text', ''))} ë¬¸ì")
            logger.info(f"   ì§ˆë¬¸ì— ì½”ë“œ í¬í•¨: {question.get('has_code', False)}")
            logger.info(f"   ë‹µë³€ì— ì½”ë“œ í¬í•¨: {answer.get('has_code', False)}")
            logger.info(f"   í’ˆì§ˆ ì ìˆ˜: {post_data.get('quality_score', 0):.2f}")
            
            # ìƒì„¸ ë‚´ìš© ì¶œë ¥
            logger.info("\nğŸ“ ìƒì„¸ ë‚´ìš©:")
            logger.info(f"   ì§ˆë¬¸: {question.get('text', 'N/A')[:100]}...")
            logger.info(f"   ë‹µë³€: {answer.get('text', 'N/A')[:100]}...")
            
        else:
            logger.error("âŒ íŒŒì‹± ì‹¤íŒ¨")
            
    except Exception as e:
        logger.error(f"âŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        logger.error(f"Traceback: {traceback.format_exc()}")

async def test_live_oppadu_parsing():
    """ì‹¤ì œ ì˜¤ë¹ ë‘ ì›¹ì‚¬ì´íŠ¸ íŒŒì‹± í…ŒìŠ¤íŠ¸ (ë§¤ìš° ì œí•œì )"""
    
    logger.info("\nğŸŒ ì‹¤ì œ ì˜¤ë¹ ë‘ ì›¹ì‚¬ì´íŠ¸ í…ŒìŠ¤íŠ¸")
    logger.info("=" * 40)
    
    try:
        local_cache = LocalCache(db_path=Path("/tmp/test_oppadu_live.db"))
        cache = APICache(local_cache)
        crawler = OppaduCrawler(cache)
        
        # ë§¤ìš° ì œí•œì ì¸ í…ŒìŠ¤íŠ¸ (1í˜ì´ì§€, ìµœëŒ€ 1ê°œ í•­ëª©)
        logger.info("ğŸ“¡ ì˜¤ë¹ ë‘ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ìƒ˜í”Œ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
        
        results = await crawler.collect_oppadu_questions(max_pages=1)
        
        if results:
            logger.info(f"âœ… ì‹¤ì œ ìˆ˜ì§‘ ì„±ê³µ: {len(results)}ê°œ í•­ëª©")
            
            first_item = results[0]
            logger.info(f"   ì œëª©: {first_item.get('title', 'N/A')[:50]}...")
            logger.info(f"   Excel ë²„ì „: {first_item.get('metadata', {}).get('excel_version', 'N/A')}")
            logger.info(f"   OS ë²„ì „: {first_item.get('metadata', {}).get('os_version', 'N/A')}")
            logger.info(f"   í’ˆì§ˆ ì ìˆ˜: {first_item.get('quality_score', 0):.2f}")
            
        else:
            logger.warning("âš ï¸ ì‹¤ì œ ìˆ˜ì§‘ ì‹¤íŒ¨ ë˜ëŠ” ë°ì´í„° ì—†ìŒ")
            
    except Exception as e:
        logger.error(f"âŒ ì‹¤ì œ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        logger.error(f"Traceback: {traceback.format_exc()}")

async def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    
    # 1. HTML íŒŒì‹± í…ŒìŠ¤íŠ¸
    test_html_parsing()
    
    # 2. ì‹¤ì œ ì›¹ì‚¬ì´íŠ¸ í…ŒìŠ¤íŠ¸ (ì„ íƒì )
    await test_live_oppadu_parsing()
    
    logger.info("\nğŸ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

if __name__ == "__main__":
    asyncio.run(main())