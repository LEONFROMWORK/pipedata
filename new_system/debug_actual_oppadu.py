#!/usr/bin/env python3
"""
ì‹¤ì œ ì˜¤ë¹ ë‘ URLë¡œ ì½˜í…ì¸  ì¶”ì¶œ ë””ë²„ê¹…
"""

import asyncio
import logging
from pathlib import Path
import sys
from bs4 import BeautifulSoup

# Add project root to path
sys.path.append(str(Path(__file__).parent))

from collectors.oppadu_crawler import OppaduCrawler
from core.cache import APICache, LocalCache

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

async def debug_actual_oppadu_urls():
    """ì‹¤ì œ ì˜¤ë¹ ë‘ URLë¡œ ì½˜í…ì¸  ì¶”ì¶œ ë””ë²„ê¹…"""
    
    logger.info("ğŸ” ì‹¤ì œ ì˜¤ë¹ ë‘ URL ë””ë²„ê¹…")
    logger.info("=" * 50)
    
    try:
        local_cache = LocalCache(db_path=Path("/tmp/debug_actual_oppadu.db"))
        cache = APICache(local_cache)
        crawler = OppaduCrawler(cache)
        
        # 1ë‹¨ê³„: ì‹¤ì œ ë‹µë³€ ì™„ë£Œ ê²Œì‹œê¸€ URL ìˆ˜ì§‘
        logger.info("ğŸ“„ ì‹¤ì œ ë‹µë³€ ì™„ë£Œ ê²Œì‹œê¸€ URL ìˆ˜ì§‘ ì¤‘...")
        answered_posts = await crawler._get_answered_posts(crawler.community_url)
        
        if answered_posts:
            logger.info(f"âœ… {len(answered_posts)}ê°œì˜ ë‹µë³€ ì™„ë£Œ ê²Œì‹œê¸€ ë°œê²¬")
            
            # ì²« ë²ˆì§¸ ê²Œì‹œê¸€ë¡œ í…ŒìŠ¤íŠ¸
            test_url = answered_posts[0]
            logger.info(f"ğŸ§ª í…ŒìŠ¤íŠ¸ URL: {test_url}")
            
            # HTML ìˆ˜ì§‘
            html_content = await crawler._fetch_with_cloudscraper(test_url)
            
            if html_content:
                logger.info(f"âœ… HTML ìˆ˜ì§‘ ì„±ê³µ: {len(html_content)} ë¬¸ì")
                
                # BeautifulSoupìœ¼ë¡œ êµ¬ì¡° ë¶„ì„
                soup = BeautifulSoup(html_content, 'html.parser')
                
                # HTML êµ¬ì¡° ì°¾ê¸°
                logger.info("\nğŸ” HTML êµ¬ì¡° ë¶„ì„:")
                
                # ì£¼ìš” ì»¨í…Œì´ë„ˆë“¤ ì°¾ê¸°
                containers = [
                    ('post-content', soup.find(class_='post-content')),
                    ('board-content', soup.find(class_='board-content')),
                    ('content', soup.find(class_='content')),
                    ('xe_content', soup.find(class_='xe_content')),
                    ('article-content', soup.find(class_='article-content')),
                    ('question-content', soup.find(class_='question-content')),
                    ('view-content', soup.find(class_='view-content')),
                ]
                
                for name, element in containers:
                    if element:
                        text = element.get_text(strip=True)
                        logger.info(f"   âœ… {name}: {text[:100]}... (ê¸¸ì´: {len(text)})")
                
                # options-container ê´€ë ¨ ì°¾ê¸°
                logger.info("\nğŸ” ì˜µì…˜ ì»¨í…Œì´ë„ˆ ì°¾ê¸°:")
                options_patterns = [
                    ('post-options-display', soup.find(class_='post-options-display')),
                    ('options-container', soup.find(class_='options-container')),
                    ('option-item', soup.find_all(class_='option-item')),
                    ('post-meta', soup.find(class_='post-meta')),
                    ('meta-info', soup.find(class_='meta-info')),
                ]
                
                for name, element in options_patterns:
                    if element:
                        if isinstance(element, list):
                            logger.info(f"   âœ… {name}: {len(element)}ê°œ ë°œê²¬")
                            for i, item in enumerate(element[:3]):
                                logger.info(f"     {i+1}: {item.get_text(strip=True)[:50]}...")
                        else:
                            logger.info(f"   âœ… {name}: {element.get_text(strip=True)[:100]}...")
                
                # ë‹µë³€ ê´€ë ¨ ì°¾ê¸°
                logger.info("\nğŸ” ë‹µë³€ ìš”ì†Œ ì°¾ê¸°:")
                answer_patterns = [
                    ('selected-answer-badge', soup.find(class_='selected-answer-badge')),
                    ('answer-complete-badge', soup.find(class_='answer-complete-badge')),
                    ('best-answer', soup.find(class_='best-answer')),
                    ('accepted-answer', soup.find(class_='accepted-answer')),
                    ('answer-content', soup.find(class_='answer-content')),
                    ('reply-content', soup.find(class_='reply-content')),
                ]
                
                for name, element in answer_patterns:
                    if element:
                        text = element.get_text(strip=True)
                        logger.info(f"   âœ… {name}: {text[:100]}... (ê¸¸ì´: {len(text)})")
                
                # ëª¨ë“  í´ë˜ìŠ¤ ë¶„ì„ (ì˜¤ë¹ ë‘ íŠ¹í™”)
                logger.info("\nğŸ“Š ë°œê²¬ëœ ëª¨ë“  í´ë˜ìŠ¤:")
                all_classes = set()
                for element in soup.find_all(True):
                    if element.get('class'):
                        all_classes.update(element.get('class'))
                
                # ê´€ë ¨ í´ë˜ìŠ¤ë§Œ í•„í„°ë§
                relevant_classes = [cls for cls in all_classes if any(keyword in cls.lower() 
                                  for keyword in ['content', 'post', 'answer', 'title', 'option', 'excel', 'question', 'reply', 'view', 'meta'])]
                
                logger.info(f"   ê´€ë ¨ í´ë˜ìŠ¤ë“¤: {sorted(relevant_classes)}")
                
                # ì‹¤ì œ íŒŒì‹± í…ŒìŠ¤íŠ¸
                logger.info("\nğŸ§ª ì‹¤ì œ íŒŒì‹± í…ŒìŠ¤íŠ¸:")
                post_data = crawler._parse_post_detail(html_content, test_url)
                
                if post_data:
                    logger.info(f"   âœ… íŒŒì‹± ì„±ê³µ!")
                    logger.info(f"     ì œëª©: {post_data.get('title', 'N/A')}")
                    logger.info(f"     Excel ë²„ì „: {post_data.get('metadata', {}).get('excel_version', 'N/A')}")
                    logger.info(f"     OS ë²„ì „: {post_data.get('metadata', {}).get('os_version', 'N/A')}")
                    
                    question = post_data.get('question', {})
                    answer = post_data.get('answer', {})
                    
                    logger.info(f"     ì§ˆë¬¸ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(question.get('text', ''))} ë¬¸ì")
                    logger.info(f"     ë‹µë³€ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(answer.get('text', ''))} ë¬¸ì")
                    
                    if question.get('text'):
                        logger.info(f"     ì§ˆë¬¸ ë¯¸ë¦¬ë³´ê¸°: {question.get('text')[:200]}...")
                    if answer.get('text'):
                        logger.info(f"     ë‹µë³€ ë¯¸ë¦¬ë³´ê¸°: {answer.get('text')[:200]}...")
                else:
                    logger.error("   âŒ íŒŒì‹± ì‹¤íŒ¨")
                
            else:
                logger.error(f"âŒ HTML ìˆ˜ì§‘ ì‹¤íŒ¨: {test_url}")
        else:
            logger.error("âŒ ë‹µë³€ ì™„ë£Œ ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            
    except Exception as e:
        logger.error(f"âŒ ë””ë²„ê¹… ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        logger.error(f"Traceback: {traceback.format_exc()}")

if __name__ == "__main__":
    asyncio.run(debug_actual_oppadu_urls())