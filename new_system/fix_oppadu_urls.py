#!/usr/bin/env python3
"""
ì˜¤ë¹ ë‘ URL ë¬¸ì œ ë¶„ì„ ë° ìˆ˜ì •
ì‹¤ì œ ê²Œì‹œê¸€ í˜ì´ì§€ê°€ ì•„ë‹Œ í™ˆí˜ì´ì§€ë¡œ ë¦¬ë””ë ‰ì…˜ë˜ëŠ” ë¬¸ì œ í•´ê²°
"""

import cloudscraper
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, parse_qs
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def analyze_oppadu_url_issue():
    """ì˜¤ë¹ ë‘ URL ë¦¬ë””ë ‰ì…˜ ë¬¸ì œ ë¶„ì„"""
    
    base_url = "https://www.oppadu.com"
    list_url = f"{base_url}/community/question/"
    
    # CloudScraper ì„¤ì •
    scraper = cloudscraper.create_scraper(
        browser={
            'browser': 'chrome',
            'platform': 'windows',
            'desktop': True
        }
    )
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
        'Referer': base_url
    }
    scraper.headers.update(headers)
    
    print("ğŸ” ì˜¤ë¹ ë‘ URL ë¬¸ì œ ë¶„ì„")
    print("="*80)
    
    try:
        # 1. ëª©ë¡ í˜ì´ì§€ì—ì„œ ì‹¤ì œ ë§í¬ ì¶”ì¶œ
        print(f"ğŸ“‹ ëª©ë¡ í˜ì´ì§€ ë¶„ì„: {list_url}")
        response = scraper.get(list_url, timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # 2. ëª¨ë“  ë§í¬ ë¶„ì„
        all_links = soup.find_all('a', href=True)
        print(f"ì´ {len(all_links)}ê°œì˜ ë§í¬ ë°œê²¬")
        
        # ê²Œì‹œê¸€ ê´€ë ¨ ë§í¬ í•„í„°ë§
        post_links = []
        for link in all_links:
            href = link['href']
            link_text = link.get_text(strip=True)
            
            # ê²Œì‹œê¸€ íŒ¨í„´ í™•ì¸
            if any(pattern in href for pattern in ['uid=', 'view', 'board_id']):
                post_links.append({
                    'href': href,
                    'text': link_text,
                    'full_url': urljoin(base_url, href)
                })
        
        print(f"ê²Œì‹œê¸€ ê´€ë ¨ ë§í¬: {len(post_links)}ê°œ")
        
        # 3. ë§í¬ íŒ¨í„´ ë¶„ì„
        print(f"\nğŸ”— ë§í¬ íŒ¨í„´ ë¶„ì„:")
        patterns = {}
        for link in post_links[:10]:  # ì²« 10ê°œë§Œ ë¶„ì„
            href = link['href']
            print(f"  ì›ë³¸ href: {href}")
            print(f"  ì „ì²´ URL: {link['full_url']}")
            print(f"  ë§í¬ í…ìŠ¤íŠ¸: {link['text'][:50]}...")
            
            # URL íŒ¨í„´ ë¶„ë¥˜
            if href.startswith('?'):
                patterns['query_only'] = patterns.get('query_only', 0) + 1
            elif href.startswith('/'):
                patterns['absolute_path'] = patterns.get('absolute_path', 0) + 1
            elif href.startswith('http'):
                patterns['full_url'] = patterns.get('full_url', 0) + 1
            else:
                patterns['relative'] = patterns.get('relative', 0) + 1
            print("-" * 40)
        
        print(f"\nURL íŒ¨í„´ í†µê³„: {patterns}")
        
        # 4. ë‹¤ì–‘í•œ URL í˜•ì‹ í…ŒìŠ¤íŠ¸
        print(f"\nğŸ§ª URL í˜•ì‹ í…ŒìŠ¤íŠ¸:")
        
        if post_links:
            test_link = post_links[0]
            original_href = test_link['href']
            
            # ì—¬ëŸ¬ URL í˜•ì‹ ì‹œë„
            url_variants = []
            
            # í˜„ì¬ ë°©ì‹ (ë¬¸ì œê°€ ìˆëŠ” ë°©ì‹)
            current_url = urljoin(base_url, original_href)
            url_variants.append(('Current (urljoin)', current_url))
            
            # ì§ì ‘ ê²°í•©
            if original_href.startswith('?'):
                direct_url = base_url + original_href
                url_variants.append(('Direct concat', direct_url))
                
                # ì»¤ë®¤ë‹ˆí‹° ê²½ë¡œ í¬í•¨
                community_url = f"{base_url}/community/question/{original_href}"
                url_variants.append(('With community path', community_url))
            
            # ê° URL ì‹œë„
            for variant_name, test_url in url_variants:
                print(f"\n  ğŸ” {variant_name}: {test_url}")
                
                try:
                    test_response = scraper.get(test_url, timeout=15, allow_redirects=False)
                    print(f"    ìƒíƒœ ì½”ë“œ: {test_response.status_code}")
                    
                    if test_response.status_code in [301, 302, 303, 307, 308]:
                        print(f"    ë¦¬ë””ë ‰ì…˜: {test_response.headers.get('Location', 'N/A')}")
                    
                    if test_response.status_code == 200:
                        # ì½˜í…ì¸  í™•ì¸
                        test_soup = BeautifulSoup(test_response.text, 'html.parser')
                        title = test_soup.find('title')
                        page_title = title.get_text() if title else "ì œëª© ì—†ìŒ"
                        print(f"    í˜ì´ì§€ ì œëª©: {page_title}")
                        
                        # ê²Œì‹œê¸€ ì»¨í…ì¸  ì—¬ë¶€ í™•ì¸
                        indicators = {
                            'post_content': test_soup.find(class_='post-content'),
                            'board_content': test_soup.find(class_='board-content'),
                            'article_content': test_soup.find(class_='article-content'),
                            'question_title': test_soup.find('h1'),
                            'home_indicators': test_soup.find_all(class_='slider-contents')
                        }
                        
                        for indicator_name, element in indicators.items():
                            if element:
                                if indicator_name == 'home_indicators':
                                    print(f"    âŒ í™ˆí˜ì´ì§€ ì½˜í…ì¸  ê°ì§€: {len(element)}ê°œ ìŠ¬ë¼ì´ë”")
                                else:
                                    text = element.get_text(strip=True) if hasattr(element, 'get_text') else str(element)
                                    print(f"    âœ… {indicator_name}: {text[:50]}...")
                
                except Exception as e:
                    print(f"    âŒ ì˜¤ë¥˜: {e}")
        
        # 5. ì˜¬ë°”ë¥¸ URL í˜•ì‹ ì œì•ˆ
        print(f"\nğŸ’¡ ìˆ˜ì • ì œì•ˆ:")
        print("  1. urljoin() ì‚¬ìš© ì‹œ base_urlì— ìŠ¬ë˜ì‹œ í™•ì¸")
        print("  2. ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ê°€ ìˆëŠ” ê²½ìš° ì˜¬ë°”ë¥¸ ê²½ë¡œ ê²°í•©")
        print("  3. ì„¸ì…˜/ì¿ í‚¤ ìš”êµ¬ì‚¬í•­ í™•ì¸")
        print("  4. User-Agent ë° Referer í—¤ë” í™•ì¸")
        
    except Exception as e:
        print(f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        print(traceback.format_exc())

if __name__ == "__main__":
    analyze_oppadu_url_issue()