#!/usr/bin/env python3
"""
OCRë§Œìœ¼ë¡œ ì‹¤ì œ ì´ë¯¸ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸
"""
import asyncio
import json
import logging
import sys
from pathlib import Path

# Add project root to path
sys.path.append(str(Path(__file__).parent))

from processors.image_processor import ImageProcessor
from core.cache import APICache, LocalCache

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

class OCROnlyImageProcessor(ImageProcessor):
    """OCRë§Œ ì‚¬ìš©í•˜ëŠ” ê°„ë‹¨í•œ ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ"""
    
    def _should_use_ai_enhancement(self, ocr_result, table_result, context_tags):
        """AI í–¥ìƒ ë¹„í™œì„±í™”"""
        return False
    
    async def _extract_tables_with_img2table(self, image_path):
        """img2table ë¹„í™œì„±í™” - ë¹ˆ ê²°ê³¼ ë°˜í™˜"""
        return {'tables_found': 0, 'markdown_content': '', 'raw_tables': []}

async def test_ocr_only_processing():
    """OCRë§Œìœ¼ë¡œ ì´ë¯¸ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸"""
    
    # Initialize cache and processor
    local_cache = LocalCache(db_path=Path("/tmp/ocr_test_cache.db"))
    cache = APICache(local_cache)
    processor = OCROnlyImageProcessor(cache)
    
    # ì‹¤ì œ ìŠ¤íƒì˜¤ë²„í”Œë¡œìš° ì´ë¯¸ì§€ URLs
    test_images = [
        {
            "url": "https://i.sstatic.net/AJr6APk8.png",
            "description": "Excel ë³µì¡í•œ ê³µì‹ ê²°ê³¼",
            "qa_context": {
                "question": "Find all rows that have 2 out of 3 columns in common",
                "formula": "=LET(_data,A2:C7,TEXTSPLIT(...))"
            }
        },
        {
            "url": "https://i.sstatic.net/TpgieF6J.png", 
            "description": "Excel íŒŒìŠ¤ì¹¼ ì‚¼ê°í˜• ë°°ì—´",
            "qa_context": {
                "question": "Excel array formula that references previously calculated values",
                "formula": "=LET(N,5,REDUCE(SEQUENCE(,N,1,0)...))"
            }
        },
        {
            "url": "https://i.sstatic.net/82NP1kgT.png",
            "description": "Excel í•„í„° ê³µì‹ ê²°ê³¼",
            "qa_context": {
                "question": "Filter a table with a list of value with dynamic array formula",
                "formula": "=FILTER(E2#, ISNUMBER(MATCH(TAKE(E2#,,1),B2#,0)))"
            }
        }
    ]
    
    logger.info("ğŸ” OCR ì „ìš© ì´ë¯¸ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸")
    logger.info("=" * 70)
    
    final_qa_samples = []
    
    for i, test_image in enumerate(test_images, 1):
        logger.info(f"[{i}/{len(test_images)}] {test_image['description']}")
        logger.info(f"URL: {test_image['url']}")
        
        try:
            context_tags = ["excel", "formula", "screenshot"]
            result = await processor.process_image_url(test_image["url"], context_tags)
            
            if result['success'] and result.get('extracted_content'):
                # Q&A ìƒ˜í”Œ ìƒì„±
                qa_sample = {
                    "id": f"excel_qa_with_ocr_{i:03d}",
                    "user_question": test_image["qa_context"]["question"],
                    "user_context": f"I have this Excel screenshot showing the result. Please help me understand the formula: {test_image['qa_context']['formula']}",
                    "assistant_response": f"Looking at your screenshot, I can see the Excel formula result. The formula {test_image['qa_context']['formula']} produces the output shown in the image.",
                    "code_blocks": [test_image["qa_context"]["formula"]],
                    "image_contexts": [
                        {
                            "source_url": test_image["url"],
                            "extracted_content": result["extracted_content"],
                            "content_type": result["extracted_content_type"],
                            "processing_method": result["processing_tier"],
                            "word_count": len(result["extracted_content"].split()),
                            "confidence": "medium"  # OCRë§Œìœ¼ë¡œëŠ” medium
                        }
                    ],
                    "metadata": {
                        "difficulty": "advanced",
                        "functions": ["LET", "FILTER", "MATCH", "ARRAY"],
                        "quality_score": 7.5,
                        "source": "stackoverflow",
                        "has_images": True,
                        "processing_cost": 0.0  # OCRëŠ” ë¬´ë£Œ
                    }
                }
                
                final_qa_samples.append(qa_sample)
                
                logger.info("âœ… ì„±ê³µ!")
                logger.info(f"   â€¢ ì¶”ì¶œ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(result['extracted_content'])} ë¬¸ì")
                logger.info(f"   â€¢ ë‹¨ì–´ ìˆ˜: {len(result['extracted_content'].split())} ê°œ")
                logger.info(f"   â€¢ ì²˜ë¦¬ ë°©ë²•: {result['processing_tier']}")
                
                # OCR ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
                text_preview = result['extracted_content'][:150].replace('\n', ' ')
                logger.info(f"   â€¢ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: '{text_preview}...'")
                
            else:
                logger.error(f"âŒ ì‹¤íŒ¨: {result.get('error', 'No content extracted')}")
                
        except Exception as e:
            logger.error(f"âŒ ì˜ˆì™¸ ë°œìƒ: {e}")
        
        logger.info("-" * 50)
    
    # ìµœì¢… ë°ì´í„°ì…‹ êµ¬ì„±
    final_dataset = {
        "dataset_info": {
            "name": "Excel Q&A Dataset with OCR-Extracted Images",
            "version": "1.0-ocr",
            "description": "Excel Q&A pairs with OCR-extracted image content",
            "total_samples": len(final_qa_samples),
            "processing_pipeline": "Tier 1 (tesseract OCR) only",
            "generated_at": "2025-07-18T07:35:00Z"
        },
        "processing_summary": {
            "images_processed": len(test_images),
            "successful_extractions": len(final_qa_samples),
            "success_rate": (len(final_qa_samples) / len(test_images) * 100) if test_images else 0,
            "average_text_length": sum(len(sample["image_contexts"][0]["extracted_content"]) for sample in final_qa_samples) / len(final_qa_samples) if final_qa_samples else 0,
            "total_processing_cost": 0.0
        },
        "samples": final_qa_samples
    }
    
    # ê²°ê³¼ ì¶œë ¥
    logger.info("=" * 70)
    logger.info("ğŸ“‹ ìµœì¢… OCR ê¸°ë°˜ Q&A ë°ì´í„°ì…‹:")
    logger.info("=" * 70)
    print(json.dumps(final_dataset, indent=2, ensure_ascii=False))
    
    # íŒŒì¼ ì €ì¥
    output_path = "/Users/kevin/bigdata/data/output/ocr_based_qa_dataset.json"
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(final_dataset, f, indent=2, ensure_ascii=False)
    
    logger.info(f"\nğŸ’¾ ë°ì´í„°ì…‹ ì €ì¥: {output_path}")
    
    # í†µê³„ ìš”ì•½
    summary = final_dataset["processing_summary"]
    logger.info(f"\nğŸ“Š OCR ì¶”ì¶œ í†µê³„:")
    logger.info(f"   â€¢ ì„±ê³µë¥ : {summary['success_rate']:.1f}%")
    logger.info(f"   â€¢ í‰ê·  í…ìŠ¤íŠ¸ ê¸¸ì´: {summary['average_text_length']:.0f} ë¬¸ì")
    logger.info(f"   â€¢ ì´ Q&A ìƒ˜í”Œ: {final_dataset['dataset_info']['total_samples']}ê°œ")
    logger.info(f"   â€¢ ì²˜ë¦¬ ë¹„ìš©: ${summary['total_processing_cost']}")
    
    return final_dataset

if __name__ == "__main__":
    asyncio.run(test_ocr_only_processing())