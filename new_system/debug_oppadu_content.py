#!/usr/bin/env python3
"""
ì˜¤ë¹ ë‘ ì½˜í…ì¸  ì¶”ì¶œ ë””ë²„ê¹…
ì‹¤ì œ HTML êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ì—¬ ë¬¸ì œì  ì°¾ê¸°
"""

import asyncio
import logging
from pathlib import Path
import sys
from bs4 import BeautifulSoup

# Add project root to path
sys.path.append(str(Path(__file__).parent))

from collectors.oppadu_crawler import OppaduCrawler
from core.cache import APICache, LocalCache

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

async def debug_oppadu_content_extraction():
    """ì˜¤ë¹ ë‘ ì½˜í…ì¸  ì¶”ì¶œ ë””ë²„ê¹…"""
    
    logger.info("ğŸ” ì˜¤ë¹ ë‘ ì½˜í…ì¸  ì¶”ì¶œ ë””ë²„ê¹…")
    logger.info("=" * 50)
    
    try:
        local_cache = LocalCache(db_path=Path("/tmp/debug_oppadu.db"))
        cache = APICache(local_cache)
        crawler = OppaduCrawler(cache)
        
        # ë‹¨ì¼ ê²Œì‹œê¸€ ìƒì„¸ í˜ì´ì§€ ì§ì ‘ ìˆ˜ì§‘
        test_url = "https://www.oppadu.com?board_id=1&action=view&uid=79620&pg=1"
        
        logger.info(f"ğŸ“¡ í…ŒìŠ¤íŠ¸ URL: {test_url}")
        
        # í˜ì´ì§€ HTML ìˆ˜ì§‘
        html_content = await crawler._fetch_with_cloudscraper(test_url)
        
        if html_content:
            logger.info(f"âœ… HTML ìˆ˜ì§‘ ì„±ê³µ: {len(html_content)} ë¬¸ì")
            
            # HTML êµ¬ì¡° ë¶„ì„
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ì œëª© ì°¾ê¸°
            logger.info("\nğŸ” ì œëª© ìš”ì†Œ ì°¾ê¸°:")
            title_candidates = [
                soup.find('h1'),
                soup.find(class_='post-title'),
                soup.find('title'),
                soup.find(class_='board_title'),
                soup.find(class_='subject')
            ]
            
            for i, candidate in enumerate(title_candidates):
                if candidate:
                    logger.info(f"   ì œëª© í›„ë³´ {i+1}: {candidate.get_text(strip=True)[:100]}...")
            
            # options-container ì°¾ê¸°
            logger.info("\nğŸ” ì˜µì…˜ ì»¨í…Œì´ë„ˆ ì°¾ê¸°:")
            options_display = soup.find(class_='post-options-display')
            if options_display:
                logger.info("   âœ… post-options-display ë°œê²¬")
                options_container = options_display.find(class_='options-container')
                if options_container:
                    logger.info("   âœ… options-container ë°œê²¬")
                    option_items = options_container.find_all(class_='option-item')
                    logger.info(f"   ğŸ“Š option-item ê°œìˆ˜: {len(option_items)}")
                    
                    for item in option_items:
                        label = item.find(class_='option-label')
                        value = item.find(class_='option-value')
                        if label and value:
                            logger.info(f"     {label.get_text(strip=True)}: {value.get_text(strip=True)}")
                else:
                    logger.warning("   âš ï¸ options-container ì—†ìŒ")
            else:
                logger.warning("   âš ï¸ post-options-display ì—†ìŒ")
            
            # post-content ì°¾ê¸°
            logger.info("\nğŸ” ê²Œì‹œê¸€ ì½˜í…ì¸  ì°¾ê¸°:")
            content_candidates = [
                soup.find(class_='post-content'),
                soup.find(class_='board_content'),
                soup.find(class_='content'),
                soup.find(class_='xe_content'),
                soup.find(class_='article-content')
            ]
            
            for i, candidate in enumerate(content_candidates):
                if candidate:
                    text = candidate.get_text(strip=True)
                    logger.info(f"   ì½˜í…ì¸  í›„ë³´ {i+1}: {text[:200]}... (ê¸¸ì´: {len(text)})")
            
            # selected-answer-badge ì°¾ê¸°
            logger.info("\nğŸ” ì±„íƒëœ ë‹µë³€ ì°¾ê¸°:")
            answer_candidates = [
                soup.find(class_='selected-answer-badge'),
                soup.find(class_='answer-complete-badge'),
                soup.find(class_='best-answer'),
                soup.find(class_='accepted-answer')
            ]
            
            for i, candidate in enumerate(answer_candidates):
                if candidate:
                    logger.info(f"   ë‹µë³€ ë°°ì§€ í›„ë³´ {i+1}: {candidate}")
                    
                    # ì£¼ë³€ ìš”ì†Œ íƒìƒ‰
                    parent = candidate.find_parent()
                    if parent:
                        answer_text = parent.get_text(strip=True)
                        logger.info(f"     ë¶€ëª¨ ìš”ì†Œ í…ìŠ¤íŠ¸: {answer_text[:200]}...")
            
            # ì „ì²´ HTML êµ¬ì¡° ì¼ë¶€ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
            logger.info("\nğŸ“„ HTML êµ¬ì¡° ìƒ˜í”Œ:")
            body = soup.find('body')
            if body:
                # ì£¼ìš” í´ë˜ìŠ¤ë“¤ ì°¾ê¸°
                all_classes = set()
                for element in body.find_all(True):
                    if element.get('class'):
                        all_classes.update(element.get('class'))
                
                logger.info(f"   ë°œê²¬ëœ CSS í´ë˜ìŠ¤ ê°œìˆ˜: {len(all_classes)}")
                relevant_classes = [cls for cls in all_classes if any(keyword in cls.lower() 
                                  for keyword in ['content', 'post', 'answer', 'title', 'option'])]
                logger.info(f"   ê´€ë ¨ í´ë˜ìŠ¤ë“¤: {sorted(relevant_classes)}")
            
            # ì‹¤ì œ íŒŒì‹± í…ŒìŠ¤íŠ¸
            logger.info("\nğŸ§ª ì‹¤ì œ íŒŒì‹± í…ŒìŠ¤íŠ¸:")
            post_data = crawler._parse_post_detail(html_content, test_url)
            
            if post_data:
                logger.info(f"   âœ… íŒŒì‹± ì„±ê³µ!")
                logger.info(f"     ì œëª©: {post_data.get('title', 'N/A')}")
                logger.info(f"     Excel ë²„ì „: {post_data.get('metadata', {}).get('excel_version', 'N/A')}")
                logger.info(f"     OS ë²„ì „: {post_data.get('metadata', {}).get('os_version', 'N/A')}")
                
                question = post_data.get('question', {})
                answer = post_data.get('answer', {})
                
                logger.info(f"     ì§ˆë¬¸ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(question.get('text', ''))} ë¬¸ì")
                logger.info(f"     ë‹µë³€ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(answer.get('text', ''))} ë¬¸ì")
                
                if question.get('text'):
                    logger.info(f"     ì§ˆë¬¸ ë¯¸ë¦¬ë³´ê¸°: {question.get('text')[:200]}...")
                if answer.get('text'):
                    logger.info(f"     ë‹µë³€ ë¯¸ë¦¬ë³´ê¸°: {answer.get('text')[:200]}...")
            else:
                logger.error("   âŒ íŒŒì‹± ì‹¤íŒ¨")
            
        else:
            logger.error("âŒ HTML ìˆ˜ì§‘ ì‹¤íŒ¨")
            
    except Exception as e:
        logger.error(f"âŒ ë””ë²„ê¹… ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        logger.error(f"Traceback: {traceback.format_exc()}")

if __name__ == "__main__":
    asyncio.run(debug_oppadu_content_extraction())