"""
Reddit ì´ë¯¸ì§€ 403 ìš°íšŒë¥¼ ìœ„í•œ ê³ ê¸‰ ê¸°ë²• êµ¬í˜„
ìµœì‹  2025ë…„ ê¸°ë²•ë“¤ì„ ì¢…í•©ì ìœ¼ë¡œ ì ìš©í•˜ì—¬ preview.redd.it ì ‘ê·¼ ì„±ê³µë¥  ê·¹ëŒ€í™”
"""
import asyncio
import random
import time
import re
import html
from urllib.parse import urlparse, parse_qs, unquote, urlencode
from typing import Dict, List, Optional, Tuple
import logging
import cloudscraper

logger = logging.getLogger('pipeline.reddit_bypasser')

class RedditImageBypasser:
    """Reddit ì´ë¯¸ì§€ 403 ìš°íšŒë¥¼ ìœ„í•œ ì¢…í•©ì ì¸ ì†”ë£¨ì…˜"""
    
    def __init__(self, reddit_credentials: Dict[str, str]):
        self.reddit_credentials = reddit_credentials
        
        # ìµœì‹  ë¸Œë¼ìš°ì € User-Agent ëª©ë¡ (2025ë…„ ê¸°ì¤€)
        self.user_agents = [
            # Chrome ìµœì‹  ë²„ì „ë“¤
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            
            # Firefox ìµœì‹  ë²„ì „ë“¤
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:122.0) Gecko/20100101 Firefox/122.0',
            'Mozilla/5.0 (X11; Linux x86_64; rv:122.0) Gecko/20100101 Firefox/122.0',
            
            # Edge ìµœì‹  ë²„ì „
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36 Edg/121.0.0.0',
            
            # Safari ìµœì‹  ë²„ì „
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2.1 Safari/605.1.15'
        ]
        
        # Reddit OAuth ì„¸ì…˜ ê´€ë¦¬
        self.reddit_session = None
        self.session_expires = 0
        
        # ì„±ê³µë¥  ì¶”ì 
        self.success_stats = {
            'total_attempts': 0,
            'successful_downloads': 0,
            'method_success': {}
        }
    
    async def get_reddit_oauth_session(self) -> Optional[str]:
        """Reddit OAuth ì„¸ì…˜ í† í° íšë“"""
        try:
            if self.reddit_session and time.time() < self.session_expires:
                return self.reddit_session
            
            # Reddit OAuth ì¸ì¦
            import aiohttp
            
            auth_data = {
                'grant_type': 'client_credentials'
            }
            
            auth_headers = {
                'User-Agent': f"ExcelQACollector/1.0 by /u/{self.reddit_credentials.get('username', 'test_user')}",
                'Authorization': f"Basic {self._get_basic_auth()}"
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    'https://www.reddit.com/api/v1/access_token',
                    data=auth_data,
                    headers=auth_headers
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        self.reddit_session = data.get('access_token')
                        self.session_expires = time.time() + data.get('expires_in', 3600) - 60
                        logger.info("âœ… Reddit OAuth ì„¸ì…˜ íšë“ ì„±ê³µ")
                        return self.reddit_session
                    else:
                        logger.warning(f"Reddit OAuth ì‹¤íŒ¨: {response.status}")
                        return None
                        
        except Exception as e:
            logger.error(f"Reddit OAuth ì˜¤ë¥˜: {e}")
            return None
    
    def _get_basic_auth(self) -> str:
        """Reddit OAuthìš© Basic ì¸ì¦ í—¤ë” ìƒì„±"""
        import base64
        credentials = f"{self.reddit_credentials.get('client_id', '')}:{self.reddit_credentials.get('client_secret', '')}"
        return base64.b64encode(credentials.encode()).decode()
    
    def decode_reddit_url(self, url: str) -> str:
        """Reddit URL ë””ì½”ë”© ë° ì •ë¦¬"""
        if not url:
            return url
        
        # HTML ì—”í‹°í‹° ë””ì½”ë”©
        url = html.unescape(url)
        
        # URL ë””ì½”ë”©
        url = unquote(url)
        
        # Reddit íŠ¹í™” ë””ì½”ë”© íŒ¨í„´ë“¤
        # amp; íŒ¨í„´ ì •ë¦¬
        url = url.replace('amp;s=', 's=')
        url = re.sub(r'amp;', '&', url)
        
        # ì¤‘ë³µ í”„ë¡œí† ì½œ ì œê±°
        url = re.sub(r'https?://https?://', 'https://', url)
        
        return url
    
    def get_alternative_reddit_urls(self, original_url: str) -> List[str]:
        """ì›ë³¸ URLì—ì„œ ë‹¤ì–‘í•œ ëŒ€ì•ˆ URL ìƒì„±"""
        alternatives = [original_url]
        
        parsed = urlparse(original_url)
        
        # 1. preview.redd.it â†’ i.redd.it ë³€í™˜
        if 'preview.redd.it' in parsed.netloc:
            i_redd_url = original_url.replace('preview.redd.it', 'i.redd.it')
            # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì œê±°í•œ ë²„ì „
            clean_i_redd = i_redd_url.split('?')[0]
            alternatives.extend([i_redd_url, clean_i_redd])
        
        # 2. external-preview.redd.it â†’ preview.redd.it ë³€í™˜
        if 'external-preview.redd.it' in parsed.netloc:
            preview_url = original_url.replace('external-preview.redd.it', 'preview.redd.it')
            alternatives.append(preview_url)
        
        # 3. ë‹¤ì–‘í•œ í¬ê¸°/í’ˆì§ˆ íŒŒë¼ë¯¸í„° ì‹œë„
        if '?' in original_url:
            base_url = original_url.split('?')[0]
            
            # ê³ í’ˆì§ˆ íŒŒë¼ë¯¸í„°ë“¤
            quality_params = [
                'format=png&auto=webp&s=',
                'format=jpg&auto=webp&s=',
                'width=1024&format=png&auto=webp&s=',
                'width=512&format=jpg&auto=webp&s='
            ]
            
            for param in quality_params:
                alternatives.append(f"{base_url}?{param}")
        
        # 4. HTTPS â†’ HTTP ëŒ€ì•ˆ (ìµœí›„ì˜ ìˆ˜ë‹¨)
        if original_url.startswith('https://'):
            alternatives.append(original_url.replace('https://', 'http://'))
        
        return list(set(alternatives))  # ì¤‘ë³µ ì œê±°
    
    def get_reddit_headers(self, url: str, oauth_token: Optional[str] = None) -> Dict[str, str]:
        """Reddit íŠ¹í™” í—¤ë” ìƒì„±"""
        parsed = urlparse(url)
        
        # ëœë¤ User-Agent ì„ íƒ
        user_agent = random.choice(self.user_agents)
        
        headers = {
            'User-Agent': user_agent,
            'Accept': 'image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache',
            'DNT': '1',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'image',
            'Sec-Fetch-Mode': 'no-cors',
            'Sec-Fetch-Site': 'cross-site',
            'Sec-Ch-Ua': '"Not A(Brand)";v="99", "Google Chrome";v="121", "Chromium";v="121"',
            'Sec-Ch-Ua-Mobile': '?0',
            'Sec-Ch-Ua-Platform': '"Windows"'
        }
        
        # Reddit ë„ë©”ì¸ë³„ íŠ¹í™” í—¤ë”
        if 'reddit' in parsed.netloc:
            headers.update({
                'Referer': 'https://www.reddit.com/',
                'Origin': 'https://www.reddit.com'
            })
            
            # OAuth í† í°ì´ ìˆìœ¼ë©´ Authorization í—¤ë” ì¶”ê°€ (íŠ¹ì • ì—”ë“œí¬ì¸íŠ¸ë§Œ)
            if oauth_token and 'oauth.reddit.com' in parsed.netloc:
                headers['Authorization'] = f'Bearer {oauth_token}'
        
        # redd.it ì´ë¯¸ì§€ ë„ë©”ì¸ íŠ¹í™”
        elif 'redd.it' in parsed.netloc:
            headers.update({
                'Referer': 'https://www.reddit.com/',
                'Sec-Fetch-Site': 'same-site'
            })
        
        return headers
    
    async def download_reddit_image_with_bypass(self, url: str) -> Tuple[Optional[bytes], str]:
        """Reddit ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ with ì¢…í•© ìš°íšŒ ê¸°ë²•"""
        self.success_stats['total_attempts'] += 1
        
        # URL ì „ì²˜ë¦¬
        cleaned_url = self.decode_reddit_url(url)
        alternative_urls = self.get_alternative_reddit_urls(cleaned_url)
        
        logger.info(f"ğŸ¯ Reddit ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹œë„: {len(alternative_urls)}ê°œ URL")
        
        # OAuth ì„¸ì…˜ íšë“ ì‹œë„
        oauth_token = await self.get_reddit_oauth_session()
        
        # ê° URLê³¼ ë°©ë²• ì¡°í•© ì‹œë„
        methods = [
            ('cloudscraper_basic', self._download_with_cloudscraper),
            ('cloudscraper_oauth', self._download_with_cloudscraper_oauth),
            ('session_spoofing', self._download_with_session_spoofing),
            ('proxy_rotation', self._download_with_proxy_simulation)
        ]
        
        for method_name, method_func in methods:
            for attempt_url in alternative_urls:
                try:
                    logger.debug(f"  ì‹œë„: {method_name} + {attempt_url[:50]}...")
                    
                    result = await method_func(attempt_url, oauth_token)
                    if result:
                        self.success_stats['successful_downloads'] += 1
                        
                        # ì„±ê³µ í†µê³„ ì—…ë°ì´íŠ¸
                        if method_name not in self.success_stats['method_success']:
                            self.success_stats['method_success'][method_name] = 0
                        self.success_stats['method_success'][method_name] += 1
                        
                        logger.info(f"âœ… ì„±ê³µ! ë°©ë²•: {method_name}")
                        return result, method_name
                    
                    # ì‹¤íŒ¨ ì‹œ ì§€ì—° (ë ˆì´íŠ¸ ë¦¬ë°‹ íšŒí”¼)
                    await asyncio.sleep(random.uniform(0.5, 1.5))
                    
                except Exception as e:
                    logger.debug(f"  ì‹¤íŒ¨: {method_name} - {e}")
                    continue
        
        logger.error(f"âŒ ëª¨ë“  ë°©ë²• ì‹¤íŒ¨: {url}")
        return None, "all_failed"
    
    async def _download_with_cloudscraper(self, url: str, oauth_token: Optional[str]) -> Optional[bytes]:
        """Cloudscraper ê¸°ë³¸ ë‹¤ìš´ë¡œë“œ"""
        scraper = cloudscraper.create_scraper(
            browser={
                'browser': 'chrome',
                'platform': 'windows',
                'desktop': True
            }
        )
        
        headers = self.get_reddit_headers(url, oauth_token)
        
        # ì§€ì—° ì¶”ê°€
        await asyncio.sleep(random.expovariate(0.3))  # í‰ê·  3.3ì´ˆ
        
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(
            None,
            lambda: scraper.get(url, headers=headers, timeout=30, allow_redirects=True)
        )
        
        if response.status_code == 200 and len(response.content) > 1000:
            return response.content
        
        return None
    
    async def _download_with_cloudscraper_oauth(self, url: str, oauth_token: Optional[str]) -> Optional[bytes]:
        """OAuth í† í°ì„ í™œìš©í•œ ë‹¤ìš´ë¡œë“œ"""
        if not oauth_token:
            return None
        
        scraper = cloudscraper.create_scraper(
            browser={
                'browser': 'firefox',
                'platform': 'linux',
                'desktop': True
            }
        )
        
        headers = self.get_reddit_headers(url, oauth_token)
        headers['Authorization'] = f'Bearer {oauth_token}'
        
        await asyncio.sleep(random.expovariate(0.5))  # í‰ê·  2ì´ˆ
        
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(
            None,
            lambda: scraper.get(url, headers=headers, timeout=30)
        )
        
        if response.status_code == 200 and len(response.content) > 1000:
            return response.content
        
        return None
    
    async def _download_with_session_spoofing(self, url: str, oauth_token: Optional[str]) -> Optional[bytes]:
        """ì„¸ì…˜ ìŠ¤í‘¸í•‘ì„ í†µí•œ ë‹¤ìš´ë¡œë“œ"""
        scraper = cloudscraper.create_scraper(
            browser={
                'browser': 'safari',
                'platform': 'darwin',
                'desktop': True
            }
        )
        
        headers = self.get_reddit_headers(url, oauth_token)
        
        # Reddit ì„¸ì…˜ ì¿ í‚¤ ì‹œë®¬ë ˆì´ì…˜
        fake_cookies = {
            'session_tracker': f'reddit_{int(time.time())}',
            'eu_cookie_v2': '1',
            'session': f'sess_{random.randint(100000, 999999)}',
            'csv': '2',
            'edgebucket': 'control_1'
        }
        
        await asyncio.sleep(random.expovariate(0.4))  # í‰ê·  2.5ì´ˆ
        
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(
            None,
            lambda: scraper.get(url, headers=headers, cookies=fake_cookies, timeout=30)
        )
        
        if response.status_code == 200 and len(response.content) > 1000:
            return response.content
        
        return None
    
    async def _download_with_proxy_simulation(self, url: str, oauth_token: Optional[str]) -> Optional[bytes]:
        """í”„ë¡ì‹œ ì‹œë®¬ë ˆì´ì…˜ ë‹¤ìš´ë¡œë“œ"""
        scraper = cloudscraper.create_scraper(
            browser={
                'browser': random.choice(['chrome', 'firefox', 'safari']),
                'platform': random.choice(['windows', 'darwin', 'linux']),
                'desktop': True
            }
        )
        
        headers = self.get_reddit_headers(url, oauth_token)
        
        # í”„ë¡ì‹œ í—¤ë” ì‹œë®¬ë ˆì´ì…˜
        proxy_headers = {
            'X-Forwarded-For': f'{random.randint(1, 255)}.{random.randint(1, 255)}.{random.randint(1, 255)}.{random.randint(1, 255)}',
            'X-Real-IP': f'{random.randint(1, 255)}.{random.randint(1, 255)}.{random.randint(1, 255)}.{random.randint(1, 255)}',
            'X-Forwarded-Proto': 'https'
        }
        headers.update(proxy_headers)
        
        await asyncio.sleep(random.expovariate(0.2))  # í‰ê·  5ì´ˆ
        
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(
            None,
            lambda: scraper.get(url, headers=headers, timeout=30)
        )
        
        if response.status_code == 200 and len(response.content) > 1000:
            return response.content
        
        return None
    
    def get_success_rate(self) -> float:
        """í˜„ì¬ ì„±ê³µë¥  ë°˜í™˜"""
        if self.success_stats['total_attempts'] == 0:
            return 0.0
        return (self.success_stats['successful_downloads'] / self.success_stats['total_attempts']) * 100
    
    def get_detailed_stats(self) -> Dict:
        """ìƒì„¸ í†µê³„ ë°˜í™˜"""
        return {
            'success_rate': self.get_success_rate(),
            'total_attempts': self.success_stats['total_attempts'],
            'successful_downloads': self.success_stats['successful_downloads'],
            'method_breakdown': self.success_stats['method_success']
        }

# í†µí•© í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
async def test_reddit_bypass():
    """Reddit ì´ë¯¸ì§€ ìš°íšŒ í…ŒìŠ¤íŠ¸"""
    
    # Reddit ìê²©ì¦ëª… (í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ)
    import os
    credentials = {
        'client_id': os.getenv('REDDIT_CLIENT_ID', ''),
        'client_secret': os.getenv('REDDIT_CLIENT_SECRET', ''),
        'username': 'test_user'
    }
    
    bypasser = RedditImageBypasser(credentials)
    
    # í…ŒìŠ¤íŠ¸í•  Reddit ì´ë¯¸ì§€ URLë“¤
    test_urls = [
        'https://preview.redd.it/76mukstfxhdf1.png',
        'https://preview.redd.it/some_test_image.jpg',
        'https://external-preview.redd.it/test_image.png'
    ]
    
    logger.info("ğŸš€ Reddit ì´ë¯¸ì§€ 403 ìš°íšŒ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    for i, url in enumerate(test_urls, 1):
        logger.info(f"[{i}/{len(test_urls)}] í…ŒìŠ¤íŠ¸: {url}")
        
        result, method = await bypasser.download_reddit_image_with_bypass(url)
        
        if result:
            logger.info(f"âœ… ì„±ê³µ! í¬ê¸°: {len(result)} bytes, ë°©ë²•: {method}")
        else:
            logger.error(f"âŒ ì‹¤íŒ¨: {method}")
        
        logger.info("-" * 50)
    
    # ìµœì¢… í†µê³„
    stats = bypasser.get_detailed_stats()
    logger.info("ğŸ“Š ìµœì¢… í†µê³„:")
    logger.info(f"   ì„±ê³µë¥ : {stats['success_rate']:.1f}%")
    logger.info(f"   ì´ ì‹œë„: {stats['total_attempts']}")
    logger.info(f"   ì„±ê³µ: {stats['successful_downloads']}")
    logger.info(f"   ë°©ë²•ë³„ ì„±ê³µ: {stats['method_breakdown']}")

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    asyncio.run(test_reddit_bypass())