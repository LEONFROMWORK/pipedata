#!/usr/bin/env python3
"""
ì´ë¯¸ì§€ ì¶”ì¶œ ë° 403 ìš°íšŒ ì‹¤ì œ í…ŒìŠ¤íŠ¸
Stack Overflow ìˆ˜ì§‘ ë°ì´í„°ì—ì„œ ì´ë¯¸ì§€ë¥¼ ì¶”ì¶œí•˜ê³  403 ìš°íšŒ ì ìš© í…ŒìŠ¤íŠ¸
"""
import asyncio
import json
import logging
import re
from pathlib import Path
from typing import List

# Add project root to path
import sys
sys.path.append(str(Path(__file__).parent))

from processors.image_processor import ImageProcessor
from core.cache import APICache, LocalCache

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

def extract_images_from_html(html_content: str) -> List[str]:
    """HTML ì»¨í…ì¸ ì—ì„œ ì´ë¯¸ì§€ URL ì¶”ì¶œ"""
    image_urls = []
    
    # <img src="..."> íŒ¨í„´
    img_pattern = r'<img[^>]+src=["\']([^"\']+)["\'][^>]*>'
    img_matches = re.findall(img_pattern, html_content, re.IGNORECASE)
    image_urls.extend(img_matches)
    
    # <a href="..."> íŒ¨í„´ (ì´ë¯¸ì§€ ë§í¬)
    link_pattern = r'<a[^>]+href=["\']([^"\']+\.(?:png|jpg|jpeg|gif|webp|svg)(?:\?[^"\']*)?)["\'][^>]*>'
    link_matches = re.findall(link_pattern, html_content, re.IGNORECASE)
    image_urls.extend(link_matches)
    
    # ë§ˆí¬ë‹¤ìš´ ìŠ¤íƒ€ì¼ ì´ë¯¸ì§€ ë§í¬ ![alt](url)
    markdown_pattern = r'!\[.*?\]\(([^)]+)\)'
    markdown_matches = re.findall(markdown_pattern, html_content)
    image_urls.extend(markdown_matches)
    
    # ì§ì ‘ URL íŒ¨í„´ (http://...image.png)
    direct_pattern = r'https?://[^\s<>"\']+\.(?:png|jpg|jpeg|gif|webp|svg)(?:\?[^\s<>"\']*)?'
    direct_matches = re.findall(direct_pattern, html_content, re.IGNORECASE)
    image_urls.extend(direct_matches)
    
    # ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
    unique_urls = list(set(image_urls))
    
    # ë¹ˆ URLì´ë‚˜ í”„ë¡œí•„ ì´ë¯¸ì§€ ì œì™¸
    filtered_urls = []
    for url in unique_urls:
        if url and 'gravatar.com' not in url and len(url.strip()) > 10:
            # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
            if url.startswith('//'):
                url = 'https:' + url
            elif url.startswith('/'):
                url = 'https://i.sstatic.net' + url
                
            filtered_urls.append(url.strip())
    
    return filtered_urls

async def test_image_extraction_and_bypass():
    """ì‹¤ì œ ë°ì´í„°ì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ ë° 403 ìš°íšŒ í…ŒìŠ¤íŠ¸"""
    
    logger.info("ğŸš€ ì´ë¯¸ì§€ ì¶”ì¶œ ë° 403 ìš°íšŒ í…ŒìŠ¤íŠ¸")
    logger.info("=" * 70)
    
    # ì´ì „ ìˆ˜ì§‘ ê²°ê³¼ ë¡œë“œ
    result_path = "/Users/kevin/bigdata/data/output/bypass_test_results.json"
    
    try:
        with open(result_path, 'r', encoding='utf-8') as f:
            test_data = json.load(f)
    except FileNotFoundError:
        logger.error("âŒ ì´ì „ ìˆ˜ì§‘ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € simple_bypass_test.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return
    
    # Stack Overflow ë°ì´í„°ì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ
    so_data = test_data.get('detailed_results', {}).get('stackoverflow', {}).get('data', [])
    all_images = []
    
    logger.info(f"ğŸ“š Stack Overflow ë°ì´í„° {len(so_data)}ê°œì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ ì¤‘...")
    
    for i, item in enumerate(so_data, 1):
        logger.info(f"[{i}/{len(so_data)}] ì§ˆë¬¸ ë¶„ì„: {item.get('title', 'Unknown')[:60]}...")
        
        # ì§ˆë¬¸ ë³¸ë¬¸ì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ
        question_body = item.get('body_markdown', '') + ' ' + item.get('body', '')
        question_images = extract_images_from_html(question_body)
        
        # ë‹µë³€ì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ
        answer_images = []
        if 'accepted_answer' in item:
            answer_body = item['accepted_answer'].get('body', '')
            answer_images = extract_images_from_html(answer_body)
        
        total_images = question_images + answer_images
        
        if total_images:
            logger.info(f"   ğŸ“¸ ë°œê²¬ëœ ì´ë¯¸ì§€: {len(total_images)}ê°œ")
            for img_url in total_images:
                logger.info(f"      â€¢ {img_url}")
                all_images.append({
                    'url': img_url,
                    'source': 'stackoverflow',
                    'question_id': item.get('question_id'),
                    'title': item.get('title', '')
                })
        else:
            logger.info("   ğŸ“¸ ì´ë¯¸ì§€ ì—†ìŒ")
    
    logger.info(f"\nğŸ¯ ì´ ì¶”ì¶œëœ ì´ë¯¸ì§€: {len(all_images)}ê°œ")
    
    if not all_images:
        logger.warning("âš ï¸  í…ŒìŠ¤íŠ¸í•  ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì´ë¯¸ì§€ ì²˜ë¦¬ ë° 403 ìš°íšŒ í…ŒìŠ¤íŠ¸
    logger.info("\nğŸ–¼ï¸  403 ìš°íšŒ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    logger.info("-" * 70)
    
    # Cache ë° processor ì´ˆê¸°í™”
    local_cache = LocalCache(db_path=Path("/tmp/image_extraction_test.db"))
    cache = APICache(local_cache)
    processor = ImageProcessor(cache)
    
    success_count = 0
    results = []
    
    for i, img_info in enumerate(all_images, 1):
        img_url = img_info['url']
        logger.info(f"[{i}/{len(all_images)}] ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸: {img_url}")
        
        try:
            # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° ì²˜ë¦¬
            result = await processor.process_image_url(img_url, ['excel'])
            
            if result and result.get('success'):
                success_count += 1
                results.append({
                    'url': img_url,
                    'success': True,
                    'processing_tier': result.get('processing_tier', ''),
                    'content_length': len(result.get('extracted_content', '')),
                    'source_info': img_info
                })
                
                logger.info(f"   âœ… ì„±ê³µ! ì²˜ë¦¬ ë°©ë²•: {result.get('processing_tier', 'Unknown')}")
                if result.get('extracted_content'):
                    preview = result['extracted_content'][:80].replace('\n', ' ')
                    logger.info(f"   ğŸ“ ì¶”ì¶œ ë‚´ìš©: {preview}...")
            else:
                results.append({
                    'url': img_url,
                    'success': False,
                    'error': result.get('error', 'Unknown'),
                    'source_info': img_info
                })
                logger.error(f"   âŒ ì‹¤íŒ¨: {result.get('error', 'Unknown')}")
                
        except Exception as e:
            logger.error(f"   âŒ ì˜ˆì™¸: {e}")
            results.append({
                'url': img_url,
                'success': False,
                'error': str(e),
                'source_info': img_info
            })
        
        logger.info("-" * 40)
    
    # ê²°ê³¼ ì •ë¦¬
    success_rate = (success_count / len(all_images) * 100) if all_images else 0
    
    # Stack Overflow ì´ë¯¸ì§€ë³„ ì„±ê³µë¥ 
    so_success = sum(1 for r in results if r.get('success', False))
    so_total = len(results)
    
    final_report = {
        "image_extraction_results": {
            "total_images_found": len(all_images),
            "total_tested": so_total,
            "successful_downloads": so_success,
            "success_rate": success_rate,
            "stackoverflow_effectiveness": {
                "images_tested": so_total,
                "images_successful": so_success,
                "success_rate": (so_success / so_total * 100) if so_total > 0 else 0
            }
        },
        "detailed_results": results,
        "image_sources": all_images
    }
    
    # ê²°ê³¼ ì €ì¥
    output_path = "/Users/kevin/bigdata/data/output/image_bypass_test_results.json"
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(final_report, f, indent=2, ensure_ascii=False)
    
    # ìµœì¢… ë³´ê³ ì„œ
    logger.info("=" * 70)
    logger.info("ğŸ“Š ì´ë¯¸ì§€ ì¶”ì¶œ ë° 403 ìš°íšŒ ìµœì¢… ê²°ê³¼:")
    logger.info(f"   â€¢ ì´ ì´ë¯¸ì§€ ë°œê²¬: {final_report['image_extraction_results']['total_images_found']}ê°œ")
    logger.info(f"   â€¢ í…ŒìŠ¤íŠ¸ëœ ì´ë¯¸ì§€: {final_report['image_extraction_results']['total_tested']}ê°œ")
    logger.info(f"   â€¢ ì„±ê³µí•œ ë‹¤ìš´ë¡œë“œ: {final_report['image_extraction_results']['successful_downloads']}ê°œ")
    logger.info(f"   â€¢ ì „ì²´ ì„±ê³µë¥ : {final_report['image_extraction_results']['success_rate']:.1f}%")
    
    logger.info(f"\nğŸ¯ Stack Overflow 403 ìš°íšŒ ì„±ê³¼:")
    logger.info(f"   ğŸ“š ì„±ê³µë¥ : {final_report['image_extraction_results']['stackoverflow_effectiveness']['success_rate']:.1f}%")
    logger.info(f"   ğŸ“š ì„±ê³µ/ì „ì²´: {final_report['image_extraction_results']['stackoverflow_effectiveness']['images_successful']}/{final_report['image_extraction_results']['stackoverflow_effectiveness']['images_tested']}")
    
    logger.info(f"\nğŸ’¾ ìƒì„¸ ê²°ê³¼ ì €ì¥: {output_path}")
    
    if success_rate >= 80:
        logger.info("ğŸ‰ 403 ìš°íšŒ ì„±ê³µ! ëª©í‘œ ë‹¬ì„±!")
    elif success_rate >= 60:
        logger.info("âœ… 403 ìš°íšŒ ì–‘í˜¸í•œ ì„±ê³¼!")
    elif success_rate >= 40:
        logger.warning("âš ï¸  403 ìš°íšŒ ë¶€ë¶„ì  ì„±ê³µ")
    else:
        logger.error("âŒ 403 ìš°íšŒ ê°œì„  í•„ìš”")
    
    return final_report

if __name__ == "__main__":
    result = asyncio.run(test_image_extraction_and_bypass())