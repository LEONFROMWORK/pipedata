"""
Continuous Data Collection System
ì‚¬ìš©ìžê°€ ì •ì§€í•  ë•Œê¹Œì§€ ì§€ì†ì ìœ¼ë¡œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ì‹œìŠ¤í…œ
"""
import asyncio
import logging
import time
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
from pathlib import Path
import json

from pipeline.main_pipeline import ExcelQAPipeline

logger = logging.getLogger('pipeline.continuous_collector')

class ContinuousCollector:
    """
    ì§€ì†ì  ë°ì´í„° ìˆ˜ì§‘ê¸°
    
    ê¸°ëŠ¥:
    1. ì •ì§€ ì‹ í˜¸ê°€ ì˜¬ ë•Œê¹Œì§€ ê³„ì† ìˆ˜ì§‘
    2. ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë°ì´í„° ìˆ˜ì§‘ ë° ì €ìž¥
    3. ì‹¤ì‹œê°„ ìƒíƒœ ì—…ë°ì´íŠ¸
    4. ìˆ˜ì§‘ëœ ë°ì´í„°ì˜ ëˆ„ì  í†µê³„
    """
    
    def __init__(self):
        self.pipeline = ExcelQAPipeline()
        self.is_running = False
        self.total_collected = 0
        self.total_processed = 0
        self.total_final = 0
        self.batch_count = 0
        self.start_time = None
        self.datasets_generated = []
        
        # ë°°ì¹˜ ì„¤ì •
        self.batch_size = 5  # í•œ ë²ˆì— ìˆ˜ì§‘í•  í•­ëª© ìˆ˜
        self.batch_interval = 10  # ë°°ì¹˜ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
        
    async def start_continuous_collection(self, sources: List[str] = None, 
                                        max_per_batch: int = 5) -> Dict[str, Any]:
        """ì§€ì†ì  ìˆ˜ì§‘ ì‹œìž‘"""
        if sources is None:
            sources = ['reddit']  # ê¸°ë³¸ì ìœ¼ë¡œ Redditë§Œ ìˆ˜ì§‘
            
        self.is_running = True
        self.start_time = datetime.now()
        self.batch_size = max_per_batch
        
        logger.info(f"Starting continuous collection: sources={sources}, batch_size={max_per_batch}")
        
        total_collected = 0
        total_processed = 0  
        total_final = 0
        
        try:
            batch_num = 1
            while self.is_running:
                logger.info(f"Starting batch {batch_num} collection...")
                
                # ðŸ”§ Pipeline State ë¦¬ì…‹ (ë¬´í•œ ëˆ„ì  ë¬¸ì œ ìˆ˜ì •)
                from pipeline.main_pipeline import PipelineState
                self.pipeline.state = PipelineState()
                logger.debug(f"Pipeline state reset for batch {batch_num}")
                
                # ë°°ì¹˜ ìˆ˜ì§‘ ì‹¤í–‰
                batch_result = await self.pipeline.run_full_pipeline(
                    from_date=None,  # ìµœì‹  ë°ì´í„°
                    max_pages=2,     # ì ì€ íŽ˜ì´ì§€ë¡œ ë¹ ë¥¸ ìˆ˜ì§‘
                    target_count=max_per_batch,
                    sources=sources
                )
                
                # ê²°ê³¼ ëˆ„ì 
                if batch_result and 'data_flow' in batch_result:
                    data_flow = batch_result['data_flow']
                    total_collected += data_flow.get('collected', 0)
                    total_processed += data_flow.get('processed', 0)
                    total_final += data_flow.get('final_output', 0)
                    
                    # ìƒì„±ëœ ë°ì´í„°ì…‹ ì¶”ê°€
                    if 'dataset_path' in batch_result:
                        self.datasets_generated.append({
                            'batch': batch_num,
                            'path': batch_result['dataset_path'],
                            'timestamp': datetime.now().isoformat(),
                            'items': data_flow.get('final_output', 0)
                        })
                
                self.batch_count = batch_num
                self.total_collected = total_collected
                self.total_processed = total_processed
                self.total_final = total_final
                
                logger.info(f"Batch {batch_num} complete: "
                           f"collected={data_flow.get('collected', 0)}, "
                           f"final={data_flow.get('final_output', 0)}")
                logger.info(f"Cumulative totals: "
                           f"collected={total_collected}, "
                           f"processed={total_processed}, "
                           f"final={total_final}")
                
                # ì •ì§€ ì‹ í˜¸ í™•ì¸
                if not self.is_running:
                    logger.info("Stop signal received, ending collection")
                    break
                
                # ë‹¤ìŒ ë°°ì¹˜ê¹Œì§€ ëŒ€ê¸°
                logger.info(f"Waiting {self.batch_interval} seconds before next batch...")
                await asyncio.sleep(self.batch_interval)
                
                batch_num += 1
                
        except Exception as e:
            logger.error(f"Continuous collection error: {e}")
            
        finally:
            await self.pipeline.so_collector.close()
            
        # ìµœì¢… ê²°ê³¼ ë°˜í™˜
        return self._create_final_result()
    
    def stop_collection(self):
        """ìˆ˜ì§‘ ì •ì§€"""
        self.is_running = False
        logger.info("Continuous collection stop requested")
        
    def get_status(self) -> Dict[str, Any]:
        """í˜„ìž¬ ìƒíƒœ ë°˜í™˜"""
        if not self.start_time:
            return {"status": "not_started"}
            
        elapsed = (datetime.now() - self.start_time).total_seconds()
        
        return {
            "status": "running" if self.is_running else "stopped",
            "elapsed_time": elapsed,
            "batch_count": self.batch_count,
            "total_collected": self.total_collected,
            "total_processed": self.total_processed,
            "total_final": self.total_final,
            "datasets_generated": len(self.datasets_generated),
            "collection_rate": self.total_final / (elapsed / 60) if elapsed > 0 else 0  # items per minute
        }
    
    def _create_final_result(self) -> Dict[str, Any]:
        """ìµœì¢… ê²°ê³¼ ìƒì„±"""
        elapsed = (datetime.now() - self.start_time).total_seconds() if self.start_time else 0
        
        return {
            "status": "completed",
            "execution_summary": {
                "total_batches": self.batch_count,
                "total_execution_time": elapsed,
                "final_status": "stopped" if not self.is_running else "completed"
            },
            "cumulative_data_flow": {
                "total_collected": self.total_collected,
                "total_processed": self.total_processed,
                "total_final": self.total_final
            },
            "datasets_generated": self.datasets_generated,
            "performance_metrics": {
                "items_per_minute": self.total_final / (elapsed / 60) if elapsed > 0 else 0,
                "batches_per_hour": self.batch_count / (elapsed / 3600) if elapsed > 0 else 0,
                "success_rate": self.total_final / self.total_collected * 100 if self.total_collected > 0 else 0
            }
        }