#!/usr/bin/env python3
"""
ì˜¤ë¹ ë‘ í†µí•© í…ŒìŠ¤íŠ¸
- í¬ë¡¤ë§ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
- í’ˆì§ˆ í‰ê°€ í…ŒìŠ¤íŠ¸  
- ë°ì´í„°ì…‹ ìƒì„± í…ŒìŠ¤íŠ¸
"""

import asyncio
import logging
from pathlib import Path
import sys
import json

# Add project root to path
sys.path.append(str(Path(__file__).parent))

from collectors.oppadu_crawler import OppaduCrawler
from quality.korean_oppadu_scorer import KoreanOppaduScorer
from output.oppadu_dataset_generator import OppaduDatasetGenerator
from core.cache import APICache, LocalCache

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def test_oppadu_integration():
    """ì˜¤ë¹ ë‘ í†µí•© í…ŒìŠ¤íŠ¸"""
    
    logger.info("ğŸ‡°ğŸ‡· ì˜¤ë¹ ë‘ í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
    logger.info("=" * 50)
    
    # Cache ì´ˆê¸°í™”
    local_cache = LocalCache(db_path=Path("/tmp/oppadu_test.db"))
    cache = APICache(local_cache)
    
    try:
        # 1. ì˜¤ë¹ ë‘ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸
        logger.info("ğŸ“¡ 1ë‹¨ê³„: ì˜¤ë¹ ë‘ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸")
        crawler = OppaduCrawler(cache)
        
        # ë§¤ìš° ì œí•œì ì¸ í…ŒìŠ¤íŠ¸ (1í˜ì´ì§€ë§Œ)
        oppadu_data = await crawler.collect_oppadu_questions(max_pages=1)
        
        if oppadu_data:
            logger.info(f"   âœ… í¬ë¡¤ë§ ì„±ê³µ: {len(oppadu_data)}ê°œ í•­ëª© ìˆ˜ì§‘")
            
            # ì²« ë²ˆì§¸ í•­ëª© ì •ë³´ ì¶œë ¥
            first_item = oppadu_data[0]
            logger.info(f"   ğŸ“ ìƒ˜í”Œ ì œëª©: {first_item.get('title', '')[:50]}...")
            logger.info(f"   ğŸ”§ Excel ë²„ì „: {first_item.get('metadata', {}).get('excel_version', 'N/A')}")
            logger.info(f"   ğŸ’» OS ë²„ì „: {first_item.get('metadata', {}).get('os_version', 'N/A')}")
            
        else:
            logger.warning("   âš ï¸ í¬ë¡¤ë§ ì‹¤íŒ¨ ë˜ëŠ” ë°ì´í„° ì—†ìŒ")
            return
        
        # 2. í•œêµ­ í’ˆì§ˆ í‰ê°€ í…ŒìŠ¤íŠ¸
        logger.info("\nğŸ¯ 2ë‹¨ê³„: í•œêµ­ í’ˆì§ˆ í‰ê°€ í…ŒìŠ¤íŠ¸")
        scorer = KoreanOppaduScorer()
        
        quality_results = scorer.score_batch(oppadu_data)
        
        if quality_results:
            logger.info(f"   âœ… í’ˆì§ˆ í‰ê°€ ì™„ë£Œ: {len(quality_results)}ê°œ í•­ëª©")
            
            # í†µê³„ ì •ë³´
            stats = scorer.get_batch_statistics(quality_results)
            logger.info(f"   ğŸ“Š í‰ê·  ì ìˆ˜: {stats.get('average_score', 0):.2f}")
            logger.info(f"   ğŸ¢ í•œêµ­ ë¹„ì¦ˆë‹ˆìŠ¤ í¬ìŠ¤íŠ¸: {stats.get('korean_business_posts', 0)}ê°œ")
            logger.info(f"   ğŸ”§ ê³ ê¸‰ í¬ìŠ¤íŠ¸: {stats.get('advanced_posts', 0)}ê°œ")
            
            # í’ˆì§ˆ í•„í„°ë§
            threshold = 5.5
            filtered_data = scorer.filter_by_quality(oppadu_data, quality_results, threshold)
            logger.info(f"   ğŸ” í’ˆì§ˆ í•„í„°ë§ (ì„ê³„ê°’ {threshold}): {len(filtered_data)}ê°œ í†µê³¼")
            
        else:
            logger.warning("   âš ï¸ í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨")
            return
        
        # 3. í•œêµ­ ë°ì´í„°ì…‹ ìƒì„± í…ŒìŠ¤íŠ¸
        logger.info("\nğŸ“ 3ë‹¨ê³„: í•œêµ­ ë°ì´í„°ì…‹ ìƒì„± í…ŒìŠ¤íŠ¸")
        generator = OppaduDatasetGenerator()
        
        if filtered_data:
            dataset_path = generator.generate_oppadu_dataset(
                filtered_data,
                metadata={'test_execution': True}
            )
            
            if dataset_path and Path(dataset_path).exists():
                logger.info(f"   âœ… ë°ì´í„°ì…‹ ìƒì„± ì„±ê³µ: {dataset_path}")
                
                # íŒŒì¼ ê²€ì¦
                validation_result = generator.validate_korean_dataset(dataset_path)
                logger.info(f"   ğŸ” ê²€ì¦ ê²°ê³¼: {validation_result.get('valid_lines', 0)}ê°œ ìœ íš¨ ë¼ì¸")
                logger.info(f"   ğŸ‡°ğŸ‡· í•œêµ­ì–´ ì½˜í…ì¸ : {validation_result.get('korean_content_lines', 0)}ê°œ ë¼ì¸ ({validation_result.get('korean_content_percentage', 0):.1f}%)")
                
                # ìƒ˜í”Œ ë°ì´í„° ì¶œë ¥
                with open(dataset_path, 'r', encoding='utf-8') as f:
                    first_line = f.readline().strip()
                    if first_line:
                        sample = json.loads(first_line)
                        logger.info(f"   ğŸ“ ìƒ˜í”Œ ì§ˆë¬¸: {sample.get('user_question', '')[:50]}...")
                        logger.info(f"   ğŸ’¡ ë¹„ì¦ˆë‹ˆìŠ¤ ë„ë©”ì¸: {sample.get('metadata', {}).get('business_domain', 'N/A')}")
                        logger.info(f"   ğŸ”§ Excel í•¨ìˆ˜: {len(sample.get('metadata', {}).get('functions', []))}ê°œ")
                
            else:
                logger.warning("   âš ï¸ ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨")
                return
        
        logger.info("\nğŸ‰ ì˜¤ë¹ ë‘ í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        logger.info("=" * 50)
        
        # ìš”ì•½ ì •ë³´
        logger.info("ğŸ“Š í…ŒìŠ¤íŠ¸ ìš”ì•½:")
        logger.info(f"   í¬ë¡¤ë§: {len(oppadu_data)}ê°œ ìˆ˜ì§‘")
        logger.info(f"   í’ˆì§ˆ í‰ê°€: {len(quality_results)}ê°œ í‰ê°€")
        logger.info(f"   ìµœì¢… ë°ì´í„°: {len(filtered_data)}ê°œ í†µê³¼")
        logger.info(f"   ë°ì´í„°ì…‹: {dataset_path}")
        
        return {
            'crawled_count': len(oppadu_data),
            'quality_assessed': len(quality_results),
            'final_count': len(filtered_data),
            'dataset_path': dataset_path
        }
        
    except Exception as e:
        logger.error(f"âŒ ì˜¤ë¹ ë‘ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        logger.error(f"Traceback: {traceback.format_exc()}")
        return None

if __name__ == "__main__":
    asyncio.run(test_oppadu_integration())