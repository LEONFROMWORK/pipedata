#!/usr/bin/env python3
"""
403 ìš°íšŒ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ë§Œ - ë‹¤ìš´ë¡œë“œ ì„±ê³µë¥  ì¸¡ì •
"""
import asyncio
import json
import logging
import re
from pathlib import Path
from typing import List

# Add project root to path
import sys
sys.path.append(str(Path(__file__).parent))

from processors.image_processor import ImageProcessor
from core.cache import APICache, LocalCache

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

def extract_images_from_html(html_content: str) -> List[str]:
    """HTML ì»¨í…ì¸ ì—ì„œ ì´ë¯¸ì§€ URL ì¶”ì¶œ"""
    image_urls = []
    
    # <img src="..."> íŒ¨í„´
    img_pattern = r'<img[^>]+src=["\']([^"\']+)["\'][^>]*>'
    img_matches = re.findall(img_pattern, html_content, re.IGNORECASE)
    image_urls.extend(img_matches)
    
    # <a href="..."> íŒ¨í„´ (ì´ë¯¸ì§€ ë§í¬)
    link_pattern = r'<a[^>]+href=["\']([^"\']+\.(?:png|jpg|jpeg|gif|webp|svg)(?:\?[^"\']*)?)["\'][^>]*>'
    link_matches = re.findall(link_pattern, html_content, re.IGNORECASE)
    image_urls.extend(link_matches)
    
    # ë§ˆí¬ë‹¤ìš´ ìŠ¤íƒ€ì¼ ì´ë¯¸ì§€ ë§í¬ ![alt](url)
    markdown_pattern = r'!\[.*?\]\(([^)]+)\)'
    markdown_matches = re.findall(markdown_pattern, html_content)
    image_urls.extend(markdown_matches)
    
    # ì§ì ‘ URL íŒ¨í„´ (http://...image.png)
    direct_pattern = r'https?://[^\s<>"\']+\.(?:png|jpg|jpeg|gif|webp|svg)(?:\?[^\s<>"\']*)?'
    direct_matches = re.findall(direct_pattern, html_content, re.IGNORECASE)
    image_urls.extend(direct_matches)
    
    # ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
    unique_urls = list(set(image_urls))
    
    # ë¹ˆ URLì´ë‚˜ í”„ë¡œí•„ ì´ë¯¸ì§€ ì œì™¸
    filtered_urls = []
    for url in unique_urls:
        if url and 'gravatar.com' not in url and len(url.strip()) > 10:
            # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
            if url.startswith('//'):
                url = 'https:' + url
            elif url.startswith('/'):
                url = 'https://i.sstatic.net' + url
                
            filtered_urls.append(url.strip())
    
    return filtered_urls

async def test_download_only():
    """403 ìš°íšŒ ë‹¤ìš´ë¡œë“œ ì„±ê³µë¥ ë§Œ í…ŒìŠ¤íŠ¸"""
    
    logger.info("ğŸš€ 403 ìš°íšŒ ë‹¤ìš´ë¡œë“œ ì„±ê³µë¥  í…ŒìŠ¤íŠ¸")
    logger.info("=" * 70)
    
    # ì´ì „ ìˆ˜ì§‘ ê²°ê³¼ ë¡œë“œ
    result_path = "/Users/kevin/bigdata/data/output/bypass_test_results.json"
    
    try:
        with open(result_path, 'r', encoding='utf-8') as f:
            test_data = json.load(f)
    except FileNotFoundError:
        logger.error("âŒ ì´ì „ ìˆ˜ì§‘ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # Stack Overflow ë°ì´í„°ì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ
    so_data = test_data.get('detailed_results', {}).get('stackoverflow', {}).get('data', [])
    all_images = []
    
    logger.info(f"ğŸ“š Stack Overflow ë°ì´í„°ì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ...")
    
    for item in so_data:
        # ì§ˆë¬¸ ë³¸ë¬¸ì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ
        question_body = item.get('body_markdown', '') + ' ' + item.get('body', '')
        question_images = extract_images_from_html(question_body)
        
        # ë‹µë³€ì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ
        answer_images = []
        if 'accepted_answer' in item:
            answer_body = item['accepted_answer'].get('body', '')
            answer_images = extract_images_from_html(answer_body)
        
        total_images = question_images + answer_images
        all_images.extend(total_images)
    
    # ì¤‘ë³µ ì œê±°
    unique_images = list(set(all_images))
    
    # Reddit í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ì¶”ê°€
    reddit_test_urls = [
        'https://preview.redd.it/76mukstfxhdf1.png',  # ì•Œë ¤ì§„ ì‘ë™ URL
    ]
    
    all_test_images = unique_images + reddit_test_urls
    
    logger.info(f"ğŸ¯ í…ŒìŠ¤íŠ¸í•  ì´ë¯¸ì§€: {len(all_test_images)}ê°œ")
    logger.info(f"   â€¢ Stack Overflow: {len(unique_images)}ê°œ")
    logger.info(f"   â€¢ Reddit: {len(reddit_test_urls)}ê°œ")
    
    # Cache ë° processor ì´ˆê¸°í™”
    local_cache = LocalCache(db_path=Path("/tmp/bypass_test.db"))
    cache = APICache(local_cache)
    processor = ImageProcessor(cache)
    
    download_results = []
    stackoverflow_success = 0
    reddit_success = 0
    
    for i, img_url in enumerate(all_test_images, 1):
        logger.info(f"[{i}/{len(all_test_images)}] ë‹¤ìš´ë¡œë“œ í…ŒìŠ¤íŠ¸: {img_url}")
        
        try:
            # ë‹¤ìš´ë¡œë“œë§Œ í…ŒìŠ¤íŠ¸ (ì²˜ë¦¬ëŠ” ê±´ë„ˆë›°ê¸°)
            downloaded_path = await processor._download_image(img_url)
            
            if downloaded_path and Path(downloaded_path).exists():
                file_size = Path(downloaded_path).stat().st_size
                
                # ì†ŒìŠ¤ íŒë³„
                is_reddit = 'redd.it' in img_url or 'reddit' in img_url
                is_stackoverflow = 'sstatic.net' in img_url
                
                if is_stackoverflow:
                    stackoverflow_success += 1
                elif is_reddit:
                    reddit_success += 1
                
                download_results.append({
                    'url': img_url,
                    'success': True,
                    'file_size': file_size,
                    'source': 'reddit' if is_reddit else 'stackoverflow'
                })
                
                logger.info(f"   âœ… ì„±ê³µ! íŒŒì¼ í¬ê¸°: {file_size:,} bytes")
                
                # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                if Path(downloaded_path).exists():
                    Path(downloaded_path).unlink()
            else:
                download_results.append({
                    'url': img_url,
                    'success': False,
                    'error': 'Download failed',
                    'source': 'reddit' if 'redd.it' in img_url else 'stackoverflow'
                })
                logger.error(f"   âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
                
        except Exception as e:
            download_results.append({
                'url': img_url,
                'success': False,
                'error': str(e),
                'source': 'reddit' if 'redd.it' in img_url else 'stackoverflow'
            })
            logger.error(f"   âŒ ì˜ˆì™¸: {e}")
        
        logger.info("-" * 40)
    
    # í†µê³„ ê³„ì‚°
    total_success = sum(1 for r in download_results if r['success'])
    total_tested = len(download_results)
    overall_success_rate = (total_success / total_tested * 100) if total_tested > 0 else 0
    
    so_total = len(unique_images)
    so_rate = (stackoverflow_success / so_total * 100) if so_total > 0 else 0
    
    reddit_total = len(reddit_test_urls)
    reddit_rate = (reddit_success / reddit_total * 100) if reddit_total > 0 else 0
    
    # ê²°ê³¼ ì •ë¦¬
    final_report = {
        "bypass_test_summary": {
            "total_images_tested": total_tested,
            "total_downloads_successful": total_success,
            "overall_success_rate": overall_success_rate,
            "stackoverflow_results": {
                "images_tested": so_total,
                "images_successful": stackoverflow_success,
                "success_rate": so_rate
            },
            "reddit_results": {
                "images_tested": reddit_total,
                "images_successful": reddit_success,
                "success_rate": reddit_rate
            }
        },
        "download_details": download_results
    }
    
    # ê²°ê³¼ ì €ì¥
    output_path = "/Users/kevin/bigdata/data/output/bypass_effectiveness_test.json"
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(final_report, f, indent=2, ensure_ascii=False)
    
    # ìµœì¢… ë³´ê³ ì„œ
    logger.info("=" * 70)
    logger.info("ğŸ“Š 403 ìš°íšŒ ë‹¤ìš´ë¡œë“œ ì„±ê³µë¥  ìµœì¢… ê²°ê³¼:")
    logger.info(f"   â€¢ ì´ í…ŒìŠ¤íŠ¸: {final_report['bypass_test_summary']['total_images_tested']}ê°œ")
    logger.info(f"   â€¢ ì„±ê³µí•œ ë‹¤ìš´ë¡œë“œ: {final_report['bypass_test_summary']['total_downloads_successful']}ê°œ")
    logger.info(f"   â€¢ ì „ì²´ ì„±ê³µë¥ : {final_report['bypass_test_summary']['overall_success_rate']:.1f}%")
    
    logger.info(f"\nğŸ¯ ì†ŒìŠ¤ë³„ 403 ìš°íšŒ ì„±ê³¼:")
    logger.info(f"   ğŸ“š Stack Overflow: {final_report['bypass_test_summary']['stackoverflow_results']['success_rate']:.1f}% ({final_report['bypass_test_summary']['stackoverflow_results']['images_successful']}/{final_report['bypass_test_summary']['stackoverflow_results']['images_tested']})")
    logger.info(f"   ğŸŸ  Reddit: {final_report['bypass_test_summary']['reddit_results']['success_rate']:.1f}% ({final_report['bypass_test_summary']['reddit_results']['images_successful']}/{final_report['bypass_test_summary']['reddit_results']['images_tested']})")
    
    logger.info(f"\nğŸ’¾ ìƒì„¸ ê²°ê³¼ ì €ì¥: {output_path}")
    
    if overall_success_rate >= 80:
        logger.info("ğŸ‰ 403 ìš°íšŒ ì„±ê³µ! ëª©í‘œ ë‹¬ì„±!")
    elif overall_success_rate >= 60:
        logger.info("âœ… 403 ìš°íšŒ ì–‘í˜¸í•œ ì„±ê³¼!")
    elif overall_success_rate >= 40:
        logger.warning("âš ï¸  403 ìš°íšŒ ë¶€ë¶„ì  ì„±ê³µ")
    else:
        logger.error("âŒ 403 ìš°íšŒ ê°œì„  í•„ìš”")
    
    return final_report

if __name__ == "__main__":
    result = asyncio.run(test_download_only())