#!/usr/bin/env python3
"""
ê¸°ì¡´ ìˆ˜ì§‘ëœ ë°ì´í„° ë¶„ì„ ë° ìºì‹œ ë°ì´í„° ê²€í† 
"""
import asyncio
import json
import sys
import sqlite3
from pathlib import Path
from datetime import datetime

sys.path.insert(0, str(Path(__file__).parent))

from config import Config
from core.cache import LocalCache, APICache
from collectors.stackoverflow_collector import StackOverflowCollector

def analyze_cache_data():
    """ìºì‹œëœ ë°ì´í„° ë¶„ì„"""
    print("ğŸ—„ï¸ ìºì‹œ ë°ì´í„° ë¶„ì„")
    print("=" * 50)
    
    try:
        # SQLite ìºì‹œ ë°ì´í„°ë² ì´ìŠ¤ ì§ì ‘ ì¡°íšŒ
        cache_db_path = Config.DATABASE_PATH
        print(f"ìºì‹œ DB ê²½ë¡œ: {cache_db_path}")
        
        if not cache_db_path.exists():
            print("âŒ ìºì‹œ ë°ì´í„°ë² ì´ìŠ¤ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return
        
        with sqlite3.connect(cache_db_path) as conn:
            # ìºì‹œ í…Œì´ë¸” ì¡°íšŒ
            cursor = conn.execute("SELECT key, value, expires_at, created_at FROM cache")
            cache_entries = cursor.fetchall()
            
            print(f"ğŸ“Š ìºì‹œ ì—”íŠ¸ë¦¬ ìˆ˜: {len(cache_entries)}")
            
            for key, value_str, expires_at, created_at in cache_entries:
                created_time = datetime.fromtimestamp(created_at)
                expires_time = datetime.fromtimestamp(expires_at)
                
                print(f"\nğŸ”‘ ìºì‹œ í‚¤: {key}")
                print(f"   ìƒì„± ì‹œê°„: {created_time}")
                print(f"   ë§Œë£Œ ì‹œê°„: {expires_time}")
                
                try:
                    # JSON ë°ì´í„° íŒŒì‹±
                    data = json.loads(value_str)
                    
                    if 'items' in data:
                        items = data['items']
                        print(f"   ë°ì´í„° í•­ëª© ìˆ˜: {len(items)}")
                        
                        # Stack Overflow ì§ˆë¬¸ ë°ì´í„°ì¸ ê²½ìš° ìƒì„¸ ë¶„ì„
                        if len(items) > 0 and 'question_id' in items[0]:
                            print(f"   ğŸ“ Stack Overflow ì§ˆë¬¸ ë°ì´í„°:")
                            analyze_stackoverflow_questions(items)
                        elif len(items) > 0 and 'answer_id' in items[0]:
                            print(f"   ğŸ’¬ Stack Overflow ë‹µë³€ ë°ì´í„°:")
                            analyze_stackoverflow_answers(items)
                    else:
                        print(f"   ë°ì´í„° íƒ€ì…: {type(data)}")
                        
                except json.JSONDecodeError:
                    print(f"   âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨")
                except Exception as e:
                    print(f"   âŒ ë°ì´í„° ë¶„ì„ ì˜¤ë¥˜: {e}")
                    
    except Exception as e:
        print(f"âŒ ìºì‹œ ë¶„ì„ ì˜¤ë¥˜: {e}")

def analyze_stackoverflow_questions(questions):
    """Stack Overflow ì§ˆë¬¸ ë°ì´í„° ë¶„ì„"""
    print(f"      ì§ˆë¬¸ ìˆ˜: {len(questions)}")
    
    # ìƒ˜í”Œ ì§ˆë¬¸ ì •ë³´
    for i, q in enumerate(questions[:3], 1):
        print(f"      ì§ˆë¬¸ {i}:")
        print(f"         ID: {q.get('question_id')}")
        print(f"         ì œëª©: {q.get('title', 'N/A')[:80]}...")
        print(f"         ì ìˆ˜: {q.get('score', 0)}")
        print(f"         ë‹µë³€ë¨: {q.get('is_answered', False)}")
        print(f"         ì±„íƒë‹µë³€ ID: {q.get('accepted_answer_id', 'N/A')}")
        print(f"         íƒœê·¸: {q.get('tags', [])}")

def analyze_stackoverflow_answers(answers):
    """Stack Overflow ë‹µë³€ ë°ì´í„° ë¶„ì„"""
    print(f"      ë‹µë³€ ìˆ˜: {len(answers)}")
    
    for i, a in enumerate(answers[:3], 1):
        print(f"      ë‹µë³€ {i}:")
        print(f"         ID: {a.get('answer_id')}")
        print(f"         ì ìˆ˜: {a.get('score', 0)}")
        body = a.get('body_markdown', a.get('body', ''))
        print(f"         ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {body[:100]}...")

async def force_new_collection():
    """ìºì‹œë¥¼ ë¬´ì‹œí•˜ê³  ìƒˆë¡œìš´ ë°ì´í„° ìˆ˜ì§‘"""
    print("\nğŸ”„ ìƒˆë¡œìš´ ë°ì´í„° ìˆ˜ì§‘ (ìºì‹œ ë¬´ì‹œ)")
    print("=" * 50)
    
    try:
        # ìºì‹œ ì´ˆê¸°í™” (ê¸°ì¡´ ìºì‹œ ì§€ìš°ê¸°)
        local_cache = LocalCache(Config.DATABASE_PATH)
        
        # ìºì‹œ ì •ë¦¬
        cleaned = local_cache.cleanup_expired()
        print(f"ì •ë¦¬ëœ ë§Œë£Œ ìºì‹œ í•­ëª©: {cleaned}")
        
        # ëª¨ë“  ìºì‹œ ì‚­ì œ (ê°•ì œ)
        with sqlite3.connect(Config.DATABASE_PATH) as conn:
            conn.execute("DELETE FROM cache WHERE key LIKE 'so_api:%'")
            conn.commit()
            print("âœ… Stack Overflow API ìºì‹œ ì‚­ì œ ì™„ë£Œ")
        
        api_cache = APICache(local_cache)
        collector = StackOverflowCollector(api_cache)
        
        # ìƒˆë¡œìš´ ìˆ˜ì§‘ ì‹¤í–‰
        from datetime import timedelta
        from_date = datetime.now() - timedelta(days=7)  # ìµœê·¼ 1ì£¼ì¼
        
        print(f"ğŸ“… ìˆ˜ì§‘ ê¸°ê°„: {from_date.strftime('%Y-%m-%d')} ~ í˜„ì¬")
        
        questions = await collector.collect_excel_questions(
            from_date=from_date,
            max_pages=2
        )
        
        print(f"\nğŸ“Š ìƒˆë¡œ ìˆ˜ì§‘ëœ ë°ì´í„°:")
        print(f"   ì´ ì§ˆë¬¸ ìˆ˜: {len(questions)}")
        
        if questions:
            # ìˆ˜ì§‘ëœ ë°ì´í„° ìƒì„¸ ë¶„ì„
            print(f"\nğŸ“ ìƒì„¸ ë¶„ì„:")
            
            # íƒœê·¸ ë¶„ì„
            all_tags = []
            for q in questions:
                all_tags.extend(q.get('tags', []))
            
            unique_tags = list(set(all_tags))
            print(f"   ì‚¬ìš©ëœ íƒœê·¸: {unique_tags}")
            
            # ì ìˆ˜ ë¶„ì„
            scores = [q.get('score', 0) for q in questions]
            print(f"   ì ìˆ˜ ë²”ìœ„: {min(scores)} ~ {max(scores)} (í‰ê· : {sum(scores)/len(scores):.1f})")
            
            # Excel ê´€ë ¨ í‚¤ì›Œë“œ ë¶„ì„
            excel_keywords = ['excel', 'formula', 'vlookup', 'index', 'match', 'sum', 'if', 'pivot']
            keyword_counts = {kw: 0 for kw in excel_keywords}
            
            for q in questions:
                text = (q.get('title', '') + ' ' + q.get('body_markdown', '')).lower()
                answer_text = q.get('accepted_answer', {}).get('body_markdown', '').lower()
                full_text = text + ' ' + answer_text
                
                for keyword in excel_keywords:
                    if keyword in full_text:
                        keyword_counts[keyword] += 1
            
            print(f"   Excel í‚¤ì›Œë“œ ë¹ˆë„:")
            for kw, count in sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True):
                if count > 0:
                    print(f"      {kw}: {count}íšŒ")
            
            # ìƒ˜í”Œ ì¶œë ¥
            print(f"\nğŸ“‹ ìƒ˜í”Œ ì§ˆë¬¸:")
            for i, q in enumerate(questions[:2], 1):
                print(f"   {i}. {q.get('title', 'N/A')}")
                print(f"      ì ìˆ˜: {q.get('score')} | íƒœê·¸: {q.get('tags', [])}")
                
                if q.get('accepted_answer'):
                    answer = q['accepted_answer']
                    print(f"      ë‹µë³€ ì ìˆ˜: {answer.get('score')}")
                    answer_preview = answer.get('body_markdown', '')[:200]
                    print(f"      ë‹µë³€ ë¯¸ë¦¬ë³´ê¸°: {answer_preview}...")
                print()
            
            # ë°ì´í„° ì €ì¥
            output_file = Path(Config.OUTPUT_DIR) / f"verified_stackoverflow_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(questions, f, indent=2, ensure_ascii=False, default=str)
            
            print(f"ğŸ’¾ ê²€ì¦ëœ ë°ì´í„° ì €ì¥: {output_file}")
            print(f"   íŒŒì¼ í¬ê¸°: {output_file.stat().st_size:,} bytes")
            
        else:
            print("âš ï¸ ìƒˆë¡œìš´ ë°ì´í„°ê°€ ìˆ˜ì§‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("   - API ì œí•œ í™•ì¸ í•„ìš”")
            print("   - ê²€ìƒ‰ ì¡°ê±´ ì¡°ì • í•„ìš”")
        
        await collector.close()
        
    except Exception as e:
        print(f"âŒ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ Stack Overflow ë°ì´í„° ìˆ˜ì§‘ ìƒíƒœ ì¢…í•© ë¶„ì„")
    print("=" * 60)
    
    # 1. ê¸°ì¡´ ìºì‹œ ë°ì´í„° ë¶„ì„
    analyze_cache_data()
    
    # 2. ìƒˆë¡œìš´ ë°ì´í„° ìˆ˜ì§‘
    await force_new_collection()
    
    print(f"\nâœ… ë¶„ì„ ì™„ë£Œ!")
    print(f"   - ìºì‹œ ë°ì´í„° ê²€í†  ì™„ë£Œ")
    print(f"   - ìƒˆë¡œìš´ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
    print(f"   - ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì™„ë£Œ")

if __name__ == "__main__":
    asyncio.run(main())