#!/usr/bin/env python3
"""
403 ìš°íšŒ ê¸°ëŠ¥ì´ ì ìš©ëœ Stack Overflow + Reddit í†µí•© ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸
ì‹¤ì œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° AI ì²˜ë¦¬ê¹Œì§€ í¬í•¨í•œ ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ ê²€ì¦
"""
import asyncio
import json
import logging
import sys
from pathlib import Path
from datetime import datetime

# Add project root to path
sys.path.append(str(Path(__file__).parent))

from collectors.stackoverflow_collector import StackOverflowCollector
from collectors.reddit_collector import RedditCollector
from pipeline.main_pipeline import ExcelQAPipeline
from processors.image_processor import ImageProcessor
from core.cache import APICache, LocalCache

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

async def test_stackoverflow_with_image_bypass():
    """Stack Overflow ìˆ˜ì§‘ + ì´ë¯¸ì§€ 403 ìš°íšŒ í…ŒìŠ¤íŠ¸"""
    
    logger.info("ğŸ“š Stack Overflow ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ (ì´ë¯¸ì§€ 403 ìš°íšŒ í¬í•¨)")
    logger.info("=" * 70)
    
    # Cache ì´ˆê¸°í™”
    local_cache = LocalCache(db_path=Path("/tmp/so_bypass_test.db"))
    cache = APICache(local_cache)
    
    # Stack Overflow collector ì´ˆê¸°í™”
    so_collector = StackOverflowCollector(cache)
    
    try:
        # ì†ŒëŸ‰ ìˆ˜ì§‘ (í…ŒìŠ¤íŠ¸ìš©)
        so_data = await so_collector.collect_qa_data(
            tags=['excel-formula'], 
            max_questions=3,  # ì ì€ ìˆ˜ëŸ‰ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
            min_score=2
        )
        
        logger.info(f"âœ… Stack Overflow ìˆ˜ì§‘ ì™„ë£Œ: {len(so_data)}ê°œ")
        
        # ì´ë¯¸ì§€ê°€ í¬í•¨ëœ ë°ì´í„° í™•ì¸
        image_count = 0
        for item in so_data:
            if 'images' in item and item['images']:
                image_count += len(item['images'])
                logger.info(f"   ğŸ“¸ ì´ë¯¸ì§€ ë°œê²¬: {len(item['images'])}ê°œ - {item.get('title', 'Unknown')[:50]}...")
        
        logger.info(f"   ì´ ì´ë¯¸ì§€: {image_count}ê°œ")
        return so_data
        
    except Exception as e:
        logger.error(f"âŒ Stack Overflow ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return []

async def test_reddit_with_image_bypass():
    """Reddit ìˆ˜ì§‘ + ì´ë¯¸ì§€ 403 ìš°íšŒ í…ŒìŠ¤íŠ¸"""
    
    logger.info("ğŸŸ  Reddit ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ (ì´ë¯¸ì§€ 403 ìš°íšŒ í¬í•¨)")
    logger.info("=" * 70)
    
    # Cache ì´ˆê¸°í™”
    local_cache = LocalCache(db_path=Path("/tmp/reddit_bypass_test.db"))
    cache = APICache(local_cache)
    
    # Reddit collector ì´ˆê¸°í™”
    reddit_collector = RedditCollector(cache)
    
    try:
        # ì†ŒëŸ‰ ìˆ˜ì§‘ (í…ŒìŠ¤íŠ¸ìš©)
        reddit_data = await reddit_collector.collect_qa_data(
            subreddit='excel',
            max_posts=3,  # ì ì€ ìˆ˜ëŸ‰ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
            min_score=1
        )
        
        logger.info(f"âœ… Reddit ìˆ˜ì§‘ ì™„ë£Œ: {len(reddit_data)}ê°œ")
        
        # ì´ë¯¸ì§€ê°€ í¬í•¨ëœ ë°ì´í„° í™•ì¸
        image_count = 0
        for item in reddit_data:
            if 'images' in item and item['images']:
                image_count += len(item['images'])
                logger.info(f"   ğŸ“¸ ì´ë¯¸ì§€ ë°œê²¬: {len(item['images'])}ê°œ - {item.get('title', 'Unknown')[:50]}...")
        
        logger.info(f"   ì´ ì´ë¯¸ì§€: {image_count}ê°œ")
        return reddit_data
        
    except Exception as e:
        logger.error(f"âŒ Reddit ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return []

async def test_image_processing_with_bypass(raw_data):
    """ì´ë¯¸ì§€ 403 ìš°íšŒê°€ í¬í•¨ëœ ì´ë¯¸ì§€ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
    
    logger.info("ğŸ–¼ï¸  ì´ë¯¸ì§€ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ (403 ìš°íšŒ ì ìš©)")
    logger.info("=" * 70)
    
    # Cache ë° processor ì´ˆê¸°í™”
    local_cache = LocalCache(db_path=Path("/tmp/image_bypass_test.db"))
    cache = APICache(local_cache)
    processor = ImageProcessor(cache)
    
    total_images = 0
    successful_downloads = 0
    processed_images = []
    
    for item in raw_data:
        if 'images' in item and item['images']:
            for image_url in item['images']:
                total_images += 1
                logger.info(f"[{total_images}] ì´ë¯¸ì§€ ì²˜ë¦¬: {image_url}")
                
                try:
                    # 3-tier ì´ë¯¸ì§€ ì²˜ë¦¬ (OCR â†’ Table â†’ AI)
                    context_tags = item.get('tags', []) + ['excel']
                    result = await processor.process_image_url(image_url, context_tags)
                    
                    if result and result.get('success'):
                        successful_downloads += 1
                        processed_images.append({
                            'source_url': image_url,
                            'processing_result': result,
                            'source_item': {
                                'title': item.get('title', ''),
                                'source': item.get('source', ''),
                                'tags': item.get('tags', [])
                            }
                        })
                        
                        logger.info(f"   âœ… ì„±ê³µ! ì²˜ë¦¬ ë°©ë²•: {result.get('processing_tier', 'Unknown')}")
                        if result.get('extracted_content'):
                            content_preview = result['extracted_content'][:100]
                            logger.info(f"   ğŸ“ ì¶”ì¶œ ë‚´ìš©: {content_preview}...")
                    else:
                        logger.error(f"   âŒ ì‹¤íŒ¨: {result.get('error', 'Unknown error')}")
                        
                except Exception as e:
                    logger.error(f"   âŒ ì˜ˆì™¸: {e}")
                
                logger.info("-" * 40)
    
    success_rate = (successful_downloads / total_images * 100) if total_images > 0 else 0
    
    logger.info("ğŸ“Š ì´ë¯¸ì§€ ì²˜ë¦¬ ê²°ê³¼:")
    logger.info(f"   â€¢ ì´ ì´ë¯¸ì§€: {total_images}ê°œ")
    logger.info(f"   â€¢ ì„±ê³µ: {successful_downloads}ê°œ")
    logger.info(f"   â€¢ ì„±ê³µë¥ : {success_rate:.1f}%")
    
    return processed_images, success_rate

async def test_full_pipeline_with_bypass():
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ (ìˆ˜ì§‘ â†’ ì²˜ë¦¬ â†’ AI í–¥ìƒ)"""
    
    logger.info("ğŸš€ ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ (403 ìš°íšŒ í¬í•¨)")
    logger.info("=" * 70)
    
    # Cache ë° pipeline ì´ˆê¸°í™”
    local_cache = LocalCache(db_path=Path("/tmp/full_pipeline_test.db"))
    cache = APICache(local_cache)
    pipeline = ExcelQAPipeline(cache)
    
    try:
        # ì‹¤ì œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ì†ŒëŸ‰)
        logger.info("âš™ï¸  ë©”ì¸ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘...")
        
        # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ê¸°ë³¸ ì„¤ì • ì‚¬ìš©)
        result = await pipeline.run_full_pipeline()
        
        logger.info("âœ… íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ!")
        logger.info(f"   â€¢ Stack Overflow: {result.get('stackoverflow_count', 0)}ê°œ")
        logger.info(f"   â€¢ Reddit: {result.get('reddit_count', 0)}ê°œ")
        logger.info(f"   â€¢ ì´ ìˆ˜ì§‘: {result.get('total_collected', 0)}ê°œ")
        logger.info(f"   â€¢ ìµœì¢… í’ˆì§ˆ í•„í„°ë§ í›„: {result.get('final_count', 0)}ê°œ")
        
        if result.get('image_processing_stats'):
            img_stats = result['image_processing_stats']
            logger.info(f"   â€¢ ì´ë¯¸ì§€ ì²˜ë¦¬ ì„±ê³µë¥ : {img_stats.get('success_rate', 0):.1f}%")
            logger.info(f"   â€¢ ì´ë¯¸ì§€ ì´ ì²˜ë¦¬: {img_stats.get('total_processed', 0)}ê°œ")
        
        return result
        
    except Exception as e:
        logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return None

async def generate_final_dataset_with_images():
    """ìµœì¢… ì´ë¯¸ì§€ í¬í•¨ ë°ì´í„°ì…‹ ìƒì„±"""
    
    logger.info("ğŸ’ ìµœì¢… ì´ë¯¸ì§€ í¬í•¨ ë°ì´í„°ì…‹ ìƒì„±")
    logger.info("=" * 70)
    
    # Step 1: Stack Overflow ìˆ˜ì§‘
    so_data = await test_stackoverflow_with_image_bypass()
    
    # Step 2: Reddit ìˆ˜ì§‘
    reddit_data = await test_reddit_with_image_bypass()
    
    # Step 3: í†µí•© ë°ì´í„°
    combined_raw_data = so_data + reddit_data
    logger.info(f"ğŸ”— í†µí•© ë°ì´í„°: {len(combined_raw_data)}ê°œ")
    
    # Step 4: ì´ë¯¸ì§€ ì²˜ë¦¬
    processed_images, img_success_rate = await test_image_processing_with_bypass(combined_raw_data)
    
    # Step 5: ìµœì¢… ë°ì´í„°ì…‹ êµ¬ì„±
    final_dataset = {
        "dataset_info": {
            "name": "Excel Q&A Dataset with 403 Bypass",
            "version": "3.0-bypass-enabled",
            "description": "Excel Q&A dataset with advanced 403 bypass for Stack Overflow and Reddit images",
            "total_samples": len(combined_raw_data),
            "images_processed": len(processed_images),
            "image_success_rate": img_success_rate,
            "bypass_methods": ["stackoverflow_cloudscraper", "reddit_oauth_multiple"],
            "generated_at": datetime.now().isoformat()
        },
        "bypass_stats": {
            "stackoverflow_images": sum(1 for img in processed_images if 'sstatic.net' in img['source_url']),
            "reddit_images": sum(1 for img in processed_images if 'redd.it' in img['source_url']),
            "total_bypass_success": len(processed_images),
            "overall_success_rate": img_success_rate
        },
        "raw_data": combined_raw_data,
        "processed_images": processed_images
    }
    
    # ì €ì¥
    output_path = "/Users/kevin/bigdata/data/output/final_dataset_with_bypass.json"
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(final_dataset, f, indent=2, ensure_ascii=False)
    
    logger.info(f"ğŸ’¾ ìµœì¢… ë°ì´í„°ì…‹ ì €ì¥: {output_path}")
    logger.info("ğŸ“Š ìµœì¢… í†µê³„:")
    logger.info(f"   â€¢ ì´ Q&A: {final_dataset['dataset_info']['total_samples']}ê°œ")
    logger.info(f"   â€¢ ì²˜ë¦¬ëœ ì´ë¯¸ì§€: {final_dataset['dataset_info']['images_processed']}ê°œ")
    logger.info(f"   â€¢ ì´ë¯¸ì§€ ì„±ê³µë¥ : {final_dataset['dataset_info']['image_success_rate']:.1f}%")
    logger.info(f"   â€¢ Stack Overflow ì´ë¯¸ì§€: {final_dataset['bypass_stats']['stackoverflow_images']}ê°œ")
    logger.info(f"   â€¢ Reddit ì´ë¯¸ì§€: {final_dataset['bypass_stats']['reddit_images']}ê°œ")
    
    return final_dataset

if __name__ == "__main__":
    logger.info("ğŸ”¥ 403 ìš°íšŒ ê¸°ëŠ¥ í¬í•¨ ì „ì²´ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    logger.info("=" * 80)
    
    # ê°œë³„ í…ŒìŠ¤íŠ¸ë“¤
    print("\n" + "ğŸ”¸" * 20 + " Phase 1: Stack Overflow í…ŒìŠ¤íŠ¸ " + "ğŸ”¸" * 20)
    so_result = asyncio.run(test_stackoverflow_with_image_bypass())
    
    print("\n" + "ğŸ”¸" * 20 + " Phase 2: Reddit í…ŒìŠ¤íŠ¸ " + "ğŸ”¸" * 20)
    reddit_result = asyncio.run(test_reddit_with_image_bypass())
    
    print("\n" + "ğŸ”¸" * 20 + " Phase 3: ìµœì¢… ë°ì´í„°ì…‹ ìƒì„± " + "ğŸ”¸" * 20)
    final_result = asyncio.run(generate_final_dataset_with_images())
    
    print("\n" + "ğŸ" * 30)
    logger.info("ğŸŠ 403 ìš°íšŒ ê¸°ëŠ¥ í¬í•¨ ì „ì²´ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    
    if final_result:
        bypass_stats = final_result['bypass_stats']
        logger.info("ğŸ¯ ìµœì¢… ì„±ê³¼:")
        logger.info(f"   âœ… ì „ì²´ ì´ë¯¸ì§€ ìš°íšŒ ì„±ê³µë¥ : {bypass_stats['overall_success_rate']:.1f}%")
        logger.info(f"   ğŸ“š Stack Overflow ì´ë¯¸ì§€ ìš°íšŒ: {bypass_stats['stackoverflow_images']}ê°œ")
        logger.info(f"   ğŸŸ  Reddit ì´ë¯¸ì§€ ìš°íšŒ: {bypass_stats['reddit_images']}ê°œ")
        logger.info(f"   ğŸ‰ 403 ì—ëŸ¬ ê·¹ë³µ ì™„ë£Œ!")
    else:
        logger.error("âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")