#!/usr/bin/env python3
"""
ê°„ë‹¨í•œ 403 ìš°íšŒ í…ŒìŠ¤íŠ¸ - Stack Overflow + Reddit ìˆ˜ì§‘ ë° ì´ë¯¸ì§€ ì²˜ë¦¬
"""
import asyncio
import json
import logging
import sys
from pathlib import Path

# Add project root to path
sys.path.append(str(Path(__file__).parent))

from collectors.stackoverflow_collector import StackOverflowCollector
from collectors.reddit_collector import RedditCollector
from processors.image_processor import ImageProcessor
from core.cache import APICache, LocalCache

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

async def simple_collection_test():
    """ê°„ë‹¨í•œ ìˆ˜ì§‘ ë° ì´ë¯¸ì§€ ìš°íšŒ í…ŒìŠ¤íŠ¸"""
    
    logger.info("ğŸš€ ê°„ë‹¨í•œ 403 ìš°íšŒ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    logger.info("=" * 70)
    
    # Cache ì´ˆê¸°í™”
    local_cache = LocalCache(db_path=Path("/tmp/simple_test.db"))
    cache = APICache(local_cache)
    
    # ê²°ê³¼ ì €ì¥
    all_results = {
        "stackoverflow": {"data": [], "images": []},
        "reddit": {"data": [], "images": []},
        "image_processing": {"total": 0, "success": 0, "results": []}
    }
    
    # 1. Stack Overflow ìˆ˜ì§‘
    logger.info("ğŸ“š Stack Overflow ìˆ˜ì§‘ ì¤‘...")
    try:
        so_collector = StackOverflowCollector(cache)
        so_data = await so_collector.collect_excel_questions(
            max_pages=1  # ì†ŒëŸ‰ í…ŒìŠ¤íŠ¸ - 1í˜ì´ì§€ë§Œ
        )
        
        all_results["stackoverflow"]["data"] = so_data
        logger.info(f"âœ… Stack Overflow: {len(so_data)}ê°œ ìˆ˜ì§‘")
        
        # ì´ë¯¸ì§€ URL ì¶”ì¶œ
        for item in so_data:
            if 'images' in item and item['images']:
                all_results["stackoverflow"]["images"].extend(item['images'])
                
        logger.info(f"   ğŸ“¸ Stack Overflow ì´ë¯¸ì§€: {len(all_results['stackoverflow']['images'])}ê°œ")
        
    except Exception as e:
        logger.error(f"âŒ Stack Overflow ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
    
    # 2. Reddit ìˆ˜ì§‘
    logger.info("ğŸŸ  Reddit ìˆ˜ì§‘ ì¤‘...")
    try:
        reddit_collector = RedditCollector(cache)
        reddit_data = await reddit_collector.collect_excel_discussions(
            max_submissions=2  # ì†ŒëŸ‰ í…ŒìŠ¤íŠ¸
        )
        
        all_results["reddit"]["data"] = reddit_data
        logger.info(f"âœ… Reddit: {len(reddit_data)}ê°œ ìˆ˜ì§‘")
        
        # ì´ë¯¸ì§€ URL ì¶”ì¶œ
        for item in reddit_data:
            if 'images' in item and item['images']:
                all_results["reddit"]["images"].extend(item['images'])
                
        logger.info(f"   ğŸ“¸ Reddit ì´ë¯¸ì§€: {len(all_results['reddit']['images'])}ê°œ")
        
    except Exception as e:
        logger.error(f"âŒ Reddit ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
    
    # 3. ì´ë¯¸ì§€ ì²˜ë¦¬ (403 ìš°íšŒ í¬í•¨)
    logger.info("ğŸ–¼ï¸  ì´ë¯¸ì§€ 403 ìš°íšŒ í…ŒìŠ¤íŠ¸...")
    
    all_images = all_results["stackoverflow"]["images"] + all_results["reddit"]["images"]
    processor = ImageProcessor(cache)
    
    for i, image_url in enumerate(all_images, 1):
        all_results["image_processing"]["total"] += 1
        
        logger.info(f"[{i}/{len(all_images)}] ì´ë¯¸ì§€ ì²˜ë¦¬: {image_url}")
        
        try:
            # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° ì²˜ë¦¬
            result = await processor.process_image_url(image_url, ['excel'])
            
            if result and result.get('success'):
                all_results["image_processing"]["success"] += 1
                all_results["image_processing"]["results"].append({
                    "url": image_url,
                    "success": True,
                    "processing_tier": result.get('processing_tier', ''),
                    "content_length": len(result.get('extracted_content', '')),
                    "source": "stackoverflow" if "sstatic.net" in image_url else "reddit"
                })
                
                logger.info(f"   âœ… ì„±ê³µ! {result.get('processing_tier', 'Unknown')}")
                if result.get('extracted_content'):
                    preview = result['extracted_content'][:80].replace('\n', ' ')
                    logger.info(f"   ğŸ“ ì¶”ì¶œ: {preview}...")
            else:
                all_results["image_processing"]["results"].append({
                    "url": image_url,
                    "success": False,
                    "error": result.get('error', 'Unknown'),
                    "source": "stackoverflow" if "sstatic.net" in image_url else "reddit"
                })
                logger.error(f"   âŒ ì‹¤íŒ¨: {result.get('error', 'Unknown')}")
                
        except Exception as e:
            logger.error(f"   âŒ ì˜ˆì™¸: {e}")
            all_results["image_processing"]["results"].append({
                "url": image_url,
                "success": False,
                "error": str(e),
                "source": "stackoverflow" if "sstatic.net" in image_url else "reddit"
            })
        
        logger.info("-" * 50)
    
    # 4. ê²°ê³¼ ì •ë¦¬ ë° ì €ì¥
    success_rate = (all_results["image_processing"]["success"] / 
                   all_results["image_processing"]["total"] * 100) if all_results["image_processing"]["total"] > 0 else 0
    
    # ì†ŒìŠ¤ë³„ ì„±ê³µë¥  ê³„ì‚°
    so_success = sum(1 for r in all_results["image_processing"]["results"] 
                    if r["source"] == "stackoverflow" and r["success"])
    so_total = sum(1 for r in all_results["image_processing"]["results"] 
                  if r["source"] == "stackoverflow")
    
    reddit_success = sum(1 for r in all_results["image_processing"]["results"] 
                        if r["source"] == "reddit" and r["success"])
    reddit_total = sum(1 for r in all_results["image_processing"]["results"] 
                      if r["source"] == "reddit")
    
    so_rate = (so_success / so_total * 100) if so_total > 0 else 0
    reddit_rate = (reddit_success / reddit_total * 100) if reddit_total > 0 else 0
    
    # ìµœì¢… ë³´ê³ ì„œ
    final_report = {
        "test_summary": {
            "total_qa_collected": len(all_results["stackoverflow"]["data"]) + len(all_results["reddit"]["data"]),
            "stackoverflow_qa": len(all_results["stackoverflow"]["data"]),
            "reddit_qa": len(all_results["reddit"]["data"]),
            "total_images": all_results["image_processing"]["total"],
            "successful_images": all_results["image_processing"]["success"],
            "overall_success_rate": success_rate
        },
        "bypass_effectiveness": {
            "stackoverflow": {
                "images_tested": so_total,
                "images_successful": so_success,
                "success_rate": so_rate
            },
            "reddit": {
                "images_tested": reddit_total,
                "images_successful": reddit_success,
                "success_rate": reddit_rate
            }
        },
        "detailed_results": all_results
    }
    
    # íŒŒì¼ë¡œ ì €ì¥
    output_path = "/Users/kevin/bigdata/data/output/bypass_test_results.json"
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(final_report, f, indent=2, ensure_ascii=False)
    
    # ê²°ê³¼ ì¶œë ¥
    logger.info("=" * 70)
    logger.info("ğŸ“Š ìµœì¢… í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    logger.info(f"   â€¢ ì´ Q&A ìˆ˜ì§‘: {final_report['test_summary']['total_qa_collected']}ê°œ")
    logger.info(f"     - Stack Overflow: {final_report['test_summary']['stackoverflow_qa']}ê°œ")
    logger.info(f"     - Reddit: {final_report['test_summary']['reddit_qa']}ê°œ")
    logger.info(f"   â€¢ ì´ ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸: {final_report['test_summary']['total_images']}ê°œ")
    logger.info(f"   â€¢ ì´ë¯¸ì§€ ì²˜ë¦¬ ì„±ê³µ: {final_report['test_summary']['successful_images']}ê°œ")
    logger.info(f"   â€¢ ì „ì²´ ì„±ê³µë¥ : {final_report['test_summary']['overall_success_rate']:.1f}%")
    
    logger.info("")
    logger.info("ğŸ¯ ì†ŒìŠ¤ë³„ 403 ìš°íšŒ ì„±ê³¼:")
    logger.info(f"   ğŸ“š Stack Overflow: {final_report['bypass_effectiveness']['stackoverflow']['success_rate']:.1f}% ({final_report['bypass_effectiveness']['stackoverflow']['images_successful']}/{final_report['bypass_effectiveness']['stackoverflow']['images_tested']})")
    logger.info(f"   ğŸŸ  Reddit: {final_report['bypass_effectiveness']['reddit']['success_rate']:.1f}% ({final_report['bypass_effectiveness']['reddit']['images_successful']}/{final_report['bypass_effectiveness']['reddit']['images_tested']})")
    
    logger.info(f"\nğŸ’¾ ìƒì„¸ ê²°ê³¼ ì €ì¥: {output_path}")
    
    if success_rate >= 80:
        logger.info("ğŸ‰ 403 ìš°íšŒ ì„±ê³µ! ëª©í‘œ ë‹¬ì„±!")
    elif success_rate >= 60:
        logger.info("âœ… 403 ìš°íšŒ ì–‘í˜¸í•œ ì„±ê³¼!")
    elif success_rate >= 40:
        logger.warning("âš ï¸  403 ìš°íšŒ ë¶€ë¶„ì  ì„±ê³µ")
    else:
        logger.error("âŒ 403 ìš°íšŒ ê°œì„  í•„ìš”")
    
    return final_report

if __name__ == "__main__":
    result = asyncio.run(simple_collection_test())