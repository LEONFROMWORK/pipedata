#!/usr/bin/env python3
"""
ì˜¤ë¹ ë‘ ê²Œì‹œê¸€ ì‹¤ì œ ì½˜í…ì¸  ê²€ì¦
ì¶”ì¶œëœ URLë“¤ì´ ì‹¤ì œ Q&A ê²Œì‹œê¸€ì¸ì§€ í™•ì¸
"""

import cloudscraper
from bs4 import BeautifulSoup
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def verify_oppadu_content():
    """ì˜¤ë¹ ë‘ ê²Œì‹œê¸€ì˜ ì‹¤ì œ ì½˜í…ì¸ ê°€ Q&Aì¸ì§€ ê²€ì¦"""
    
    # ë¶„ì„í•  ìƒ˜í”Œ URLë“¤ (ìœ„ ë¶„ì„ì—ì„œ ë‚˜ì˜¨ ë‹µë³€ ì™„ë£Œ ê²Œì‹œê¸€ë“¤)
    sample_urls = [
        "https://www.oppadu.com?board_id=1&action=view&uid=79620&pg=1",
        "https://www.oppadu.com?board_id=1&action=view&uid=79616&pg=1", 
        "https://www.oppadu.com?board_id=1&action=view&uid=79613&pg=1"
    ]
    
    # CloudScraper ì„¤ì •
    scraper = cloudscraper.create_scraper(
        browser={
            'browser': 'chrome',
            'platform': 'windows',
            'desktop': True
        }
    )
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
        'Referer': 'https://www.oppadu.com/community/question/'
    }
    scraper.headers.update(headers)
    
    print("ğŸ” ì˜¤ë¹ ë‘ ê²Œì‹œê¸€ ì½˜í…ì¸  ê²€ì¦")
    print("="*80)
    
    for i, url in enumerate(sample_urls, 1):
        print(f"\nğŸ“ ê²Œì‹œê¸€ {i} ë¶„ì„: {url}")
        print("-" * 60)
        
        try:
            response = scraper.get(url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # 1. í˜ì´ì§€ ì œëª©
            title = soup.find('title')
            page_title = title.get_text() if title else "ì œëª© ì—†ìŒ"
            print(f"í˜ì´ì§€ ì œëª©: {page_title}")
            
            # 2. ê²Œì‹œê¸€ ì œëª©
            post_title = soup.find('h1') or soup.find(class_='post-title')
            if post_title:
                print(f"ê²Œì‹œê¸€ ì œëª©: {post_title.get_text(strip=True)}")
            
            # 3. ì§ˆë¬¸ ë‚´ìš© í™•ì¸
            post_content = soup.find(class_='post-content')
            if post_content:
                content_text = post_content.get_text(strip=True)
                print(f"ì§ˆë¬¸ ë‚´ìš© ê¸¸ì´: {len(content_text)} ë¬¸ì")
                print(f"ì§ˆë¬¸ ë¯¸ë¦¬ë³´ê¸°: {content_text[:200]}...")
                
                # Q&A íŠ¹ì„± í‚¤ì›Œë“œ í™•ì¸
                qa_keywords = ['ì—‘ì…€', 'í•¨ìˆ˜', 'ìˆ˜ì‹', 'ì…€', 'ì§ˆë¬¸', 'ë¬¸ì œ', 'ë„ì›€', 'ë°©ë²•']
                found_keywords = [kw for kw in qa_keywords if kw in content_text]
                print(f"Q&A ê´€ë ¨ í‚¤ì›Œë“œ: {found_keywords}")
            
            # 4. ë‹µë³€ ì„¹ì…˜ í™•ì¸
            answer_sections = soup.find_all(class_='answer-item') or soup.find_all(class_='reply-item')
            print(f"ë‹µë³€ ê°œìˆ˜: {len(answer_sections)}ê°œ")
            
            # 5. ì±„íƒëœ ë‹µë³€ í™•ì¸
            selected_answer = soup.find(class_='selected-answer-badge') or soup.find(class_='best-answer')
            if selected_answer:
                print("âœ… ì±„íƒëœ ë‹µë³€ ìˆìŒ")
                # ì±„íƒ ë‹µë³€ ë‚´ìš© ì°¾ê¸°
                answer_content = None
                parent = selected_answer.find_parent()
                if parent:
                    answer_content = parent.find(class_='answer-content') or parent.find(class_='post-content')
                    if answer_content:
                        answer_text = answer_content.get_text(strip=True)
                        print(f"ì±„íƒ ë‹µë³€ ê¸¸ì´: {len(answer_text)} ë¬¸ì")
                        print(f"ì±„íƒ ë‹µë³€ ë¯¸ë¦¬ë³´ê¸°: {answer_text[:150]}...")
            else:
                print("âŒ ì±„íƒëœ ë‹µë³€ ë¯¸ë°œê²¬")
            
            # 6. ë©”íƒ€ë°ì´í„° í™•ì¸
            metadata_indicators = {
                'excel_version': ['ì—‘ì…€ë²„ì „', 'ì—‘ì…€ ë²„ì „', 'Excel'],
                'os_version': ['OSë²„ì „', 'ìš´ì˜ì²´ì œ', 'Windows'],
                'question_type': ['ì§ˆë¬¸', 'ë¬¸ì˜', 'ë„ì›€', 'ë¬¸ì œ']
            }
            
            page_text = soup.get_text()
            for meta_type, keywords in metadata_indicators.items():
                found = any(keyword in page_text for keyword in keywords)
                print(f"{meta_type}: {'âœ…' if found else 'âŒ'}")
            
            # 7. ì½˜í…ì¸  ìœ í˜• íŒë‹¨
            print(f"\nğŸ“Š ì½˜í…ì¸  ìœ í˜• ë¶„ì„:")
            
            # í”„ë¡œëª¨ì…˜ ì½˜í…ì¸  íŠ¹ì„±
            promo_indicators = ['ê´‘ê³ ', 'í”„ë¡œëª¨ì…˜', 'ì´ë²¤íŠ¸', 'í• ì¸', 'êµ¬ë§¤', 'ìƒí’ˆ']
            promo_found = any(indicator in page_text for indicator in promo_indicators)
            
            # Q&A ì½˜í…ì¸  íŠ¹ì„±  
            qa_indicators = ['ì§ˆë¬¸', 'ë‹µë³€', 'í•´ê²°', 'ë„ì›€', 'í•¨ìˆ˜', 'ìˆ˜ì‹', 'ë¬¸ì œ', 'ë°©ë²•']
            qa_found = sum(1 for indicator in qa_indicators if indicator in page_text)
            
            print(f"  í”„ë¡œëª¨ì…˜ íŠ¹ì„±: {'âŒ ë°œê²¬ë¨' if promo_found else 'âœ… ì—†ìŒ'}")
            print(f"  Q&A íŠ¹ì„±: {qa_found}ê°œ í‚¤ì›Œë“œ ë°œê²¬")
            
            if qa_found >= 3 and not promo_found:
                print("  ğŸ¯ ê²°ë¡ : ì •ìƒì ì¸ Q&A ê²Œì‹œê¸€")
            elif promo_found:
                print("  âš ï¸  ê²°ë¡ : í”„ë¡œëª¨ì…˜ ì½˜í…ì¸  ì˜ì‹¬")
            else:
                print("  â“ ê²°ë¡ : ë¶ˆí™•ì‹¤")
                
        except Exception as e:
            print(f"âŒ ê²Œì‹œê¸€ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
    
    print(f"\n" + "="*80)
    print("âœ… ì½˜í…ì¸  ê²€ì¦ ì™„ë£Œ")

if __name__ == "__main__":
    verify_oppadu_content()