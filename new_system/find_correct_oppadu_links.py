#!/usr/bin/env python3
"""
ì˜¤ë¹ ë‘ ì˜¬ë°”ë¥¸ ê²Œì‹œê¸€ ë§í¬ ì°¾ê¸°
post-item-modern ë‚´ë¶€ì˜ ì‹¤ì œ ê²Œì‹œê¸€ ë§í¬ ì¶”ì¶œ
"""

import cloudscraper
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def find_correct_oppadu_links():
    """ì˜¤ë¹ ë‘ì˜ ì˜¬ë°”ë¥¸ ê²Œì‹œê¸€ ë§í¬ ì°¾ê¸°"""
    
    base_url = "https://www.oppadu.com"
    list_url = f"{base_url}/community/question/"
    
    # CloudScraper ì„¤ì •
    scraper = cloudscraper.create_scraper(
        browser={
            'browser': 'chrome',
            'platform': 'windows',
            'desktop': True
        }
    )
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
        'Referer': base_url
    }
    scraper.headers.update(headers)
    
    print("ğŸ” ì˜¤ë¹ ë‘ ì˜¬ë°”ë¥¸ ê²Œì‹œê¸€ ë§í¬ ì°¾ê¸°")
    print("="*80)
    
    try:
        response = scraper.get(list_url, timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # 1. post-list-modern ì»¨í…Œì´ë„ˆ ì°¾ê¸°
        post_list = soup.find('div', class_='post-list-modern')
        if not post_list:
            print("âŒ post-list-modern ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return
        
        print("âœ… post-list-modern ì»¨í…Œì´ë„ˆ ë°œê²¬")
        
        # 2. post-item-modern í•­ëª©ë“¤ ë¶„ì„
        post_items = post_list.find_all('div', class_='post-item-modern')
        print(f"ğŸ“ ì´ {len(post_items)}ê°œì˜ post-item-modern ë°œê²¬")
        
        # 3. ê° ê²Œì‹œê¸€ í•­ëª©ì˜ ë§í¬ êµ¬ì¡° ìƒì„¸ ë¶„ì„
        for i, item in enumerate(post_items[:5]):  # ì²« 5ê°œë§Œ ë¶„ì„
            print(f"\n--- ê²Œì‹œê¸€ {i+1} ë§í¬ êµ¬ì¡° ë¶„ì„ ---")
            
            # item ë‚´ì˜ ëª¨ë“  ë§í¬ ì°¾ê¸°
            all_links_in_item = item.find_all('a', href=True)
            print(f"  ì´ í•­ëª© ë‚´ ì´ ë§í¬ ìˆ˜: {len(all_links_in_item)}")
            
            for j, link in enumerate(all_links_in_item):
                href = link['href']
                link_text = link.get_text(strip=True)
                classes = link.get('class', [])
                
                print(f"    ë§í¬ {j+1}:")
                print(f"      href: {href}")
                print(f"      text: {link_text[:50]}...")
                print(f"      classes: {classes}")
                print(f"      full_url: {urljoin(base_url, href)}")
                
                # ê²Œì‹œê¸€ ì œëª© ë§í¬ì¸ì§€ í™•ì¸
                if any(cls in classes for cls in ['post-title', 'post-title-modern']):
                    print(f"      ğŸ¯ ê²Œì‹œê¸€ ì œëª© ë§í¬ë¡œ ì¶”ì •")
                
                # ì‚¬ìš©ì ë§í¬ ì œì™¸
                if 'user-info' in href:
                    print(f"      âŒ ì‚¬ìš©ì ì •ë³´ ë§í¬ (ì œì™¸)")
                elif 'board_id' in href and 'action=view' in href:
                    print(f"      âœ… ê²Œì‹œê¸€ ë§í¬ë¡œ ì¶”ì •")
                
                print()
            
            # ë‹µë³€ ì™„ë£Œ ë°°ì§€ í™•ì¸
            answer_badge = item.find(class_='answer-complete-badge')
            print(f"  ë‹µë³€ ì™„ë£Œ ë°°ì§€: {'âœ… ìˆìŒ' if answer_badge else 'âŒ ì—†ìŒ'}")
            
            # ì¶”ì²œ ë§í¬ ì¶”ì¶œ ë¡œì§
            title_link = item.find('a', class_='post-title-modern')
            if title_link:
                recommended_url = urljoin(base_url, title_link['href'])
                print(f"  ğŸ¯ ì¶”ì²œ URL: {recommended_url}")
                
                # ì´ URLë¡œ ì‹¤ì œ ìš”ì²­ í…ŒìŠ¤íŠ¸
                print(f"  ğŸ§ª URL í…ŒìŠ¤íŠ¸ ì¤‘...")
                try:
                    test_response = scraper.get(recommended_url, timeout=15)
                    test_soup = BeautifulSoup(test_response.text, 'html.parser')
                    
                    # í˜ì´ì§€ íƒ€ì´í‹€ í™•ì¸
                    page_title = test_soup.find('title')
                    if page_title:
                        title_text = page_title.get_text()
                        print(f"    í˜ì´ì§€ ì œëª©: {title_text}")
                        
                        # í™ˆí˜ì´ì§€ì¸ì§€ ê²Œì‹œê¸€ì¸ì§€ íŒë‹¨
                        if "ì—‘ì…€ê°•ì˜ ëŒ€í‘œì±„ë„" in title_text:
                            print(f"    âŒ í™ˆí˜ì´ì§€ë¡œ ë¦¬ë””ë ‰ì…˜ë¨")
                        elif title_text.strip() and title_text != "ì˜¤ë¹ ë‘ì—‘ì…€":
                            print(f"    âœ… ê°œë³„ ê²Œì‹œê¸€ í˜ì´ì§€")
                        else:
                            print(f"    â“ ë¶ˆí™•ì‹¤")
                    
                    # ê²Œì‹œê¸€ ì½˜í…ì¸  ìš”ì†Œ í™•ì¸
                    content_indicators = [
                        ('post-content', test_soup.find(class_='post-content')),
                        ('xe-content', test_soup.find(class_='xe-content')),
                        ('board-content', test_soup.find(class_='board-content')),
                        ('question-content', test_soup.find(class_='question-content'))
                    ]
                    
                    found_content = False
                    for indicator_name, element in content_indicators:
                        if element:
                            content_text = element.get_text(strip=True)
                            if len(content_text) > 50:  # ì˜ë¯¸ìˆëŠ” ì½˜í…ì¸ ê°€ ìˆëŠ”ì§€
                                print(f"    âœ… {indicator_name} ë°œê²¬: {content_text[:100]}...")
                                found_content = True
                                break
                    
                    if not found_content:
                        # í™ˆí˜ì´ì§€ ìš”ì†Œ í™•ì¸
                        home_elements = test_soup.find_all(class_='slider-contents')
                        if home_elements:
                            print(f"    âŒ í™ˆí˜ì´ì§€ ìŠ¬ë¼ì´ë” ì½˜í…ì¸  ë°œê²¬")
                        else:
                            print(f"    â“ ì½˜í…ì¸  êµ¬ì¡° ë¶ˆëª…í™•")
                
                except Exception as e:
                    print(f"    âŒ URL í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            
            print("-" * 60)
        
        # 4. ì˜¬ë°”ë¥¸ ë§í¬ ì¶”ì¶œ ë¡œì§ ì œì•ˆ
        print(f"\nğŸ’¡ ì˜¬ë°”ë¥¸ ë§í¬ ì¶”ì¶œ ë¡œì§:")
        print("1. post-item-modern ë‚´ì—ì„œ post-title-modern í´ë˜ìŠ¤ì˜ <a> íƒœê·¸ ì°¾ê¸°")
        print("2. user-info ë§í¬ëŠ” ì œì™¸")
        print("3. board_id, action=view, uid íŒŒë¼ë¯¸í„°ê°€ ìˆëŠ” ë§í¬ ì„ íƒ")
        print("4. urljoinìœ¼ë¡œ ì ˆëŒ€ URL ìƒì„±")
        
    except Exception as e:
        print(f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        print(traceback.format_exc())

if __name__ == "__main__":
    find_correct_oppadu_links()