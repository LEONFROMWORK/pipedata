#!/usr/bin/env python3
"""
ì§€ì†ì  ìˆ˜ì§‘ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
"""
import asyncio
import json
import logging
import time
from pathlib import Path
import sys
import aiohttp

# Add project root to path
sys.path.append(str(Path(__file__).parent))

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

class ContinuousCollectionTester:
    """ì§€ì†ì  ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    def __init__(self, base_url="http://127.0.0.1:8000"):
        self.base_url = base_url
        self.session = None
    
    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def check_api_server(self):
        """API ì„œë²„ ìƒíƒœ í™•ì¸"""
        try:
            async with self.session.get(f"{self.base_url}/api/status") as response:
                if response.status == 200:
                    data = await response.json()
                    return True, data
                else:
                    return False, f"HTTP {response.status}"
        except Exception as e:
            return False, str(e)
    
    async def start_continuous_collection(self, sources=["reddit"], max_per_batch=3):
        """ì§€ì†ì  ìˆ˜ì§‘ ì‹œì‘"""
        payload = {
            "sources": sources,
            "max_per_batch": max_per_batch
        }
        
        try:
            async with self.session.post(
                f"{self.base_url}/api/run-continuous",
                json=payload,
                headers={"Content-Type": "application/json"}
            ) as response:
                
                if response.status == 200:
                    data = await response.json()
                    return True, data
                else:
                    error_text = await response.text()
                    return False, f"HTTP {response.status}: {error_text}"
                    
        except Exception as e:
            return False, str(e)
    
    async def get_pipeline_status(self):
        """íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì¡°íšŒ"""
        try:
            async with self.session.get(f"{self.base_url}/api/status") as response:
                if response.status == 200:
                    return await response.json()
                else:
                    return None
        except Exception as e:
            logger.error(f"Status check failed: {e}")
            return None
    
    async def stop_continuous_collection(self):
        """ì§€ì†ì  ìˆ˜ì§‘ ì •ì§€"""
        try:
            async with self.session.post(f"{self.base_url}/api/stop-pipeline") as response:
                if response.status == 200:
                    data = await response.json()
                    return True, data
                else:
                    error_text = await response.text()
                    return False, f"HTTP {response.status}: {error_text}"
                    
        except Exception as e:
            return False, str(e)
    
    async def monitor_collection(self, duration_seconds=60):
        """ì§€ì†ì  ìˆ˜ì§‘ ëª¨ë‹ˆí„°ë§"""
        logger.info(f"ğŸ” {duration_seconds}ì´ˆê°„ ì§€ì†ì  ìˆ˜ì§‘ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
        
        start_time = time.time()
        last_counts = {}
        
        while time.time() - start_time < duration_seconds:
            status = await self.get_pipeline_status()
            
            if status:
                current_counts = {
                    'collected': status.get('collected_count', 0),
                    'processed': status.get('processed_count', 0),
                    'final': status.get('final_count', 0),
                    'status': status.get('status', 'unknown')
                }
                
                # ë³€í™” ê°ì§€
                if current_counts != last_counts:
                    logger.info(f"ğŸ“Š ìƒíƒœ ì—…ë°ì´íŠ¸: {current_counts}")
                    
                    if current_counts['status'] == 'error':
                        errors = status.get('errors', [])
                        logger.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {errors}")
                        break
                    
                    elif current_counts['status'] == 'completed':
                        logger.info("âœ… ìˆ˜ì§‘ ì™„ë£Œ")
                        break
                    
                    last_counts = current_counts
            
            await asyncio.sleep(5)  # 5ì´ˆë§ˆë‹¤ ì²´í¬
        
        logger.info("ğŸ ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ")
        return last_counts

async def test_continuous_collection():
    """ì§€ì†ì  ìˆ˜ì§‘ ì „ì²´ í…ŒìŠ¤íŠ¸"""
    
    logger.info("ğŸš€ ì§€ì†ì  ìˆ˜ì§‘ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    logger.info("=" * 60)
    
    async with ContinuousCollectionTester() as tester:
        
        # 1. API ì„œë²„ ìƒíƒœ í™•ì¸
        logger.info("1ï¸âƒ£ API ì„œë²„ ìƒíƒœ í™•ì¸")
        is_running, status_data = await tester.check_api_server()
        
        if not is_running:
            logger.error(f"âŒ API ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {status_data}")
            logger.info("   ëŒ€ì‹œë³´ë“œê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”:")
            logger.info("   cd /Users/kevin/bigdata/dashboard-ui && npm run dev")
            return
        
        logger.info(f"âœ… API ì„œë²„ ì •ìƒ ì‘ë™: {status_data.get('status', 'unknown')}")
        
        # 2. ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ ì •ì§€ (í˜¹ì‹œ ì‹¤í–‰ ì¤‘ì¸ ê²½ìš°)
        logger.info("\n2ï¸âƒ£ ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ ì •ì§€")
        stop_success, stop_result = await tester.stop_continuous_collection()
        logger.info(f"   ì •ì§€ ê²°ê³¼: {stop_result}")
        
        # ì ì‹œ ëŒ€ê¸°
        await asyncio.sleep(2)
        
        # 3. ì§€ì†ì  ìˆ˜ì§‘ ì‹œì‘
        logger.info("\n3ï¸âƒ£ ì§€ì†ì  ìˆ˜ì§‘ ì‹œì‘")
        sources = ["reddit"]  # Redditë§Œ í…ŒìŠ¤íŠ¸ (ë¹ ë¥¸ ì‘ë‹µ)
        max_per_batch = 3     # ì‘ì€ ë°°ì¹˜ë¡œ í…ŒìŠ¤íŠ¸
        
        start_success, start_result = await tester.start_continuous_collection(
            sources=sources,
            max_per_batch=max_per_batch
        )
        
        if not start_success:
            logger.error(f"âŒ ì§€ì†ì  ìˆ˜ì§‘ ì‹œì‘ ì‹¤íŒ¨: {start_result}")
            return
        
        logger.info(f"âœ… ì§€ì†ì  ìˆ˜ì§‘ ì‹œì‘ë¨: {start_result}")
        
        # 4. ìˆ˜ì§‘ ëª¨ë‹ˆí„°ë§
        logger.info("\n4ï¸âƒ£ ìˆ˜ì§‘ ì§„í–‰ ëª¨ë‹ˆí„°ë§ (30ì´ˆ)")
        final_counts = await tester.monitor_collection(duration_seconds=30)
        
        # 5. ìˆ˜ì§‘ ì •ì§€
        logger.info("\n5ï¸âƒ£ ì§€ì†ì  ìˆ˜ì§‘ ì •ì§€")
        stop_success, stop_result = await tester.stop_continuous_collection()
        logger.info(f"   ì •ì§€ ê²°ê³¼: {stop_result}")
        
        # 6. ìµœì¢… ìƒíƒœ í™•ì¸
        logger.info("\n6ï¸âƒ£ ìµœì¢… ìƒíƒœ í™•ì¸")
        final_status = await tester.get_pipeline_status()
        
        if final_status:
            logger.info(f"   ìµœì¢… ìƒíƒœ: {final_status.get('status')}")
            logger.info(f"   ìˆ˜ì§‘ëœ ë°ì´í„°: {final_status.get('collected_count', 0)}ê°œ")
            logger.info(f"   ì²˜ë¦¬ëœ ë°ì´í„°: {final_status.get('processed_count', 0)}ê°œ")
            logger.info(f"   ìµœì¢… ì¶œë ¥: {final_status.get('final_count', 0)}ê°œ")
        
        # 7. ìƒì„±ëœ íŒŒì¼ í™•ì¸
        logger.info("\n7ï¸âƒ£ ìƒì„±ëœ íŒŒì¼ í™•ì¸")
        output_dir = Path("/Users/kevin/bigdata/data/output")
        
        # ìµœì‹  íŒŒì¼ ì°¾ê¸°
        jsonl_files = list(output_dir.rglob("*.jsonl"))
        if jsonl_files:
            latest_file = max(jsonl_files, key=lambda x: x.stat().st_mtime)
            
            # íŒŒì¼ í¬ê¸° ë° ë¼ì¸ ìˆ˜ í™•ì¸
            file_size = latest_file.stat().st_size
            with open(latest_file, 'r') as f:
                line_count = sum(1 for _ in f)
            
            logger.info(f"   ìµœì‹  íŒŒì¼: {latest_file.name}")
            logger.info(f"   íŒŒì¼ í¬ê¸°: {file_size} bytes")
            logger.info(f"   ë°ì´í„° í•­ëª©: {line_count}ê°œ")
            
            # ìƒ˜í”Œ ë°ì´í„° í™•ì¸
            if line_count > 0:
                with open(latest_file, 'r') as f:
                    sample = json.loads(f.readline())
                    logger.info(f"   ìƒ˜í”Œ ì§ˆë¬¸: {sample.get('user_question', '')[:50]}...")
                    logger.info(f"   ë‹µë³€ ê¸¸ì´: {len(sample.get('assistant_response', ''))} ë¬¸ì")
        else:
            logger.warning("   âš ï¸ ìƒì„±ëœ JSONL íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")

async def quick_continuous_test():
    """ë¹ ë¥¸ ì§€ì†ì  ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ (APIë§Œ)"""
    
    logger.info("âš¡ ë¹ ë¥¸ ì§€ì†ì  ìˆ˜ì§‘ API í…ŒìŠ¤íŠ¸")
    logger.info("=" * 40)
    
    async with ContinuousCollectionTester() as tester:
        
        # API ì„œë²„ í™•ì¸
        is_running, status = await tester.check_api_server()
        if not is_running:
            logger.error(f"âŒ API ì„œë²„ ë¯¸ì‹¤í–‰: {status}")
            return
        
        logger.info("âœ… API ì„œë²„ ì •ìƒ")
        
        # ì§€ì†ì  ìˆ˜ì§‘ ì‹œì‘ ìš”ì²­
        start_success, result = await tester.start_continuous_collection(
            sources=["reddit"],
            max_per_batch=1
        )
        
        if start_success:
            logger.info(f"âœ… ì§€ì†ì  ìˆ˜ì§‘ ì‹œì‘: {result}")
            
            # 5ì´ˆ í›„ ìƒíƒœ í™•ì¸
            await asyncio.sleep(5)
            status = await tester.get_pipeline_status()
            logger.info(f"ğŸ“Š 5ì´ˆ í›„ ìƒíƒœ: {status.get('status', 'unknown')}")
            
            # ì •ì§€
            await tester.stop_continuous_collection()
            logger.info("ğŸ›‘ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
            
        else:
            logger.error(f"âŒ ì§€ì†ì  ìˆ˜ì§‘ ì‹œì‘ ì‹¤íŒ¨: {result}")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "quick":
        # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
        asyncio.run(quick_continuous_test())
    else:
        # ì „ì²´ í…ŒìŠ¤íŠ¸
        asyncio.run(test_continuous_collection())