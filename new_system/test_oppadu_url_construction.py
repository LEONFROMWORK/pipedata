#!/usr/bin/env python3
"""
ì˜¤ë¹ ë‘ URL êµ¬ì„± ë°©ë²• í…ŒìŠ¤íŠ¸
ì˜¬ë°”ë¥¸ ê²Œì‹œê¸€ URL í˜•ì‹ ì°¾ê¸°
"""

import cloudscraper
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def test_oppadu_url_construction():
    """ë‹¤ì–‘í•œ URL êµ¬ì„± ë°©ë²• í…ŒìŠ¤íŠ¸"""
    
    base_url = "https://www.oppadu.com"
    community_url = f"{base_url}/community/question/"
    
    # í…ŒìŠ¤íŠ¸í•  ìƒ˜í”Œ href (ëª©ë¡ì—ì„œ ì¶”ì¶œí•œ ê°’)
    sample_href = "?board_id=1&action=view&uid=79638&pg=1"
    
    # CloudScraper ì„¤ì •
    scraper = cloudscraper.create_scraper(
        browser={
            'browser': 'chrome',
            'platform': 'windows',
            'desktop': True
        }
    )
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
        'Referer': community_url
    }
    scraper.headers.update(headers)
    
    print("ğŸ” ì˜¤ë¹ ë‘ URL êµ¬ì„± ë°©ë²• í…ŒìŠ¤íŠ¸")
    print("="*80)
    print(f"ìƒ˜í”Œ href: {sample_href}")
    print()
    
    # ë‹¤ì–‘í•œ URL êµ¬ì„± ë°©ë²• í…ŒìŠ¤íŠ¸
    url_variants = [
        ("1. urljoin(base_url, href)", urljoin(base_url, sample_href)),
        ("2. urljoin(community_url, href)", urljoin(community_url, sample_href)),
        ("3. base_url + href", base_url + sample_href),
        ("4. community_url + href", community_url + sample_href),
        ("5. base_url + /community/question/ + href", f"{base_url}/community/question/{sample_href}"),
        ("6. ì ˆëŒ€ ê²½ë¡œ", f"{base_url}/community/question{sample_href}"),
    ]
    
    for method_name, test_url in url_variants:
        print(f"\nğŸ§ª {method_name}")
        print(f"   URL: {test_url}")
        
        try:
            # ìš”ì²­ ë³´ë‚´ê¸° (ë¦¬ë””ë ‰ì…˜ í—ˆìš© ì•ˆí•¨ìœ¼ë¡œ ìƒíƒœ í™•ì¸)
            response = scraper.get(test_url, timeout=10, allow_redirects=False)
            print(f"   ìƒíƒœ ì½”ë“œ: {response.status_code}")
            
            if response.status_code in [301, 302, 303, 307, 308]:
                redirect_location = response.headers.get('Location', 'N/A')
                print(f"   ğŸ”„ ë¦¬ë””ë ‰ì…˜: {redirect_location}")
                
                # ë¦¬ë””ë ‰ì…˜ëœ í˜ì´ì§€ë„ í™•ì¸
                if redirect_location and redirect_location != '/':
                    final_response = scraper.get(redirect_location, timeout=10)
                    final_soup = BeautifulSoup(final_response.text, 'html.parser')
                    final_title = final_soup.find('title')
                    if final_title:
                        print(f"   ğŸ¯ ìµœì¢… í˜ì´ì§€ ì œëª©: {final_title.get_text()}")
            
            elif response.status_code == 200:
                # ì„±ê³µì ì¸ ì‘ë‹µì˜ ê²½ìš° ë‚´ìš© ë¶„ì„
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # í˜ì´ì§€ ì œëª©
                title = soup.find('title')
                page_title = title.get_text() if title else "ì œëª© ì—†ìŒ"
                print(f"   ğŸ“„ í˜ì´ì§€ ì œëª©: {page_title}")
                
                # ê²Œì‹œê¸€ ì½˜í…ì¸  vs í™ˆí˜ì´ì§€ íŒë‹¨
                is_homepage = False
                is_post = False
                
                # í™ˆí˜ì´ì§€ í‘œì‹œ ìš”ì†Œë“¤
                home_indicators = [
                    soup.find_all(class_='slider-contents'),
                    soup.find_all(class_='main-page-list'),
                    soup.find(string=lambda text: text and "ì—‘ì…€ê°•ì˜ ëŒ€í‘œì±„ë„" in text)
                ]
                
                if any(home_indicators):
                    is_homepage = True
                    print(f"   âŒ í™ˆí˜ì´ì§€ ì½˜í…ì¸  ê°ì§€")
                
                # ê²Œì‹œê¸€ ì½˜í…ì¸  ìš”ì†Œë“¤
                post_indicators = [
                    ('post-content', soup.find(class_='post-content')),
                    ('article-content', soup.find(class_='article-content')),
                    ('board-content', soup.find(class_='board-content')),
                    ('xe-content', soup.find(class_='xe-content')),
                    ('view-content', soup.find(class_='view-content'))
                ]
                
                for indicator_name, element in post_indicators:
                    if element:
                        content_text = element.get_text(strip=True)
                        if len(content_text) > 30:
                            is_post = True
                            print(f"   âœ… {indicator_name} ë°œê²¬: {content_text[:80]}...")
                            break
                
                if not is_homepage and not is_post:
                    print(f"   â“ ì½˜í…ì¸  ìœ í˜• ë¶ˆëª…í™•")
                
                # ê²°ë¡ 
                if is_post and not is_homepage:
                    print(f"   ğŸ¯ ê²°ë¡ : ì˜¬ë°”ë¥¸ ê²Œì‹œê¸€ í˜ì´ì§€!")
                elif is_homepage:
                    print(f"   âŒ ê²°ë¡ : í™ˆí˜ì´ì§€ë¡œ ë¦¬ë””ë ‰ì…˜ë¨")
                else:
                    print(f"   â“ ê²°ë¡ : íŒë‹¨ ë¶ˆê°€")
            
            else:
                print(f"   âŒ HTTP ì˜¤ë¥˜: {response.status_code}")
        
        except Exception as e:
            print(f"   âŒ ìš”ì²­ ì‹¤íŒ¨: {e}")
    
    print(f"\n" + "="*80)
    print("ğŸ“Š ê²°ë¡  ë° ê¶Œì¥ì‚¬í•­")
    print("="*80)

if __name__ == "__main__":
    test_oppadu_url_construction()