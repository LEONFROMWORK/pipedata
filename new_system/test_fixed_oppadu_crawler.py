#!/usr/bin/env python3
"""
ìˆ˜ì •ëœ ì˜¤ë¹ ë‘ í¬ë¡¤ëŸ¬ ì „ì²´ í…ŒìŠ¤íŠ¸
ì‹¤ì œ Q&A ë°ì´í„°ë¥¼ ì—¬ëŸ¬ ê°œ ìˆ˜ì§‘í•´ì„œ ê²€ì¦
"""

import asyncio
import logging
from pathlib import Path
import sys
import json

# Add project root to path
sys.path.append(str(Path(__file__).parent))

from collectors.oppadu_crawler import OppaduCrawler
from core.cache import APICache, LocalCache

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def test_fixed_oppadu_crawler():
    """ìˆ˜ì •ëœ ì˜¤ë¹ ë‘ í¬ë¡¤ëŸ¬ ì „ì²´ í…ŒìŠ¤íŠ¸"""
    
    logger.info("ğŸ§ª ìˆ˜ì •ëœ ì˜¤ë¹ ë‘ í¬ë¡¤ëŸ¬ ì „ì²´ í…ŒìŠ¤íŠ¸")
    logger.info("=" * 80)
    
    try:
        # ìºì‹œ ë° í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        local_cache = LocalCache(db_path=Path("/tmp/test_fixed_oppadu.db"))
        cache = APICache(local_cache)
        crawler = OppaduCrawler(cache)
        
        # 1. ì†ŒëŸ‰ ë°ì´í„° ìˆ˜ì§‘ (ìµœëŒ€ 3ê°œ ê²Œì‹œê¸€)
        logger.info("ğŸ“Š ì†ŒëŸ‰ Q&A ë°ì´í„° ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ (ìµœëŒ€ 1í˜ì´ì§€)")
        collected_data = await crawler.collect_oppadu_questions(max_pages=1)
        
        if collected_data:
            logger.info(f"âœ… {len(collected_data)}ê°œì˜ Q&A ìˆ˜ì§‘ ì™„ë£Œ")
            
            # ìˆ˜ì§‘ëœ ë°ì´í„° ê²€ì¦
            logger.info("\nğŸ“‹ ìˆ˜ì§‘ëœ ë°ì´í„° ê²€ì¦:")
            
            valid_count = 0
            for i, item in enumerate(collected_data, 1):
                logger.info(f"\n--- Q&A {i} ---")
                logger.info(f"ì œëª©: {item.get('title', 'N/A')}")
                logger.info(f"URL: {item.get('url', 'N/A')}")
                logger.info(f"Excel ë²„ì „: {item.get('metadata', {}).get('excel_version', 'N/A')}")
                logger.info(f"OS ë²„ì „: {item.get('metadata', {}).get('os_version', 'N/A')}")
                
                question = item.get('question', {})
                answer = item.get('answer', {})
                
                q_text_len = len(question.get('text', ''))
                a_text_len = len(answer.get('text', ''))
                
                logger.info(f"ì§ˆë¬¸ ê¸¸ì´: {q_text_len} ë¬¸ì")
                logger.info(f"ë‹µë³€ ê¸¸ì´: {a_text_len} ë¬¸ì")
                logger.info(f"í’ˆì§ˆ ì ìˆ˜: {item.get('quality_score', 'N/A')}")
                
                # ì§ˆë¬¸ ë¯¸ë¦¬ë³´ê¸°
                if q_text_len > 0:
                    logger.info(f"ì§ˆë¬¸ ë¯¸ë¦¬ë³´ê¸°: {question.get('text', '')[:100]}...")
                
                # ë‹µë³€ ë¯¸ë¦¬ë³´ê¸°
                if a_text_len > 0:
                    logger.info(f"ë‹µë³€ ë¯¸ë¦¬ë³´ê¸°: {answer.get('text', '')[:100]}...")
                
                # ìœ íš¨ì„± ê²€ì¦
                is_valid = (
                    item.get('title', '').strip() != '' and
                    q_text_len > 10 and
                    a_text_len > 5 and
                    item.get('url', '').startswith('https://www.oppadu.com/community/question/?')
                )
                
                if is_valid:
                    valid_count += 1
                    logger.info("âœ… ìœ íš¨í•œ Q&A ë°ì´í„°")
                else:
                    logger.info("âŒ ë°ì´í„° í’ˆì§ˆ ë¬¸ì œ")
            
            # ì „ì²´ ê²°ê³¼ ìš”ì•½
            logger.info(f"\n" + "=" * 80)
            logger.info("ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½")
            logger.info("=" * 80)
            logger.info(f"ì´ ìˆ˜ì§‘: {len(collected_data)}ê°œ")
            logger.info(f"ìœ íš¨ ë°ì´í„°: {valid_count}ê°œ")
            logger.info(f"ì„±ê³µë¥ : {valid_count/len(collected_data)*100:.1f}%" if collected_data else "0%")
            
            if valid_count > 0:
                logger.info("ğŸ‰ ì˜¤ë¹ ë‘ í¬ë¡¤ëŸ¬ ìˆ˜ì • ì„±ê³µ!")
                logger.info("   - URL êµ¬ì„± ë¬¸ì œ í•´ê²°")
                logger.info("   - ì‹¤ì œ Q&A ì½˜í…ì¸  ì¶”ì¶œ")
                logger.info("   - ë‹µë³€ ì¶”ì¶œ ë¡œì§ ê°œì„ ")
                
                # ìƒ˜í”Œ ë°ì´í„° JSONìœ¼ë¡œ ì €ì¥
                sample_data = collected_data[:2]  # ì²˜ìŒ 2ê°œë§Œ
                output_file = Path("/tmp/oppadu_sample_data.json")
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(sample_data, f, ensure_ascii=False, indent=2)
                logger.info(f"ğŸ“ ìƒ˜í”Œ ë°ì´í„° ì €ì¥: {output_file}")
            else:
                logger.error("âŒ ìœ íš¨í•œ ë°ì´í„°ê°€ ìˆ˜ì§‘ë˜ì§€ ì•ŠìŒ")
        else:
            logger.error("âŒ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
            
    except Exception as e:
        logger.error(f"âŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        logger.error(f"Traceback: {traceback.format_exc()}")

if __name__ == "__main__":
    asyncio.run(test_fixed_oppadu_crawler())