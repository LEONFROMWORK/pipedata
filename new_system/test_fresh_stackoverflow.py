#!/usr/bin/env python3
"""
ì™„ì „íˆ ìƒˆë¡œìš´ Stack Overflow ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸
- ëª¨ë“  ìºì‹œì™€ ì¤‘ë³µ ê²€ì¶œ ì™„ì „ ì´ˆê¸°í™”
- ìƒˆë¡œìš´ ìˆ˜ì§‘ê¸° ì¸ìŠ¤í„´ìŠ¤ë¡œ 10ê°œ ìˆ˜ì§‘
"""
import asyncio
import json
import sqlite3
import sys
import shutil
from pathlib import Path
from datetime import datetime, timedelta

sys.path.insert(0, str(Path(__file__).parent))

from config import Config
from core.cache import LocalCache, APICache
from collectors.fixed_stackoverflow_collector import FixedStackOverflowCollector

def complete_reset():
    """ëª¨ë“  ë°ì´í„° ì™„ì „ ì´ˆê¸°í™”"""
    print("ğŸ”„ Stack Overflow ë°ì´í„° ì™„ì „ ì´ˆê¸°í™”")
    
    try:
        # 1. ìºì‹œ ë°ì´í„°ë² ì´ìŠ¤ ì™„ì „ ì‚­ì œ
        cache_db = Config.DATABASE_PATH
        if cache_db.exists():
            cache_db.unlink()
            print(f"   âœ… ìºì‹œ ë°ì´í„°ë² ì´ìŠ¤ ì‚­ì œ: {cache_db}")
        
        # 2. ì¤‘ë³µ ì¶”ì ê¸° ë°ì´í„°ë² ì´ìŠ¤ ì‚­ì œ
        dedup_db = Path("/Users/kevin/bigdata/data/deduplication_tracker.db")
        if dedup_db.exists():
            dedup_db.unlink()
            print(f"   âœ… ì¤‘ë³µ ì¶”ì ê¸° ì‚­ì œ: {dedup_db}")
        
        # 3. ë””ë ‰í† ë¦¬ ë‹¤ì‹œ ìƒì„±
        Config.ensure_directories()
        print("   âœ… ë””ë ‰í† ë¦¬ ì¬ìƒì„± ì™„ë£Œ")
        
        print("ğŸ¯ ì™„ì „ ì´ˆê¸°í™” ì™„ë£Œ - ëª¨ë“  ë°ì´í„°ê°€ ìƒˆë¡­ê²Œ ìˆ˜ì§‘ë©ë‹ˆë‹¤")
        
    except Exception as e:
        print(f"âŒ ì™„ì „ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

async def fresh_stackoverflow_collection():
    """ì™„ì „íˆ ìƒˆë¡œìš´ Stack Overflow ìˆ˜ì§‘"""
    print("\nğŸ†• ì™„ì „íˆ ìƒˆë¡œìš´ Stack Overflow ìˆ˜ì§‘")
    print("=" * 60)
    
    try:
        # ìƒˆë¡œìš´ ìˆ˜ì§‘ê¸° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        local_cache = LocalCache(Config.DATABASE_PATH)
        api_cache = APICache(local_cache)
        collector = FixedStackOverflowCollector(api_cache)
        
        print("âœ… ìƒˆë¡œìš´ ìˆ˜ì§‘ê¸° ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì™„ë£Œ")
        print("ğŸ¯ ëª©í‘œ: 10ê°œ ê³ í’ˆì§ˆ Q&A ìˆ˜ì§‘")
        
        # ë” ë„“ì€ ê¸°ê°„ìœ¼ë¡œ ì„¤ì • (ë” ë§ì€ ë°ì´í„° í™•ë³´)
        from_date = datetime.now() - timedelta(days=90)  # 3ê°œì›”
        print(f"ğŸ“… ìˆ˜ì§‘ ê¸°ê°„: {from_date.strftime('%Y-%m-%d')} ~ í˜„ì¬ (3ê°œì›”)")
        
        print("\nğŸš€ ìƒˆë¡œìš´ ìˆ˜ì§‘ ì‹œì‘...")
        start_time = datetime.now()
        
        # ë‹¨ì¼ í˜¸ì¶œë¡œ ì¶©ë¶„í•œ ë°ì´í„° ìˆ˜ì§‘
        qa_pairs = await collector.collect_excel_questions_fixed(
            from_date=from_date,
            max_pages=3  # 3í˜ì´ì§€ë¡œ ì¶©ë¶„í•œ ë°ì´í„° í™•ë³´
        )
        
        end_time = datetime.now()
        duration = end_time - start_time
        
        print(f"\nâ±ï¸ ìˆ˜ì§‘ ì™„ë£Œ (ì†Œìš” ì‹œê°„: {duration.total_seconds():.1f}ì´ˆ)")
        print(f"ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼: {len(qa_pairs)}ê°œ Q&A ìŒ")
        
        if qa_pairs:
            # 10ê°œë§Œ ì„ ë³„ (ê³ í’ˆì§ˆ ìˆœìœ¼ë¡œ)
            target_count = 10
            selected_pairs = sorted(qa_pairs, key=lambda x: x.get('quality_score', 0), reverse=True)[:target_count]
            
            print(f"ğŸ¯ ê³ í’ˆì§ˆ {len(selected_pairs)}ê°œ ì„ ë³„ ì™„ë£Œ")
            
            # ìƒì„¸ ë¶„ì„
            print(f"\nğŸ” ì„ ë³„ëœ ë°ì´í„° ë¶„ì„:")
            
            complete_pairs = sum(1 for pair in selected_pairs if pair.get('answer'))
            print(f"   ì™„ì „í•œ Q&A ìŒ: {complete_pairs}/{len(selected_pairs)} ({complete_pairs/len(selected_pairs)*100:.1f}%)")
            
            # í’ˆì§ˆ ë¶„ì„
            quality_scores = [pair.get('quality_score', 0) for pair in selected_pairs]
            question_scores = [pair['question'].get('score', 0) for pair in selected_pairs]
            answer_scores = [pair['answer'].get('score', 0) for pair in selected_pairs if pair.get('answer')]
            
            print(f"   í’ˆì§ˆ ì ìˆ˜: {min(quality_scores)} ~ {max(quality_scores)} (í‰ê· : {sum(quality_scores)/len(quality_scores):.1f})")
            print(f"   ì§ˆë¬¸ ì ìˆ˜: {min(question_scores)} ~ {max(question_scores)} (í‰ê· : {sum(question_scores)/len(question_scores):.1f})")
            if answer_scores:
                print(f"   ë‹µë³€ ì ìˆ˜: {min(answer_scores)} ~ {max(answer_scores)} (í‰ê· : {sum(answer_scores)/len(answer_scores):.1f})")
            
            # Excel í•¨ìˆ˜ ë¶„ì„
            excel_functions = ['IF', 'VLOOKUP', 'INDEX', 'MATCH', 'SUMIF', 'COUNTIF', 'LAMBDA', 'LET', 'XLOOKUP']
            function_counts = {func: 0 for func in excel_functions}
            
            for pair in selected_pairs:
                full_text = (
                    pair['question'].get('title', '') + ' ' + 
                    pair['question'].get('body_markdown', '') + ' ' + 
                    pair.get('answer', {}).get('body_markdown', '')
                ).upper()
                
                for func in excel_functions:
                    if func in full_text:
                        function_counts[func] += 1
            
            print(f"\nğŸ”§ Excel í•¨ìˆ˜ ì–¸ê¸‰:")
            for func, count in sorted(function_counts.items(), key=lambda x: x[1], reverse=True):
                if count > 0:
                    print(f"   {func}: {count}íšŒ")
            
            # ìƒìœ„ 5ê°œ Q&A ìƒ˜í”Œ ì¶œë ¥
            print(f"\nğŸ“ ìƒìœ„ 5ê°œ Q&A ìƒ˜í”Œ:")
            for i, pair in enumerate(selected_pairs[:5], 1):
                question = pair['question']
                answer = pair.get('answer', {})
                
                print(f"\n   {i}. í’ˆì§ˆì ìˆ˜: {pair.get('quality_score', 0)}")
                print(f"      ID: {question.get('question_id')}")
                print(f"      ì œëª©: {question.get('title', 'N/A')[:80]}...")
                print(f"      ì ìˆ˜: Q{question.get('score', 0)}/A{answer.get('score', 0)}")
                print(f"      íƒœê·¸: {', '.join(question.get('tags', []))}")
                print(f"      ì¡°íšŒìˆ˜: {question.get('view_count', 0):,}")
                
                # ì§ˆë¬¸ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
                q_body = question.get('body_markdown', '')[:200]
                print(f"      ì§ˆë¬¸: {q_body}...")
                
                # ë‹µë³€ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
                a_body = answer.get('body_markdown', '')[:200]
                print(f"      ë‹µë³€: {a_body}...")
            
            # ë°ì´í„° ì €ì¥ (ì˜¤ë¹ ë‘ë‚˜/ë ˆë”§ê³¼ ë™ì¼í•œ ìœ„ì¹˜)
            now = datetime.now()
            partition_dir = Path(Config.OUTPUT_DIR) / f"year={now.year}" / f"month={now.month:02d}" / f"day={now.day:02d}"
            partition_dir.mkdir(parents=True, exist_ok=True)
            
            timestamp = now.strftime('%H%M%S')
            output_file = partition_dir / f"stackoverflow_{now.strftime('%Y%m%d')}.jsonl"
            metadata_file = partition_dir / f"stackoverflow_collection_metadata_{timestamp}.json"
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥ (ì˜¤ë¹ ë‘ë‚˜/ë ˆë”§ í˜•ì‹ê³¼ ë™ì¼)
            metadata = {
                'source': 'stackoverflow',
                'collection_method': 'fresh_api_collection',
                'collected_at': datetime.now().isoformat(),
                'target_count': target_count,
                'actual_count': len(selected_pairs),
                'complete_pairs': complete_pairs,
                'collection_duration_seconds': duration.total_seconds(),
                'success': len(selected_pairs) >= target_count,
                'collection_stats': {
                    'total_available': len(qa_pairs),
                    'selected': len(selected_pairs),
                    'quality_threshold': 'top_10_by_score',
                    'avg_quality_score': sum(quality_scores) / len(quality_scores) if quality_scores else 0
                }
            }
            
            # JSONL í˜•ì‹ìœ¼ë¡œ ì €ì¥ (ì˜¤ë¹ ë‘ë‚˜/ë ˆë”§ê³¼ ë™ì¼)
            with open(output_file, 'w', encoding='utf-8') as f:
                for i, pair in enumerate(selected_pairs, 1):
                    qa_item = {
                        'id': f"stackoverflow_{pair['question'].get('question_id')}",
                        'rank': i,
                        'question': pair['question'],
                        'answer': pair.get('answer'),
                        'quality_score': pair.get('quality_score', 0),
                        'source': 'stackoverflow_api',
                        'collected_at': datetime.now().isoformat()
                    }
                    f.write(json.dumps(qa_item, ensure_ascii=False) + '\n')
            
            # ë©”íƒ€ë°ì´í„° íŒŒì¼ ì €ì¥
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False, default=str)
            
            print(f"\nğŸ’¾ ìˆ˜ì§‘ ë°ì´í„° ì €ì¥ (ì˜¤ë¹ ë‘ë‚˜/ë ˆë”§ê³¼ ë™ì¼í•œ ìœ„ì¹˜):")
            print(f"   ğŸ“ ë””ë ‰í† ë¦¬: {partition_dir}")
            print(f"   ğŸ“„ ë°ì´í„° íŒŒì¼: {output_file.name}")
            print(f"   ğŸ“„ ë©”íƒ€ë°ì´í„°: {metadata_file.name}")
            print(f"   ğŸ’¾ í¬ê¸°: {output_file.stat().st_size:,} bytes")
            
            # API ì‚¬ìš©ëŸ‰ ì²´í¬
            stats = collector.get_collection_stats()
            print(f"\nğŸ“Š API ì‚¬ìš© í†µê³„:")
            print(f"   ì˜¤ëŠ˜ ì‚¬ìš©ëŸ‰: {stats['requests_today']}")
            print(f"   ë‚¨ì€ í• ë‹¹ëŸ‰: {stats['daily_quota_remaining']}")
            
            # ì„±ê³µ íŒì •
            success = len(selected_pairs) >= target_count and complete_pairs == len(selected_pairs)
            
            if success:
                print(f"\nğŸ‰ Stack Overflow ìƒˆë¡œìš´ ìˆ˜ì§‘ ì„±ê³µ!")
                print(f"   âœ… ëª©í‘œ ë‹¬ì„±: {len(selected_pairs)}/{target_count}ê°œ")
                print(f"   âœ… 100% ì™„ì„±ë„: ëª¨ë“  Q&Aì— ë‹µë³€ í¬í•¨")
                print(f"   âœ… ê³ í’ˆì§ˆ Excel ê´€ë ¨ ë°ì´í„°")
                print(f"   ğŸš€ ì˜¤ë¹ ë‘ë‚˜/ë ˆë”§ê³¼ ë™ì¼í•œ ìˆ˜ì¤€ì˜ ì•ˆì •ì„±")
            else:
                print(f"\nâš ï¸ ë¶€ë¶„ ì„±ê³µ")
                print(f"   ğŸ“Š ìˆ˜ì§‘ëŸ‰: {len(selected_pairs)}/{target_count}ê°œ")
                print(f"   ğŸ“Š ì™„ì„±ë„: {complete_pairs}/{len(selected_pairs)}ê°œ")
            
            await collector.close()
            return selected_pairs
        
        else:
            print("âŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            await collector.close()
            return []
        
    except Exception as e:
        print(f"âŒ ìƒˆë¡œìš´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return []

def verify_data_freshness():
    """ë°ì´í„° ì‹ ì„ ë„ ê²€ì¦ (ì˜¤ë¹ ë‘ë‚˜/ë ˆë”§ê³¼ ë™ì¼í•œ ìœ„ì¹˜ì—ì„œ)"""
    print(f"\nğŸ” ë°ì´í„° ì‹ ì„ ë„ ê²€ì¦")
    print("=" * 40)
    
    try:
        # ì˜¤ëŠ˜ ë‚ ì§œ ë””ë ‰í† ë¦¬ì—ì„œ Stack Overflow íŒŒì¼ ì°¾ê¸°
        now = datetime.now()
        partition_dir = Path(Config.OUTPUT_DIR) / f"year={now.year}" / f"month={now.month:02d}" / f"day={now.day:02d}"
        
        if not partition_dir.exists():
            print("âŒ ì˜¤ëŠ˜ ë‚ ì§œ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # Stack Overflow ë©”íƒ€ë°ì´í„° íŒŒì¼ ì°¾ê¸°
        metadata_files = list(partition_dir.glob("stackoverflow_collection_metadata_*.json"))
        
        if metadata_files:
            latest_metadata_file = max(metadata_files, key=lambda f: f.stat().st_mtime)
            
            with open(latest_metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            # í•´ë‹¹í•˜ëŠ” JSONL íŒŒì¼ ì°¾ê¸°
            jsonl_file = partition_dir / f"stackoverflow_{now.strftime('%Y%m%d')}.jsonl"
            
            print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {partition_dir}")
            print(f"ğŸ“„ ë©”íƒ€ë°ì´í„°: {latest_metadata_file.name}")
            print(f"ğŸ“„ ë°ì´í„° íŒŒì¼: {jsonl_file.name if jsonl_file.exists() else 'ì—†ìŒ'}")
            print(f"ğŸ“… ìˆ˜ì§‘ ì‹œê°„: {metadata.get('collected_at', 'N/A')}")
            print(f"ğŸ“Š ìˆ˜ì§‘ ì„±ê³µ: {'âœ…' if metadata.get('success') else 'âŒ'}")
            print(f"ğŸ¯ ëª©í‘œ/ì‹¤ì œ: {metadata.get('target_count')}/{metadata.get('actual_count')}ê°œ")
            print(f"âœ… ì™„ì„±ë„: {metadata.get('complete_pairs')}/{metadata.get('actual_count')}ê°œ")
            
            # JSONL íŒŒì¼ì—ì„œ ì²« ë²ˆì§¸ í•­ëª© í™•ì¸
            if jsonl_file.exists():
                with open(jsonl_file, 'r', encoding='utf-8') as f:
                    first_line = f.readline()
                    if first_line:
                        first_qa = json.loads(first_line)
                        print(f"\nğŸ“‹ ìµœê³  í’ˆì§ˆ Q&A:")
                        print(f"   ìˆœìœ„: #{first_qa.get('rank')}")
                        print(f"   í’ˆì§ˆì ìˆ˜: {first_qa.get('quality_score')}")
                        print(f"   ì œëª©: {first_qa['question'].get('title', 'N/A')[:60]}...")
        else:
            print("âŒ Stack Overflow ë©”íƒ€ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        print(f"âŒ ì‹ ì„ ë„ ê²€ì¦ ì‹¤íŒ¨: {e}")

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ Stack Overflow ì™„ì „ ìƒˆë¡œìš´ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸")
    print("=" * 70)
    print("ğŸ¯ ëª©í‘œ: ì˜¤ë¹ ë‘ë‚˜/ë ˆë”§ì²˜ëŸ¼ ì•ˆì •ì ìœ¼ë¡œ 10ê°œ Q&A ìˆ˜ì§‘")
    
    # 1. ì™„ì „ ì´ˆê¸°í™”
    complete_reset()
    
    # 2. ìƒˆë¡œìš´ ìˆ˜ì§‘ ì‹¤í–‰
    results = await fresh_stackoverflow_collection()
    
    # 3. ë°ì´í„° ì‹ ì„ ë„ ê²€ì¦
    verify_data_freshness()
    
    # 4. ìµœì¢… ê²°ê³¼
    print(f"\nğŸ Stack Overflow ìƒˆë¡œìš´ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    
    if results and len(results) >= 10:
        print(f"   ğŸ‰ ì™„ì „ ì„±ê³µ: {len(results)}ê°œ ê³ í’ˆì§ˆ Q&A ìˆ˜ì§‘")
        print(f"   âœ… ì˜¤ë¹ ë‘ë‚˜/ë ˆë”§ ìˆ˜ì¤€ì˜ ì•ˆì •ì  ìˆ˜ì§‘ ë‹¬ì„±")
        print(f"   ğŸš€ Stack Overflow ìˆ˜ì§‘ê¸° ìš´ì˜ ì¤€ë¹„ ì™„ë£Œ")
    elif results:
        print(f"   âš ï¸ ë¶€ë¶„ ì„±ê³µ: {len(results)}ê°œ Q&A ìˆ˜ì§‘")
        print(f"   ğŸ“ˆ ìˆ˜ì§‘ ë²”ìœ„ í™•ì¥ìœ¼ë¡œ ëª©í‘œ ë‹¬ì„± ê°€ëŠ¥")
    else:
        print(f"   âŒ ìˆ˜ì§‘ ì‹¤íŒ¨")
        print(f"   ğŸ”§ API í‚¤ ë˜ëŠ” ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ ì ê²€ í•„ìš”")

if __name__ == "__main__":
    asyncio.run(main())