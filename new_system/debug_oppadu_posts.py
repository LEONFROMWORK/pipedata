#!/usr/bin/env python3
"""
ì˜¤ë¹ ë‘ ê²Œì‹œê¸€ URL ì¶”ì¶œ ìƒì„¸ ë¶„ì„
í˜„ì¬ í¬ë¡¤ëŸ¬ê°€ ì¶”ì¶œí•˜ëŠ” URLë“¤ì´ ì‹¤ì œ Q&A ê²Œì‹œê¸€ì¸ì§€ í™•ì¸
"""

import cloudscraper
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def debug_oppadu_posts():
    """ì˜¤ë¹ ë‘ ê²Œì‹œê¸€ URL ì¶”ì¶œ ë¡œì§ ë””ë²„ê¹…"""
    
    base_url = "https://www.oppadu.com"
    url = f"{base_url}/community/question/"
    
    # CloudScraper ì„¤ì •
    scraper = cloudscraper.create_scraper(
        browser={
            'browser': 'chrome',
            'platform': 'windows',
            'desktop': True
        }
    )
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
        'Referer': 'https://www.oppadu.com/'
    }
    scraper.headers.update(headers)
    
    try:
        print(f"ğŸ” ì˜¤ë¹ ë‘ ê²Œì‹œê¸€ URL ì¶”ì¶œ ë¶„ì„: {url}")
        response = scraper.get(url, timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        print("\n" + "="*80)
        print("ğŸ“‹ POST URL EXTRACTION ANALYSIS")
        print("="*80)
        
        # 1. post-list-modern ì»¨í…Œì´ë„ˆ í™•ì¸
        post_list = soup.find('div', class_='post-list-modern')
        if not post_list:
            print("âŒ post-list-modern ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return
        else:
            print("âœ… post-list-modern ì»¨í…Œì´ë„ˆ ë°œê²¬")
        
        # 2. post-item-modern í•­ëª©ë“¤ í™•ì¸
        post_items = post_list.find_all('div', class_='post-item-modern')
        print(f"ğŸ“ ì´ {len(post_items)}ê°œì˜ post-item-modern ë°œê²¬")
        
        answered_posts = []
        all_posts = []
        
        # 3. ê° ê²Œì‹œê¸€ í•­ëª© ë¶„ì„
        for i, item in enumerate(post_items):
            print(f"\n--- ê²Œì‹œê¸€ {i+1} ë¶„ì„ ---")
            
            # ë‹µë³€ ì™„ë£Œ ë°°ì§€ í™•ì¸
            answer_badge = item.find(class_='answer-complete-badge')
            has_answer = answer_badge is not None
            print(f"  ë‹µë³€ ì™„ë£Œ ë°°ì§€: {'âœ… ìˆìŒ' if has_answer else 'âŒ ì—†ìŒ'}")
            
            if answer_badge:
                badge_text = answer_badge.get_text(strip=True)
                print(f"  ë°°ì§€ í…ìŠ¤íŠ¸: '{badge_text}'")
            
            # ê²Œì‹œê¸€ ë§í¬ ì¶”ì¶œ
            link_element = item.find('a', href=True)
            if link_element:
                post_url = urljoin(base_url, link_element['href'])
                post_title = link_element.get_text(strip=True)
                
                print(f"  ì œëª©: {post_title[:50]}...")
                print(f"  URL: {post_url}")
                
                all_posts.append({
                    'title': post_title,
                    'url': post_url,
                    'has_answer': has_answer
                })
                
                if has_answer:
                    answered_posts.append(post_url)
                    print("  ğŸ¯ ë‹µë³€ ì™„ë£Œ ê²Œì‹œê¸€ë¡œ ë¶„ë¥˜ë¨")
            else:
                print("  âŒ ë§í¬ ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            
            # HTML êµ¬ì¡° í™•ì¸
            print(f"  HTML êµ¬ì¡°: {str(item)[:200]}...")
        
        # 4. ê²°ê³¼ ìš”ì•½
        print(f"\n" + "="*80)
        print(f"ğŸ“Š ë¶„ì„ ê²°ê³¼ ìš”ì•½")
        print(f"="*80)
        print(f"ì „ì²´ ê²Œì‹œê¸€: {len(all_posts)}ê°œ")
        print(f"ë‹µë³€ ì™„ë£Œ ê²Œì‹œê¸€: {len(answered_posts)}ê°œ")
        print(f"ë‹µë³€ ì™„ë£Œìœ¨: {len(answered_posts)/len(all_posts)*100:.1f}%" if all_posts else "0%")
        
        # 5. ë‹µë³€ ì™„ë£Œ ê²Œì‹œê¸€ ëª©ë¡
        if answered_posts:
            print(f"\nâœ… ë‹µë³€ ì™„ë£Œ ê²Œì‹œê¸€ URL ëª©ë¡:")
            for i, url in enumerate(answered_posts, 1):
                print(f"  {i}. {url}")
        
        # 6. URL íŒ¨í„´ ë¶„ì„
        print(f"\nğŸ”— URL íŒ¨í„´ ë¶„ì„:")
        url_patterns = {}
        for post in all_posts:
            url = post['url']
            if '?board_id=' in url and 'action=view' in url and 'uid=' in url:
                url_patterns['standard_view'] = url_patterns.get('standard_view', 0) + 1
            elif '/community/question/' == url:
                url_patterns['base_page'] = url_patterns.get('base_page', 0) + 1
            else:
                url_patterns['other'] = url_patterns.get('other', 0) + 1
        
        for pattern, count in url_patterns.items():
            print(f"  {pattern}: {count}ê°œ")
        
        # 7. ë¬¸ì œ ì§„ë‹¨
        print(f"\nğŸ” ë¬¸ì œ ì§„ë‹¨:")
        base_page_count = url_patterns.get('base_page', 0)
        if base_page_count > 0:
            print(f"  âš ï¸  WARNING: {base_page_count}ê°œì˜ URLì´ ê¸°ë³¸ í˜ì´ì§€ë¡œ ì—°ê²°ë¨")
            print(f"     ì´ëŠ” ì‹¤ì œ ê²Œì‹œê¸€ì´ ì•„ë‹Œ í”„ë¡œëª¨ì…˜ ì½˜í…ì¸ ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ")
        
        standard_count = url_patterns.get('standard_view', 0)
        if standard_count > 0:
            print(f"  âœ… {standard_count}ê°œì˜ í‘œì¤€ ê²Œì‹œê¸€ URL ë°œê²¬")
        
        # 8. ìƒ˜í”Œ ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸ (ë‹µë³€ ì™„ë£Œëœ ê²ƒ ì¤‘ ì²« ë²ˆì§¸)
        if answered_posts:
            print(f"\nğŸ” ìƒ˜í”Œ ê²Œì‹œê¸€ ë‚´ìš© í™•ì¸:")
            sample_url = answered_posts[0]
            print(f"URL: {sample_url}")
            
            try:
                sample_response = scraper.get(sample_url, timeout=30)
                sample_soup = BeautifulSoup(sample_response.text, 'html.parser')
                
                # ì œëª© í™•ì¸
                title = sample_soup.find('h1') or sample_soup.find(class_='post-title')
                if title:
                    print(f"ì œëª©: {title.get_text(strip=True)}")
                
                # ì§ˆë¬¸ ë‚´ìš© í™•ì¸
                post_content = sample_soup.find(class_='post-content')
                if post_content:
                    content_text = post_content.get_text(strip=True)[:200]
                    print(f"ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {content_text}...")
                
                # ë‹µë³€ í™•ì¸
                answer_badge = sample_soup.find(class_='selected-answer-badge')
                if answer_badge:
                    print("âœ… ì±„íƒëœ ë‹µë³€ ë°œê²¬")
                else:
                    print("âŒ ì±„íƒëœ ë‹µë³€ ë¯¸ë°œê²¬")
                
            except Exception as e:
                print(f"ìƒ˜í”Œ ê²Œì‹œê¸€ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return all_posts, answered_posts
        
    except Exception as e:
        print(f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
        return None, None

if __name__ == "__main__":
    debug_oppadu_posts()