#!/usr/bin/env python3
"""
ìˆ˜ì§‘ëœ Stack Overflow ë°ì´í„°ì˜ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ë° ì¡°íšŒ í…ŒìŠ¤íŠ¸
"""
import asyncio
import json
import sqlite3
import sys
from pathlib import Path
from datetime import datetime

sys.path.insert(0, str(Path(__file__).parent))

from config import Config
from core.cache import LocalCache, APICache

def extract_cached_stackoverflow_data():
    """ìºì‹œì—ì„œ Stack Overflow ë°ì´í„° ì¶”ì¶œ"""
    print("ğŸ“Š ìºì‹œì—ì„œ Stack Overflow ë°ì´í„° ì¶”ì¶œ")
    print("=" * 50)
    
    cache_db_path = Config.DATABASE_PATH
    all_questions = []
    
    with sqlite3.connect(cache_db_path) as conn:
        cursor = conn.execute("SELECT key, value FROM cache WHERE key LIKE 'so_api:%'")
        cache_entries = cursor.fetchall()
        
        for key, value_str in cache_entries:
            try:
                data = json.loads(value_str)
                if 'items' in data and len(data['items']) > 0:
                    # ì§ˆë¬¸ ë°ì´í„°ì¸ì§€ í™•ì¸
                    first_item = data['items'][0]
                    if 'question_id' in first_item:
                        all_questions.extend(data['items'])
                        print(f"   ì¶”ì¶œ: {len(data['items'])}ê°œ ì§ˆë¬¸ (í‚¤: {key[:20]}...)")
            except Exception as e:
                print(f"   âš ï¸ ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨ (í‚¤: {key[:20]}...): {e}")
    
    # ì¤‘ë³µ ì œê±° (question_id ê¸°ì¤€)
    unique_questions = {}
    for q in all_questions:
        qid = q.get('question_id')
        if qid and qid not in unique_questions:
            unique_questions[qid] = q
    
    final_questions = list(unique_questions.values())
    print(f"âœ… ì´ {len(final_questions)}ê°œì˜ ê³ ìœ  ì§ˆë¬¸ ì¶”ì¶œ ì™„ë£Œ")
    
    return final_questions

def create_analysis_database():
    """ë¶„ì„ìš© ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±"""
    print("\nğŸ—„ï¸ ë¶„ì„ìš© ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±")
    print("=" * 50)
    
    db_path = Path(Config.DATA_DIR) / "stackoverflow_analysis.db"
    
    with sqlite3.connect(db_path) as conn:
        # ì§ˆë¬¸ í…Œì´ë¸” ìƒì„±
        conn.execute('''
            CREATE TABLE IF NOT EXISTS questions (
                question_id INTEGER PRIMARY KEY,
                title TEXT NOT NULL,
                body_markdown TEXT,
                score INTEGER DEFAULT 0,
                view_count INTEGER DEFAULT 0,
                favorite_count INTEGER DEFAULT 0,
                is_answered BOOLEAN DEFAULT 0,
                accepted_answer_id INTEGER,
                tags TEXT,  -- JSON array as text
                owner_reputation INTEGER DEFAULT 0,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                collected_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # ë‹µë³€ í…Œì´ë¸” ìƒì„±
        conn.execute('''
            CREATE TABLE IF NOT EXISTS answers (
                answer_id INTEGER PRIMARY KEY,
                question_id INTEGER,
                body_markdown TEXT,
                score INTEGER DEFAULT 0,
                owner_reputation INTEGER DEFAULT 0,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (question_id) REFERENCES questions (question_id)
            )
        ''')
        
        # ì¸ë±ìŠ¤ ìƒì„±
        conn.execute('CREATE INDEX IF NOT EXISTS idx_questions_score ON questions(score)')
        conn.execute('CREATE INDEX IF NOT EXISTS idx_questions_tags ON questions(tags)')
        conn.execute('CREATE INDEX IF NOT EXISTS idx_answers_question_id ON answers(question_id)')
        
        conn.commit()
    
    print(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì™„ë£Œ: {db_path}")
    return db_path

def store_questions_in_database(questions, db_path):
    """ì§ˆë¬¸ ë°ì´í„°ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
    print(f"\nğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ì— {len(questions)}ê°œ ì§ˆë¬¸ ì €ì¥")
    print("=" * 50)
    
    stored_questions = 0
    stored_answers = 0
    
    with sqlite3.connect(db_path) as conn:
        for question in questions:
            try:
                # ì§ˆë¬¸ ë°ì´í„° ì €ì¥
                conn.execute('''
                    INSERT OR REPLACE INTO questions (
                        question_id, title, body_markdown, score, view_count, 
                        favorite_count, is_answered, accepted_answer_id, 
                        tags, owner_reputation, collected_at
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    question.get('question_id'),
                    question.get('title', ''),
                    question.get('body_markdown', ''),
                    question.get('score', 0),
                    question.get('view_count', 0),
                    question.get('favorite_count', 0),
                    question.get('is_answered', False),
                    question.get('accepted_answer_id'),
                    json.dumps(question.get('tags', [])),
                    question.get('owner', {}).get('reputation', 0),
                    datetime.now().isoformat()
                ))
                stored_questions += 1
                
                # ì±„íƒëœ ë‹µë³€ì´ ìˆìœ¼ë©´ ì €ì¥
                if question.get('accepted_answer'):
                    answer = question['accepted_answer']
                    conn.execute('''
                        INSERT OR REPLACE INTO answers (
                            answer_id, question_id, body_markdown, score, 
                            owner_reputation
                        ) VALUES (?, ?, ?, ?, ?)
                    ''', (
                        answer.get('answer_id'),
                        question.get('question_id'),
                        answer.get('body_markdown', ''),
                        answer.get('score', 0),
                        answer.get('owner', {}).get('reputation', 0)
                    ))
                    stored_answers += 1
                    
            except Exception as e:
                print(f"   âŒ ì§ˆë¬¸ {question.get('question_id')} ì €ì¥ ì‹¤íŒ¨: {e}")
        
        conn.commit()
    
    print(f"âœ… ì €ì¥ ì™„ë£Œ:")
    print(f"   ì§ˆë¬¸: {stored_questions}ê°œ")
    print(f"   ë‹µë³€: {stored_answers}ê°œ")

def analyze_stored_data(db_path):
    """ì €ì¥ëœ ë°ì´í„° ë¶„ì„"""
    print(f"\nğŸ“ˆ ì €ì¥ëœ ë°ì´í„° ë¶„ì„")
    print("=" * 50)
    
    with sqlite3.connect(db_path) as conn:
        # ê¸°ë³¸ í†µê³„
        question_count = conn.execute("SELECT COUNT(*) FROM questions").fetchone()[0]
        answer_count = conn.execute("SELECT COUNT(*) FROM answers").fetchone()[0]
        answered_count = conn.execute("SELECT COUNT(*) FROM questions WHERE is_answered = 1").fetchone()[0]
        
        print(f"ğŸ“Š ê¸°ë³¸ í†µê³„:")
        print(f"   ì´ ì§ˆë¬¸: {question_count}ê°œ")
        print(f"   ì´ ë‹µë³€: {answer_count}ê°œ")
        print(f"   ë‹µë³€ëœ ì§ˆë¬¸: {answered_count}ê°œ ({answered_count/question_count*100:.1f}%)")
        
        # ì ìˆ˜ ë¶„í¬
        score_stats = conn.execute('''
            SELECT MIN(score), MAX(score), AVG(score)
            FROM questions
        ''').fetchone()
        
        print(f"   ì§ˆë¬¸ ì ìˆ˜: ìµœì†Œ {score_stats[0]}, ìµœëŒ€ {score_stats[1]}, í‰ê·  {score_stats[2]:.1f}")
        
        # ì¸ê¸° íƒœê·¸ ë¶„ì„
        print(f"\nğŸ·ï¸ íƒœê·¸ ë¶„ì„:")
        tag_analysis = conn.execute("SELECT tags FROM questions WHERE tags != '[]'").fetchall()
        
        all_tags = []
        for (tags_str,) in tag_analysis:
            try:
                tags = json.loads(tags_str)
                all_tags.extend(tags)
            except:
                pass
        
        tag_counts = {}
        for tag in all_tags:
            tag_counts[tag] = tag_counts.get(tag, 0) + 1
        
        for tag, count in sorted(tag_counts.items(), key=lambda x: x[1], reverse=True)[:10]:
            print(f"   {tag}: {count}íšŒ")
        
        # ê³ í’ˆì§ˆ ì§ˆë¬¸ (ì ìˆ˜ ë†’ì€ ìˆœ)
        print(f"\nâ­ ê³ í’ˆì§ˆ ì§ˆë¬¸ (ì ìˆ˜ ë†’ì€ ìˆœ):")
        high_quality = conn.execute('''
            SELECT question_id, title, score, view_count
            FROM questions
            ORDER BY score DESC
            LIMIT 5
        ''').fetchall()
        
        for qid, title, score, views in high_quality:
            print(f"   ì ìˆ˜ {score}: {title[:80]}...")
            print(f"            (ID: {qid}, ì¡°íšŒìˆ˜: {views:,})")
        
        # Excel í•¨ìˆ˜ ì–¸ê¸‰ ë¶„ì„
        print(f"\nğŸ”§ Excel í•¨ìˆ˜ ì–¸ê¸‰ ë¶„ì„:")
        excel_functions = ['vlookup', 'index', 'match', 'sumif', 'countif', 'if(', 'pivot']
        
        for func in excel_functions:
            count = conn.execute('''
                SELECT COUNT(*) FROM questions 
                WHERE LOWER(title) LIKE ? OR LOWER(body_markdown) LIKE ?
            ''', (f'%{func}%', f'%{func}%')).fetchone()[0]
            
            if count > 0:
                print(f"   {func.upper()}: {count}ê°œ ì§ˆë¬¸")
        
        # ìƒ˜í”Œ Q&A ìŒ ì¶œë ¥
        print(f"\nğŸ“ ìƒ˜í”Œ Q&A ìŒ:")
        samples = conn.execute('''
            SELECT q.question_id, q.title, q.score as q_score, 
                   a.body_markdown, a.score as a_score
            FROM questions q
            JOIN answers a ON q.question_id = a.question_id
            ORDER BY q.score DESC
            LIMIT 2
        ''').fetchall()
        
        for i, (qid, title, q_score, answer_body, a_score) in enumerate(samples, 1):
            print(f"\n   ìƒ˜í”Œ {i}:")
            print(f"   ì§ˆë¬¸: {title}")
            print(f"   ì ìˆ˜: Q{q_score}/A{a_score}")
            print(f"   ë‹µë³€ ë¯¸ë¦¬ë³´ê¸°: {answer_body[:300]}...")

def export_analysis_results(db_path):
    """ë¶„ì„ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
    print(f"\nğŸ“¤ ë¶„ì„ ê²°ê³¼ ë‚´ë³´ë‚´ê¸°")
    print("=" * 50)
    
    try:
        # JSON í˜•íƒœë¡œ ì „ì²´ ë°ì´í„° ë‚´ë³´ë‚´ê¸°
        with sqlite3.connect(db_path) as conn:
            # ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ì¡°ì¸í•œ ì™„ì „í•œ ë°ì´í„°
            complete_data = conn.execute('''
                SELECT 
                    q.question_id, q.title, q.body_markdown as question_body,
                    q.score as question_score, q.view_count, q.tags,
                    a.answer_id, a.body_markdown as answer_body, a.score as answer_score
                FROM questions q
                LEFT JOIN answers a ON q.question_id = a.question_id
                ORDER BY q.score DESC
            ''').fetchall()
            
            # ë°ì´í„° êµ¬ì¡°í™”
            export_data = []
            for row in complete_data:
                qid, title, q_body, q_score, views, tags_str, aid, a_body, a_score = row
                
                try:
                    tags = json.loads(tags_str) if tags_str else []
                except:
                    tags = []
                
                item = {
                    "question": {
                        "id": qid,
                        "title": title,
                        "body": q_body,
                        "score": q_score,
                        "view_count": views,
                        "tags": tags
                    },
                    "answer": {
                        "id": aid,
                        "body": a_body,
                        "score": a_score
                    } if aid else None
                }
                export_data.append(item)
            
            # íŒŒì¼ ì €ì¥
            export_file = Path(Config.OUTPUT_DIR) / f"stackoverflow_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(export_file, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… ë¶„ì„ ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì™„ë£Œ:")
            print(f"   íŒŒì¼: {export_file}")
            print(f"   í¬ê¸°: {export_file.stat().st_size:,} bytes")
            print(f"   í•­ëª© ìˆ˜: {len(export_data)}")
            
            # ìš”ì•½ í†µê³„ íŒŒì¼ ìƒì„±
            summary_file = Path(Config.OUTPUT_DIR) / f"stackoverflow_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
            with open(summary_file, 'w', encoding='utf-8') as f:
                f.write("Stack Overflow Excel Q&A ë°ì´í„° ìˆ˜ì§‘ ìš”ì•½\n")
                f.write("=" * 50 + "\n\n")
                f.write(f"ìˆ˜ì§‘ ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"ì´ ì§ˆë¬¸ ìˆ˜: {len([x for x in export_data if x['question']])}\n")
                f.write(f"ë‹µë³€ í¬í•¨ ì§ˆë¬¸: {len([x for x in export_data if x['answer']])}\n")
                f.write(f"í‰ê·  ì§ˆë¬¸ ì ìˆ˜: {sum(x['question']['score'] for x in export_data if x['question']) / len(export_data):.1f}\n")
                f.write(f"ë°ì´í„° íŒŒì¼: {export_file.name}\n")
            
            print(f"   ìš”ì•½ íŒŒì¼: {summary_file}")
            
    except Exception as e:
        print(f"âŒ ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ Stack Overflow ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ë° ë¶„ì„ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    try:
        # 1. ìºì‹œì—ì„œ ë°ì´í„° ì¶”ì¶œ
        questions = extract_cached_stackoverflow_data()
        
        if not questions:
            print("âŒ ì¶”ì¶œí•  ì§ˆë¬¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # 2. ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
        db_path = create_analysis_database()
        
        # 3. ë°ì´í„° ì €ì¥
        store_questions_in_database(questions, db_path)
        
        # 4. ë°ì´í„° ë¶„ì„
        analyze_stored_data(db_path)
        
        # 5. ê²°ê³¼ ë‚´ë³´ë‚´ê¸°
        export_analysis_results(db_path)
        
        print(f"\nğŸ‰ ë°ì´í„°ë² ì´ìŠ¤ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print(f"   âœ… ìºì‹œ ë°ì´í„° ì¶”ì¶œ ì„±ê³µ")
        print(f"   âœ… ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì„±ê³µ")
        print(f"   âœ… ë°ì´í„° ë¶„ì„ ì™„ë£Œ")
        print(f"   âœ… ê²°ê³¼ ë‚´ë³´ë‚´ê¸° ì™„ë£Œ")
        print(f"\nğŸ“ ìƒì„±ëœ íŒŒì¼:")
        print(f"   - ë°ì´í„°ë² ì´ìŠ¤: {db_path}")
        print(f"   - ë‚´ë³´ë‚´ê¸° íŒŒì¼: {Config.OUTPUT_DIR}")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()