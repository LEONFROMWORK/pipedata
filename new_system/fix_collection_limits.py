#!/usr/bin/env python3
"""
ìˆ˜ì§‘ í•œê³„ ë¬¸ì œ í•´ê²° ë° ì„¤ì • ìµœì í™”
"""
import asyncio
import logging
from datetime import datetime, timedelta
from pathlib import Path
import sys

# Add project root to path
sys.path.append(str(Path(__file__).parent))

from collectors.stackoverflow_collector import StackOverflowCollector
from collectors.reddit_collector import RedditCollector
from core.cache import APICache, LocalCache
from core.dedup_tracker import get_global_tracker

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

async def reset_deduplication_for_fresh_collection():
    """ì¤‘ë³µ ì¶”ì ê¸°ë¥¼ ë¦¬ì…‹í•˜ì—¬ ìƒˆë¡œìš´ ìˆ˜ì§‘ í—ˆìš©"""
    
    logger.info("ğŸ”„ ì¤‘ë³µ ì¶”ì ê¸° ë¦¬ì…‹")
    logger.info("=" * 50)
    
    tracker = get_global_tracker()
    
    # ê¸°ì¡´ í†µê³„ í™•ì¸
    stats = tracker.get_collection_stats(days=1)
    if stats:
        so_count = stats.get('stackoverflow', {}).get('total_collected', 0)
        reddit_count = stats.get('reddit', {}).get('total_collected', 0)
        logger.info(f"   ğŸ“Š ê¸°ì¡´ ìˆ˜ì§‘ëŸ‰: SO {so_count}ê°œ, Reddit {reddit_count}ê°œ")
    
    # ì˜¤ë˜ëœ ë ˆì½”ë“œ ì •ë¦¬ (1ì¼ ì´ìƒ)
    deleted_count = tracker.cleanup_old_records(days=1)
    logger.info(f"   ğŸ—‘ï¸  ì •ë¦¬ëœ ë ˆì½”ë“œ: {deleted_count}ê°œ")
    
    # ìƒˆë¡œìš´ í†µê³„ í™•ì¸
    new_stats = tracker.get_collection_stats(days=1)
    if new_stats:
        new_so_count = new_stats.get('stackoverflow', {}).get('total_collected', 0)
        new_reddit_count = new_stats.get('reddit', {}).get('total_collected', 0)
        logger.info(f"   âœ… ë¦¬ì…‹ í›„: SO {new_so_count}ê°œ, Reddit {new_reddit_count}ê°œ")

async def test_optimized_collection():
    """ìµœì í™”ëœ ì„¤ì •ìœ¼ë¡œ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸"""
    
    logger.info("\nğŸ¯ ìµœì í™”ëœ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸")
    logger.info("=" * 50)
    
    # Cache ì´ˆê¸°í™”
    local_cache = LocalCache(db_path=Path("/tmp/optimized_collection.db"))
    cache = APICache(local_cache)
    
    # 1. Stack Overflow í…ŒìŠ¤íŠ¸
    logger.info("ğŸ“š Stack Overflow ìµœì í™” í…ŒìŠ¤íŠ¸:")
    so_collector = StackOverflowCollector(cache)
    
    # ë” ë„“ì€ ê¸°ê°„ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ (7ì¼)
    from_date = datetime.now() - timedelta(days=7)
    
    try:
        so_data = await so_collector.collect_excel_questions(
            from_date=from_date,
            max_pages=5  # ì ì€ í˜ì´ì§€ë¡œ í…ŒìŠ¤íŠ¸
        )
        logger.info(f"   âœ… Stack Overflow: {len(so_data)}ê°œ ìˆ˜ì§‘")
        
        if len(so_data) > 0:
            sample = so_data[0]
            logger.info(f"   ğŸ“ ìƒ˜í”Œ: {sample.get('title', 'No title')[:50]}...")
        
    except Exception as e:
        logger.error(f"   âŒ Stack Overflow í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    # 2. Reddit í…ŒìŠ¤íŠ¸  
    logger.info("\nğŸŸ  Reddit ìµœì í™” í…ŒìŠ¤íŠ¸:")
    reddit_collector = RedditCollector(cache)
    
    try:
        reddit_data = await reddit_collector.collect_excel_discussions(
            from_date=from_date,
            max_submissions=10  # ì ì€ ìˆ˜ë¡œ í…ŒìŠ¤íŠ¸
        )
        logger.info(f"   âœ… Reddit: {len(reddit_data)}ê°œ ìˆ˜ì§‘")
        
        if len(reddit_data) > 0:
            sample = reddit_data[0]
            logger.info(f"   ğŸ“ ìƒ˜í”Œ: {sample.submission.get('title', 'No title')[:50]}...")
            
    except Exception as e:
        logger.error(f"   âŒ Reddit í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

async def create_small_dataset():
    """ì‘ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""
    
    logger.info("\nğŸ“¦ ì†Œê·œëª¨ ë°ì´í„°ì…‹ ìƒì„±")
    logger.info("=" * 50)
    
    # ê¸°ì¡´ ë°ì´í„° í™œìš©í•˜ì—¬ ìµœì†Œ ë°ì´í„°ì…‹ ìƒì„±
    from output.dataset_generator import JSONLDatasetGenerator
    
    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_qa_pairs = [
        {
            'id': 'test_1',
            'source': 'stackoverflow',
            'question': {
                'title': 'Test Excel Question 1',
                'body_markdown': 'How to use VLOOKUP?',
                'question_id': 99999999,
                'tags': ['excel', 'excel-formula'],
                'score': 5
            },
            'answer': {
                'body': 'Use =VLOOKUP(lookup_value, table_array, col_index_num, FALSE)',
                'answer_id': 88888888,
                'score': 10,
                'is_accepted': True
            },
            'quality_metrics': {
                'overall_score': 8.5,
                'raw_question_score': 5.0,
                'raw_answer_score': 10.0
            },
            'has_accepted_answer': True
        },
        {
            'id': 'test_2', 
            'source': 'reddit',
            'question': {
                'title': 'Test Excel Question 2',
                'text': 'Need help with INDEX MATCH formula',
                'reddit_id': 'test123',
                'flair': 'solved'
            },
            'answer': {
                'text': 'Try =INDEX(return_array, MATCH(lookup_value, lookup_array, 0))',
                'reddit_id': 'reply123'
            },
            'quality_metrics': {
                'overall_score': 7.5,
                'raw_question_score': 4.0,
                'raw_answer_score': 8.0
            }
        }
    ]
    
    try:
        generator = JSONLDatasetGenerator()
        
        # ë°ì´í„°ì…‹ ìƒì„±
        output_path = generator.generate_dataset(
            test_qa_pairs,
            data_sources=['stackoverflow', 'reddit']
        )
        
        logger.info(f"   âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„±: {output_path}")
        
        # ê²€ì¦
        validation_result = generator.validate_dataset(output_path)
        if validation_result['valid']:
            logger.info(f"   âœ… ë°ì´í„°ì…‹ ê²€ì¦ í†µê³¼: {validation_result['valid_lines']}ê°œ í•­ëª©")
        else:
            logger.warning(f"   âš ï¸  ë°ì´í„°ì…‹ ê²€ì¦ ì‹¤íŒ¨: {validation_result.get('errors', [])}")
            
        return output_path
        
    except Exception as e:
        logger.error(f"   âŒ ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨: {e}")
        return None

async def suggest_dashboard_settings():
    """ëŒ€ì‹œë³´ë“œ ì„¤ì • ê¶Œì¥ì‚¬í•­"""
    
    logger.info("\nâš™ï¸  ëŒ€ì‹œë³´ë“œ ì„¤ì • ê¶Œì¥ì‚¬í•­")
    logger.info("=" * 50)
    
    logger.info("1. ìˆ˜ì§‘ ë§¤ê°œë³€ìˆ˜ ì¡°ì •:")
    logger.info("   â€¢ target_count: 100 â†’ 20 (ì‘ê²Œ ì‹œì‘)")
    logger.info("   â€¢ max_pages: 50 â†’ 10 (API í•œë„ ê³ ë ¤)")
    logger.info("   â€¢ collection_period: 7ì¼ â†’ 30ì¼ (ë” ë§ì€ ë°ì´í„°)")
    
    logger.info("\n2. Rate Limiting ê°•í™”:")
    logger.info("   â€¢ Stack Overflow: 2ì´ˆ â†’ 3ì´ˆ ê°„ê²©")
    logger.info("   â€¢ Reddit: 1ì´ˆ â†’ 2ì´ˆ ê°„ê²©")
    logger.info("   â€¢ ë™ì‹œ ìˆ˜ì§‘ â†’ ìˆœì°¨ ìˆ˜ì§‘")
    
    logger.info("\n3. ì¤‘ë³µ ë°©ì§€ ìµœì í™”:")
    logger.info("   â€¢ ì¼ì¼ ìë™ ì •ë¦¬ í™œì„±í™”")
    logger.info("   â€¢ ìˆ˜ì§‘ ì „ ì¤‘ë³µ ì²´í¬ ë¡œê·¸ ê°œì„ ")
    logger.info("   â€¢ ìƒˆë¡œìš´ ë°ì´í„° ìš°ì„  ìˆ˜ì§‘")
    
    logger.info("\n4. ì˜¤ë¥˜ ì²˜ë¦¬ ê°•í™”:")
    logger.info("   â€¢ API í•œë„ ë„ë‹¬ ì‹œ graceful ì¤‘ë‹¨")
    logger.info("   â€¢ ë¶€ë¶„ ì„±ê³µ ì‹œ ê²°ê³¼ ì €ì¥")
    logger.info("   â€¢ ì¬ì‹œë„ ë¡œì§ ê°œì„ ")

async def main():
    """ë©”ì¸ ìˆ˜ì • ë£¨í‹´"""
    logger.info("ğŸ”§ ìˆ˜ì§‘ í•œê³„ ë¬¸ì œ í•´ê²°")
    logger.info("=" * 60)
    
    # 1. ì¤‘ë³µ ì¶”ì ê¸° ë¦¬ì…‹
    await reset_deduplication_for_fresh_collection()
    
    # 2. ìµœì í™”ëœ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸
    await test_optimized_collection()
    
    # 3. ì†Œê·œëª¨ ë°ì´í„°ì…‹ ìƒì„±
    test_dataset = await create_small_dataset()
    
    # 4. ëŒ€ì‹œë³´ë“œ ì„¤ì • ê¶Œì¥ì‚¬í•­
    await suggest_dashboard_settings()
    
    # ìµœì¢… ìš”ì•½
    logger.info("\n" + "ğŸ‰" * 30)
    logger.info("ìˆ˜ì§‘ í•œê³„ ë¬¸ì œ í•´ê²° ì™„ë£Œ!")
    logger.info("")
    logger.info("ë‹¤ìŒ ë‹¨ê³„:")
    logger.info("1. ëŒ€ì‹œë³´ë“œì—ì„œ ì‘ì€ target_count(20)ë¡œ í…ŒìŠ¤íŠ¸")
    logger.info("2. ìˆ˜ì§‘ ê¸°ê°„ì„ 30ì¼ë¡œ ì„¤ì •") 
    logger.info("3. Rate limiting ê°„ê²© ì¦ê°€")
    logger.info("4. ì„±ê³µ ì‹œ ì ì§„ì ìœ¼ë¡œ ì¦ê°€")

if __name__ == "__main__":
    asyncio.run(main())