"""
ì˜¤ë¹ ë‘(oppadu.com) ì›¹ í¬ë¡¤ëŸ¬
í•œêµ­ Excel ì»¤ë®¤ë‹ˆí‹° ë°ì´í„° ìˆ˜ì§‘ì„ ìœ„í•œ ê³ ê¸‰ ì›¹ í¬ë¡¤ë§ ì‹œìŠ¤í…œ
"""

import asyncio
import logging
import time
import random
import os
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional, Tuple
from urllib.parse import urljoin, urlparse
import json
import re

import aiohttp
import cloudscraper
from bs4 import BeautifulSoup
from fake_useragent import UserAgent

# Selenium imports - optional for environments without Chrome
SELENIUM_ENABLED = os.getenv('SELENIUM_ENABLED', 'false').lower() == 'true'

if SELENIUM_ENABLED:
    try:
        import undetected_chromedriver as uc
        from selenium.webdriver.common.by import By
        from selenium.webdriver.support.ui import WebDriverWait
        from selenium.webdriver.support import expected_conditions as EC
        from selenium.webdriver.common.action_chains import ActionChains
        from selenium.common.exceptions import TimeoutException, NoSuchElementException
        SELENIUM_AVAILABLE = True
    except ImportError:
        SELENIUM_AVAILABLE = False
        logging.warning("Selenium/undetected_chromedriver not available. Some features will be limited.")
else:
    SELENIUM_AVAILABLE = False
    logging.info("Selenium disabled by environment variable SELENIUM_ENABLED=false")

from core.cache import APICache
from core.dedup_tracker import get_global_tracker
from config import Config

logger = logging.getLogger('pipeline.oppadu_crawler')

class OppaduAntiDetection:
    """ì˜¤ë¹ ë‘ í¬ë¡¤ë§ ë°©ì§€ ìš°íšŒë¥¼ ìœ„í•œ ê³ ê¸‰ ê¸°ìˆ """
    
    def __init__(self):
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:121.0) Gecko/20100101 Firefox/121.0',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/120.0.0.0 Safari/537.36'
        ]
        self.session_headers = self._generate_headers()
        
    def _generate_headers(self) -> Dict[str, str]:
        """í•œêµ­ ì‚¬ìš©ìë¥¼ ëª¨ë°©í•œ í—¤ë” ìƒì„±"""
        return {
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Ch-Ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
            'Sec-Ch-Ua-Mobile': '?0',
            'Sec-Ch-Ua-Platform': '"Windows"',
            'Cache-Control': 'max-age=0'
        }
    
    def rotate_headers(self):
        """í—¤ë” ë¡œí…Œì´ì…˜"""
        self.session_headers['User-Agent'] = random.choice(self.user_agents)
        
    async def human_delay(self, min_delay: float = 2.0, max_delay: float = 5.0):
        """ì¸ê°„ì ì¸ ì§€ì—° ì‹œë®¬ë ˆì´ì…˜"""
        delay = random.uniform(min_delay, max_delay)
        await asyncio.sleep(delay)

class OppaduCrawler:
    """
    ì˜¤ë¹ ë‘ ì»¤ë®¤ë‹ˆí‹° í¬ë¡¤ëŸ¬
    - í¬ë¡¤ë§ ë°©ì§€ ìš°íšŒ
    - í˜ì´ì§€ë„¤ì´ì…˜ ì²˜ë¦¬
    - í•œêµ­ ë°ì´í„° íŠ¹ì„±í™”
    - ì¤‘ë³µ ë°©ì§€
    """
    
    def __init__(self, cache: APICache):
        self.cache = cache
        self.base_url = "https://www.oppadu.com"
        self.community_url = f"{self.base_url}/community/question/"
        self.dedup_tracker = get_global_tracker()
        self.anti_detection = OppaduAntiDetection()
        
        # CloudScraper ì„¤ì • (Cloudflare ìš°íšŒ)
        self.scraper = cloudscraper.create_scraper(
            browser={
                'browser': 'chrome',
                'platform': 'windows',
                'desktop': True
            }
        )
        
        # Selenium ë“œë¼ì´ë²„ (í•„ìš”ì‹œ ì‚¬ìš©)
        self.driver = None
        
        logger.info("OppaduCrawler initialized with anti-detection measures")
    
    async def collect_oppadu_questions(self, max_pages: int = 50) -> List[Dict[str, Any]]:
        """
        ì˜¤ë¹ ë‘ ì»¤ë®¤ë‹ˆí‹°ì—ì„œ ë‹µë³€ ì™„ë£Œëœ ì§ˆë¬¸ë“¤ì„ ìˆ˜ì§‘
        
        Args:
            max_pages: ìˆ˜ì§‘í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜
            
        Returns:
            ìˆ˜ì§‘ëœ Q&A ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        logger.info(f"ğŸ‡°ğŸ‡· ì˜¤ë¹ ë‘ ì»¤ë®¤ë‹ˆí‹° ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ (ìµœëŒ€ {max_pages}í˜ì´ì§€)")
        
        collected_data = []
        page = 1
        
        try:
            while page <= max_pages:
                logger.info(f"ğŸ“„ í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘...")
                
                # í˜ì´ì§€ URL ìƒì„±
                page_url = f"{self.community_url}?page={page}" if page > 1 else self.community_url
                
                # ë‹µë³€ ì™„ë£Œëœ ê²Œì‹œê¸€ ëª©ë¡ ìˆ˜ì§‘
                answered_posts = await self._get_answered_posts(page_url)
                
                if not answered_posts:
                    logger.info(f"í˜ì´ì§€ {page}ì—ì„œ ë” ì´ìƒ ë‹µë³€ ì™„ë£Œëœ ê²Œì‹œê¸€ì´ ì—†ìŒ")
                    break
                
                logger.info(f"   ğŸ¯ {len(answered_posts)}ê°œì˜ ë‹µë³€ ì™„ë£Œ ê²Œì‹œê¸€ ë°œê²¬")
                
                # ê° ê²Œì‹œê¸€ì˜ ìƒì„¸ ë°ì´í„° ìˆ˜ì§‘
                for post_url in answered_posts:
                    try:
                        # ì¤‘ë³µ ì²´í¬
                        post_id = self._extract_post_id(post_url)
                        if self.dedup_tracker.is_oppadu_post_collected(post_id):
                            logger.debug(f"ì´ë¯¸ ìˆ˜ì§‘ëœ ê²Œì‹œê¸€ ê±´ë„ˆëœ€: {post_id}")
                            continue
                        
                        # ìƒì„¸ ë°ì´í„° ìˆ˜ì§‘
                        post_data = await self._scrape_post_detail(post_url)
                        
                        if post_data:
                            # í•œêµ­ ë°ì´í„° íŠ¹ì„±í™” ë©”íƒ€ë°ì´í„° ì¶”ê°€
                            post_data['metadata']['country'] = 'KR'
                            post_data['metadata']['language'] = 'ko'
                            post_data['metadata']['source_type'] = 'korean_community'
                            post_data['metadata']['cultural_context'] = 'korean_business'
                            
                            collected_data.append(post_data)
                            
                            # ì¤‘ë³µ ë°©ì§€ ì¶”ì ê¸°ì— ë“±ë¡
                            self.dedup_tracker.mark_oppadu_collected(
                                post_id,
                                post_data.get('title', ''),
                                quality_score=post_data.get('quality_score', 0.0),
                                metadata={'page': page, 'collection_date': datetime.now().isoformat()}
                            )
                            
                            logger.info(f"   âœ… ê²Œì‹œê¸€ ìˆ˜ì§‘ ì™„ë£Œ: {post_data.get('title', '')[:50]}...")
                        
                        # ì¸ê°„ì ì¸ ì§€ì—°
                        await self.anti_detection.human_delay(1.0, 3.0)
                        
                    except Exception as e:
                        logger.error(f"ê²Œì‹œê¸€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {post_url} - {e}")
                        continue
                
                # í˜ì´ì§€ë„¤ì´ì…˜ í™•ì¸
                has_next = await self._check_next_page(page_url)
                if not has_next:
                    logger.info("ë§ˆì§€ë§‰ í˜ì´ì§€ì— ë„ë‹¬í•¨")
                    break
                
                page += 1
                
                # í˜ì´ì§€ ê°„ ì§€ì—°
                await self.anti_detection.human_delay(3.0, 7.0)
        
        except Exception as e:
            logger.error(f"ì˜¤ë¹ ë‘ ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
        
        finally:
            if self.driver:
                self.driver.quit()
        
        logger.info(f"ğŸ‰ ì˜¤ë¹ ë‘ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(collected_data)}ê°œ í•­ëª©")
        return collected_data
    
    async def _get_answered_posts(self, page_url: str) -> List[str]:
        """ë‹µë³€ ì™„ë£Œëœ ê²Œì‹œê¸€ URL ëª©ë¡ ìˆ˜ì§‘"""
        try:
            # 1ì°¨ ì‹œë„: CloudScraper
            response = await self._fetch_with_cloudscraper(page_url)
            if response:
                return self._parse_answered_posts(response)
            
            # 2ì°¨ ì‹œë„: Selenium (JavaScript ë Œë”ë§ í•„ìš”í•œ ê²½ìš°)
            return await self._fetch_with_selenium(page_url)
            
        except Exception as e:
            logger.error(f"ë‹µë³€ ì™„ë£Œ ê²Œì‹œê¸€ ëª©ë¡ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []
    
    async def _fetch_with_cloudscraper(self, url: str) -> Optional[str]:
        """CloudScraperë¥¼ ì‚¬ìš©í•œ í˜ì´ì§€ ìˆ˜ì§‘"""
        try:
            # í—¤ë” ë¡œí…Œì´ì…˜
            self.anti_detection.rotate_headers()
            self.scraper.headers.update(self.anti_detection.session_headers)
            
            # Referer ì„¤ì • (ìì—°ìŠ¤ëŸ¬ìš´ íƒìƒ‰ ì‹œë®¬ë ˆì´ì…˜)
            if 'page=' in url:
                self.scraper.headers['Referer'] = self.community_url
            
            response = self.scraper.get(url, timeout=30)
            response.raise_for_status()
            
            logger.debug(f"CloudScraper ì„±ê³µ: {url}")
            return response.text
            
        except Exception as e:
            logger.warning(f"CloudScraper ì‹¤íŒ¨: {e}")
            return None
    
    async def _fetch_with_selenium(self, url: str) -> List[str]:
        """Seleniumì„ ì‚¬ìš©í•œ JavaScript ë Œë”ë§ í˜ì´ì§€ ìˆ˜ì§‘"""
        if not SELENIUM_AVAILABLE:
            self.logger.warning("Selenium not available, falling back to regular HTTP")
            return []
            
        try:
            if not self.driver:
                # Undetected Chrome ì„¤ì •
                options = uc.ChromeOptions()
                options.add_argument('--no-sandbox')
                options.add_argument('--disable-dev-shm-usage')
                options.add_argument('--disable-gpu')
                options.add_argument('--window-size=1920,1080')
                options.add_argument('--user-agent=' + random.choice(self.anti_detection.user_agents))
                
                # í•œêµ­ ì–¸ì–´ ì„¤ì •
                options.add_argument('--lang=ko-KR')
                options.add_experimental_option('prefs', {
                    'intl.accept_languages': 'ko-KR,ko,en'
                })
                
                self.driver = uc.Chrome(options=options)
            
            self.driver.get(url)
            
            # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.CLASS_NAME, "post-list-modern"))
            )
            
            # ìŠ¤í¬ë¡¤ ë‹¤ìš´ (ë ˆì´ì§€ ë¡œë”© ëŒ€ì‘)
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            await asyncio.sleep(2)
            
            page_source = self.driver.page_source
            return self._parse_answered_posts(page_source)
            
        except Exception as e:
            logger.error(f"Selenium í˜ì´ì§€ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []
    
    def _parse_answered_posts(self, html_content: str) -> List[str]:
        """HTMLì—ì„œ ë‹µë³€ ì™„ë£Œëœ ê²Œì‹œê¸€ URL ì¶”ì¶œ"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # post-list-modern ì»¨í…Œì´ë„ˆ ì°¾ê¸°
            post_list = soup.find('div', class_='post-list-modern')
            if not post_list:
                logger.warning("post-list-modern ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return []
            
            answered_posts = []
            
            # ê° post-item-modern ê²€ì‚¬
            post_items = post_list.find_all('div', class_='post-item-modern')
            logger.debug(f"ì´ {len(post_items)}ê°œì˜ ê²Œì‹œê¸€ í•­ëª© ë°œê²¬")
            
            for item in post_items:
                # answer-complete-badgeê°€ ìˆëŠ”ì§€ í™•ì¸
                answer_badge = item.find(class_='answer-complete-badge')
                if answer_badge:
                    # ê²Œì‹œê¸€ ë§í¬ ì¶”ì¶œ (post-title-modern í´ë˜ìŠ¤ ìš°ì„ )
                    link_element = item.find('a', class_='post-title-modern') or item.find('a', href=True)
                    if link_element:
                        # ì˜¬ë°”ë¥¸ URL êµ¬ì„±: community_url + href (hrefëŠ” ?ë¡œ ì‹œì‘)
                        href = link_element['href']
                        if href.startswith('?'):
                            # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ë§Œ ìˆëŠ” ê²½ìš° community_urlê³¼ ê²°í•©
                            post_url = self.community_url + href
                        elif href.startswith('/'):
                            # ì ˆëŒ€ ê²½ë¡œì¸ ê²½ìš° base_urlê³¼ ê²°í•©
                            post_url = self.base_url + href
                        elif href.startswith('http'):
                            # ì™„ì „í•œ URLì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
                            post_url = href
                        else:
                            # ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° urljoin ì‚¬ìš©
                            post_url = urljoin(self.community_url, href)
                        
                        # URL ê²€ì¦: ì‹¤ì œ ê²Œì‹œê¸€ URLì¸ì§€ í™•ì¸
                        if 'board_id=' in post_url and 'action=view' in post_url and 'uid=' in post_url:
                            answered_posts.append(post_url)
                            logger.debug(f"ë‹µë³€ ì™„ë£Œ ê²Œì‹œê¸€ ë°œê²¬: {post_url}")
                        else:
                            logger.debug(f"ìœ íš¨í•˜ì§€ ì•Šì€ ê²Œì‹œê¸€ URL ë¬´ì‹œ: {post_url}")
            
            logger.info(f"ë‹µë³€ ì™„ë£Œëœ ê²Œì‹œê¸€ {len(answered_posts)}ê°œ ë°œê²¬")
            return answered_posts
            
        except Exception as e:
            logger.error(f"HTML íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
            return []
    
    async def _scrape_post_detail(self, post_url: str) -> Optional[Dict[str, Any]]:
        """ê²Œì‹œê¸€ ìƒì„¸ í˜ì´ì§€ì—ì„œ ë°ì´í„° ì¶”ì¶œ"""
        try:
            # ìƒì„¸ í˜ì´ì§€ ìˆ˜ì§‘
            html_content = await self._fetch_with_cloudscraper(post_url)
            if not html_content and self.driver:
                self.driver.get(post_url)
                await asyncio.sleep(3)
                html_content = self.driver.page_source
            
            if not html_content:
                logger.error(f"ìƒì„¸ í˜ì´ì§€ ìˆ˜ì§‘ ì‹¤íŒ¨: {post_url}")
                return None
            
            return self._parse_post_detail(html_content, post_url)
            
        except Exception as e:
            logger.error(f"ê²Œì‹œê¸€ ìƒì„¸ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def _parse_post_detail(self, html_content: str, post_url: str) -> Optional[Dict[str, Any]]:
        """ê²Œì‹œê¸€ ìƒì„¸ í˜ì´ì§€ HTML íŒŒì‹±"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ê¸°ë³¸ ë°ì´í„° êµ¬ì¡°
            post_data = {
                'source': 'oppadu',
                'url': post_url,
                'post_id': self._extract_post_id(post_url),
                'collected_at': datetime.now().isoformat(),
                'metadata': {}
            }
            
            # ì œëª© ì¶”ì¶œ
            title_element = soup.find('h1') or soup.find(class_='post-title')
            post_data['title'] = title_element.get_text(strip=True) if title_element else ""
            
            # ì‹œìŠ¤í…œ ì •ë³´ ì¶”ì¶œ (post-options-display > options-container)
            options_display = soup.find(class_='post-options-display')
            if options_display:
                options_container = options_display.find(class_='options-container')
                if options_container:
                    excel_version = self._extract_version_info(options_container, 'ì—‘ì…€ë²„ì „')
                    os_version = self._extract_version_info(options_container, 'OSë²„ì „')
                    
                    post_data['metadata']['excel_version'] = excel_version
                    post_data['metadata']['os_version'] = os_version
                    logger.debug(f"Extracted versions: Excel={excel_version}, OS={os_version}")
            
            # ì§ˆë¬¸ ë‚´ìš© ì¶”ì¶œ (post-content)
            post_content = soup.find(class_='post-content')
            if post_content:
                question_data = self._extract_content_data(post_content)
                post_data['question'] = question_data
                logger.debug(f"Extracted question data: {len(question_data.get('text', ''))} chars")
            
            # ì±„íƒëœ ë‹µë³€ ì¶”ì¶œ (selected-answer-badgeì™€ ì—°ê´€ëœ ë‹µë³€ ì°¾ê¸°)
            selected_answer_badge = soup.find(class_='selected-answer-badge')
            if selected_answer_badge:
                # ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ ë‹µë³€ ì»¨í…Œì´ë„ˆ ì°¾ê¸°
                answer_container = None
                
                # ë°©ë²• 1: ë¶€ëª¨ ìš”ì†Œì—ì„œ comment-content-wrapper ì°¾ê¸°
                parent = selected_answer_badge.find_parent()
                if parent:
                    # ì˜¤ë¹ ë‘ íŠ¹í™”: comment-wrapper selected-answer ë‚´ì˜ comment-content-wrapper
                    answer_content = (parent.find(class_='comment-content-wrapper') or 
                                    parent.find(class_='answer-content') or 
                                    parent.find(class_='post-content'))
                    if answer_content:
                        answer_container = answer_content
                
                # ë°©ë²• 2: ì¡°ìƒ ìš”ì†Œì—ì„œ comment-wrapper selected-answer ì°¾ê¸°
                if not answer_container:
                    comment_wrapper = selected_answer_badge.find_parent(class_='comment-wrapper')
                    if comment_wrapper and 'selected-answer' in comment_wrapper.get('class', []):
                        content_wrapper = comment_wrapper.find(class_='comment-content-wrapper')
                        if content_wrapper:
                            answer_container = content_wrapper
                
                # ë°©ë²• 3: í˜•ì œ ìš”ì†Œì—ì„œ ì°¾ê¸°
                if not answer_container:
                    next_sibling = selected_answer_badge.find_next_sibling()
                    if next_sibling:
                        answer_container = next_sibling
                
                # ë°©ë²• 4: ì „ì²´ ë‹µë³€ ì˜ì—­ì—ì„œ ì°¾ê¸°
                if not answer_container:
                    answer_section = soup.find(class_='answer-section') or soup.find(class_='answers')
                    if answer_section:
                        answer_container = answer_section
                
                if answer_container:
                    answer_data = self._extract_content_data(answer_container)
                    post_data['answer'] = answer_data
                    logger.debug(f"Extracted answer data: {len(answer_data.get('text', ''))} chars")
                else:
                    logger.warning("Selected answer badge found but no answer content located")
                    post_data['answer'] = {'text': '', 'images': [], 'has_code': False}
            else:
                # ì±„íƒëœ ë‹µë³€ì´ ì—†ëŠ” ê²½ìš° ë¹ˆ ë‹µë³€ ì„¤ì •
                post_data['answer'] = {'text': '', 'images': [], 'has_code': False}
            # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (í•œêµ­ ë°ì´í„° íŠ¹ì„± ë°˜ì˜)
            post_data['quality_score'] = self._calculate_korean_quality_score(post_data)
            
            return post_data
            
        except Exception as e:
            logger.error(f"ê²Œì‹œê¸€ ìƒì„¸ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def _extract_version_info(self, container, version_type: str) -> str:
        """ë²„ì „ ì •ë³´ ì¶”ì¶œ (ì‹¤ì œ HTML êµ¬ì¡°ì— ë§ê²Œ)"""
        try:
            # options-container ë‚´ì—ì„œ option-itemë“¤ì„ ì°¾ê¸°
            option_items = container.find_all('div', class_='option-item')
            
            for option_item in option_items:
                # option-labelê³¼ option-value ì°¾ê¸°
                label_element = option_item.find('span', class_='option-label')
                value_element = option_item.find('span', class_='option-value')
                
                if label_element and value_element:
                    label_text = label_element.get_text(strip=True)
                    value_text = value_element.get_text(strip=True)
                    
                    # ì—‘ì…€ë²„ì „ ë˜ëŠ” OSë²„ì „ ë§¤ì¹˜
                    if label_text == version_type:
                        logger.debug(f"Found {version_type}: {value_text}")
                        return value_text
            
            # ëŒ€ì²´ ë°©ë²•: ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ íŒ¨í„´ ê²€ìƒ‰
            text = container.get_text()
            if version_type in text:
                # ì •ê·œì‹ìœ¼ë¡œ ë²„ì „ ì •ë³´ ì¶”ì¶œ
                pattern = rf"{version_type}[:\s]*([^\n,]+)"
                match = re.search(pattern, text)
                if match:
                    return match.group(1).strip()
            
            return ""
            
        except Exception as e:
            logger.debug(f"Error extracting {version_type}: {e}")
            return ""
    
    def _extract_content_data(self, container) -> Dict[str, Any]:
        """ì»¨í…ì¸  ë°ì´í„° ì¶”ì¶œ (í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€) - ê°œì„ ëœ ë²„ì „"""
        if not container:
            return {'text': '', 'images': [], 'has_code': False}
        
        try:
            # ì›ë³¸ HTML í…ìŠ¤íŠ¸ ì¶”ì¶œ
            raw_text = container.get_text(strip=True, separator=' ')
            
            # ì‘ë‹µ ì •ë¦¬ (ìƒˆë¡œìš´ í´ë¦¬ë„ˆ ì‚¬ìš©)
            from core.oppadu_response_cleaner import OppaduResponseCleaner
            cleaner = OppaduResponseCleaner()
            cleaned_result = cleaner.clean_response(str(container))
            
            # ì •ë¦¬ëœ í…ìŠ¤íŠ¸ ì‚¬ìš©
            text_content = cleaned_result['cleaned_response']
            
            # ì´ë¯¸ì§€ URL ì¶”ì¶œ
            images = []
            img_tags = container.find_all('img')
            for img in img_tags:
                img_src = img.get('src') or img.get('data-src')
                if img_src and not any(skip in img_src for skip in ['icon', 'emoji', 'button']):
                    full_img_url = urljoin(self.base_url, img_src)
                    images.append(full_img_url)
            
            # ì½”ë“œ ë¸”ë¡ í™•ì¸ (ê°œì„ ëœ ë¡œì§)
            has_code = cleaned_result['has_excel_content']
            
            return {
                'text': text_content,
                'images': images,
                'has_code': has_code,
                'word_count': len(text_content.split()),
                'excel_formulas': cleaned_result['excel_formulas'],
                'explanation': cleaned_result['explanation']
            }
            
        except Exception as e:
            logger.error(f"ì»¨í…ì¸  ë°ì´í„° ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            # ê¸°ë³¸ ë°©ì‹ìœ¼ë¡œ í´ë°±
            text_content = container.get_text(strip=True, separator=' ') if container else ''
            return {
                'text': text_content,
                'images': [],
                'has_code': '=' in text_content,
                'word_count': len(text_content.split()),
                'excel_formulas': [],
                'explanation': text_content
            }
    
    def _calculate_korean_quality_score(self, post_data: Dict[str, Any]) -> float:
        """í•œêµ­ ë°ì´í„° íŠ¹ì„±ì„ ë°˜ì˜í•œ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        score = 5.0  # ê¸°ë³¸ ì ìˆ˜
        
        try:
            question = post_data.get('question', {})
            answer = post_data.get('answer', {})
            
            # í…ìŠ¤íŠ¸ ê¸¸ì´ ì ìˆ˜
            q_word_count = question.get('word_count', 0)
            a_word_count = answer.get('word_count', 0)
            
            if q_word_count >= 10:
                score += 1.0
            if a_word_count >= 15:
                score += 1.5
            
            # ì‹œìŠ¤í…œ ì •ë³´ ìˆìœ¼ë©´ ê°€ì‚°ì 
            if post_data.get('metadata', {}).get('excel_version'):
                score += 0.5
            if post_data.get('metadata', {}).get('os_version'):
                score += 0.5
            
            # ì½”ë“œë‚˜ ìˆ˜ì‹ ìˆìœ¼ë©´ ê°€ì‚°ì 
            if question.get('has_code') or answer.get('has_code'):
                score += 1.0
            
            # ì´ë¯¸ì§€ ìˆìœ¼ë©´ ê°€ì‚°ì 
            if question.get('images') or answer.get('images'):
                score += 0.5
            
            # í•œêµ­ì–´ íŠ¹ì„± í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤
            korean_keywords = ['ì—‘ì…€', 'í•¨ìˆ˜', 'ìˆ˜ì‹', 'ì…€', 'ì›Œí¬ì‹œíŠ¸', 'ì°¨íŠ¸', 'í”¼ë²—']
            text_combined = f"{question.get('text', '')} {answer.get('text', '')}"
            keyword_count = sum(1 for keyword in korean_keywords if keyword in text_combined)
            score += keyword_count * 0.2
            
            return min(score, 10.0)  # ìµœëŒ€ 10ì 
            
        except:
            return 5.0
    
    def _extract_post_id(self, url: str) -> str:
        """URLì—ì„œ ê²Œì‹œê¸€ ID ì¶”ì¶œ"""
        try:
            # URL íŒ¨í„´ì— ë”°ë¼ ID ì¶”ì¶œ
            parts = url.split('/')
            for part in reversed(parts):
                if part.isdigit():
                    return part
            # ìˆ«ì IDê°€ ì—†ìœ¼ë©´ URL í•´ì‹œ ì‚¬ìš©
            return str(hash(url))
        except:
            return str(hash(url))
    
    async def _check_next_page(self, current_url: str) -> bool:
        """ë‹¤ìŒ í˜ì´ì§€ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
        try:
            html_content = await self._fetch_with_cloudscraper(current_url)
            if not html_content:
                return False
            
            soup = BeautifulSoup(html_content, 'html.parser')
            pagination = soup.find(class_='oppadu-pagination')
            
            if pagination:
                # ë‹¤ìŒ í˜ì´ì§€ ë§í¬ë‚˜ ë²„íŠ¼ í™•ì¸
                next_link = pagination.find('a', string=re.compile('ë‹¤ìŒ|>|Next'))
                return next_link is not None
            
            return False
            
        except:
            return False
    
    def get_collection_stats(self) -> Dict[str, Any]:
        """ìˆ˜ì§‘ í†µê³„ ë°˜í™˜"""
        return {
            'source': 'oppadu',
            'total_collected': 0,  # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì¶”ì 
            'last_collection': datetime.now().isoformat()
        }