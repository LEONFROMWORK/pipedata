#!/usr/bin/env python3
"""
ë¬´í•œ ìŠ¤í¬ë¡¤ ë°©ì‹ ì›¹ ìˆ˜ì§‘ê¸°
- Reddit ìŠ¤íƒ€ì¼ ë¬´í•œ ìŠ¤í¬ë¡¤ ì²˜ë¦¬
- Stack Overflow í˜ì´ì§€ë„¤ì´ì…˜ + ë¬´í•œ ìŠ¤í¬ë¡¤ í•˜ì´ë¸Œë¦¬ë“œ
- ë” ë§ì€ ë°ì´í„° ìˆ˜ì§‘ì„ ìœ„í•œ ê³ ë„í™”ëœ ìŠ¤í¬ë˜í•‘
"""
import asyncio
import logging
import time
import json
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
from urllib.parse import urljoin, parse_qs, urlparse

import httpx
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.keys import Keys
from selenium.common.exceptions import TimeoutException, NoSuchElementException

from core.cache import APICache, LocalCache
from core.dedup_tracker import get_global_tracker
from config import Config

logger = logging.getLogger('pipeline.infinite_scroll_collector')

class InfiniteScrollCollector:
    """
    ë¬´í•œ ìŠ¤í¬ë¡¤ + í˜ì´ì§€ë„¤ì´ì…˜ í•˜ì´ë¸Œë¦¬ë“œ ìˆ˜ì§‘ê¸°
    - Stack Overflow: s-pagination í´ë˜ìŠ¤ ê¸°ë°˜ í˜ì´ì§€ ìˆœíšŒ
    - Reddit: ë¬´í•œ ìŠ¤í¬ë¡¤ ë°©ì‹ìœ¼ë¡œ ë” ë§ì€ ê²Œì‹œë¬¼ ë¡œë“œ
    - ë¸Œë¼ìš°ì € ìë™í™”ë¡œ ë™ì  ì½˜í…ì¸  ìˆ˜ì§‘
    """
    
    def __init__(self, cache: APICache, headless: bool = True):
        self.cache = cache
        self.dedup_tracker = get_global_tracker()
        self.headless = headless
        
        # Chrome ì˜µì…˜ ì„¤ì •
        self.chrome_options = Options()
        if headless:
            self.chrome_options.add_argument('--headless')
        
        self.chrome_options.add_argument('--no-sandbox')
        self.chrome_options.add_argument('--disable-dev-shm-usage')
        self.chrome_options.add_argument('--disable-gpu')
        self.chrome_options.add_argument('--window-size=1920,1080')
        self.chrome_options.add_argument('--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        
        self.driver = None
        self.collected_count = 0
        
        logger.info("InfiniteScrollCollector initialized")

    async def collect_stackoverflow_infinite(self, max_pages: int = 20, 
                                           max_questions_per_page: int = 20) -> List[Dict[str, Any]]:
        """
        Stack Overflow ë¬´í•œ ìŠ¤í¬ë¡¤ + í˜ì´ì§€ë„¤ì´ì…˜ í•˜ì´ë¸Œë¦¬ë“œ ìˆ˜ì§‘
        """
        logger.info(f"ğŸŒŠ Stack Overflow ë¬´í•œ ìŠ¤í¬ë¡¤ ìˆ˜ì§‘ ì‹œì‘ (ìµœëŒ€ {max_pages}í˜ì´ì§€)")
        
        self._initialize_driver()
        collected_qa_pairs = []
        
        try:
            base_url = "https://stackoverflow.com/questions/tagged/excel-formula?tab=newest"
            
            for page in range(1, max_pages + 1):
                logger.info(f"ğŸ“„ í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘...")
                
                # í˜ì´ì§€ URL êµ¬ì„±
                if page == 1:
                    page_url = base_url
                else:
                    page_url = f"{base_url}&page={page}"
                
                # í˜ì´ì§€ ë¡œë“œ
                self.driver.get(page_url)
                await asyncio.sleep(2)  # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
                
                # ë¬´í•œ ìŠ¤í¬ë¡¤ë¡œ ë” ë§ì€ ì§ˆë¬¸ ë¡œë“œ
                page_questions = await self._infinite_scroll_questions(max_questions_per_page)
                
                if not page_questions:
                    logger.info(f"í˜ì´ì§€ {page}ì—ì„œ ì§ˆë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    break
                
                logger.info(f"ğŸ“ í˜ì´ì§€ {page}ì—ì„œ {len(page_questions)}ê°œ ì§ˆë¬¸ ë°œê²¬")
                
                # ê° ì§ˆë¬¸ì˜ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
                for i, question_link in enumerate(page_questions, 1):
                    try:
                        logger.info(f"   ì§ˆë¬¸ {i}/{len(page_questions)} ì²˜ë¦¬ ì¤‘...")
                        
                        qa_pair = await self._scrape_question_with_selenium(question_link)
                        
                        if qa_pair and qa_pair.get('answer'):
                            # ì¤‘ë³µ ì²´í¬
                            question_id = qa_pair['question'].get('question_id')
                            if question_id:
                                if not any(existing['question'].get('question_id') == question_id 
                                          for existing in collected_qa_pairs):
                                    collected_qa_pairs.append(qa_pair)
                                    
                                    # ì¤‘ë³µ ì¶”ì ê¸°ì— ë“±ë¡
                                    self.dedup_tracker.mark_stackoverflow_collected(
                                        question_id,
                                        qa_pair['question'].get('title', ''),
                                        quality_score=qa_pair.get('quality_score', 0),
                                        metadata={'page': page, 'source': 'infinite_scroll'}
                                    )
                        
                        # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
                        await asyncio.sleep(0.5)
                        
                    except Exception as e:
                        logger.warning(f"ì§ˆë¬¸ {question_link} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                        continue
                
                logger.info(f"âœ… í˜ì´ì§€ {page} ì™„ë£Œ: {len([p for p in collected_qa_pairs if p.get('answer')])}ê°œ Q&A ìˆ˜ì§‘")
                
                # í˜ì´ì§€ ê°„ ì§€ì—°
                await asyncio.sleep(2)
            
            logger.info(f"ğŸ‰ ë¬´í•œ ìŠ¤í¬ë¡¤ ìˆ˜ì§‘ ì™„ë£Œ: {len(collected_qa_pairs)}ê°œ Q&A ìŒ")
            return collected_qa_pairs
            
        finally:
            self._close_driver()

    async def collect_reddit_infinite_scroll(self, subreddit: str = "excel", 
                                            max_posts: int = 100) -> List[Dict[str, Any]]:
        """
        Reddit ë¬´í•œ ìŠ¤í¬ë¡¤ ë°©ì‹ìœ¼ë¡œ ê²Œì‹œë¬¼ ìˆ˜ì§‘
        """
        logger.info(f"ğŸŒŠ Reddit r/{subreddit} ë¬´í•œ ìŠ¤í¬ë¡¤ ìˆ˜ì§‘ ì‹œì‘ (ìµœëŒ€ {max_posts}ê°œ)")
        
        self._initialize_driver()
        collected_posts = []
        
        try:
            # Reddit í˜ì´ì§€ ë¡œë“œ
            reddit_url = f"https://www.reddit.com/r/{subreddit}/top/?t=month"
            self.driver.get(reddit_url)
            await asyncio.sleep(3)  # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
            
            # ë¬´í•œ ìŠ¤í¬ë¡¤ë¡œ ê²Œì‹œë¬¼ ìˆ˜ì§‘
            seen_posts = set()
            scroll_attempts = 0
            max_scroll_attempts = max_posts // 10  # ëŒ€ëµì ì¸ ìŠ¤í¬ë¡¤ íšŸìˆ˜
            
            while len(collected_posts) < max_posts and scroll_attempts < max_scroll_attempts:
                logger.info(f"ğŸ”„ ìŠ¤í¬ë¡¤ {scroll_attempts + 1}/{max_scroll_attempts} (ìˆ˜ì§‘ëœ ê²Œì‹œë¬¼: {len(collected_posts)}ê°œ)")
                
                # í˜„ì¬ í™”ë©´ì˜ ê²Œì‹œë¬¼ë“¤ ìˆ˜ì§‘
                current_posts = await self._extract_reddit_posts_from_page()
                
                new_posts_found = 0
                for post in current_posts:
                    post_id = post.get('id')
                    if post_id and post_id not in seen_posts:
                        seen_posts.add(post_id)
                        collected_posts.append(post)
                        new_posts_found += 1
                
                logger.info(f"   ìƒˆë¡œìš´ ê²Œì‹œë¬¼ {new_posts_found}ê°œ ë°œê²¬")
                
                # ìŠ¤í¬ë¡¤ ë‹¤ìš´
                await self._scroll_down_reddit()
                
                # ìƒˆë¡œìš´ ì½˜í…ì¸  ë¡œë”© ëŒ€ê¸°
                await asyncio.sleep(2)
                
                scroll_attempts += 1
                
                # ë” ì´ìƒ ìƒˆë¡œìš´ ê²Œì‹œë¬¼ì´ ì—†ìœ¼ë©´ ì¤‘ë‹¨
                if new_posts_found == 0:
                    logger.info("ë” ì´ìƒ ìƒˆë¡œìš´ ê²Œì‹œë¬¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                    break
            
            logger.info(f"ğŸ‰ Reddit ë¬´í•œ ìŠ¤í¬ë¡¤ ìˆ˜ì§‘ ì™„ë£Œ: {len(collected_posts)}ê°œ ê²Œì‹œë¬¼")
            return collected_posts
            
        finally:
            self._close_driver()

    def _initialize_driver(self):
        """Selenium WebDriver ì´ˆê¸°í™”"""
        try:
            self.driver = webdriver.Chrome(options=self.chrome_options)
            logger.info("âœ… WebDriver ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ WebDriver ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    def _close_driver(self):
        """WebDriver ì¢…ë£Œ"""
        if self.driver:
            self.driver.quit()
            self.driver = None
            logger.info("ğŸ”š WebDriver ì¢…ë£Œ")

    async def _infinite_scroll_questions(self, max_questions: int) -> List[str]:
        """Stack Overflow í˜ì´ì§€ì—ì„œ ë¬´í•œ ìŠ¤í¬ë¡¤ë¡œ ì§ˆë¬¸ ë§í¬ ìˆ˜ì§‘"""
        question_links = []
        seen_links = set()
        
        try:
            # ì´ˆê¸° ì§ˆë¬¸ë“¤ ìˆ˜ì§‘
            initial_questions = self.driver.find_elements(By.CSS_SELECTOR, '.s-post-summary .s-link')
            
            for element in initial_questions:
                href = element.get_attribute('href')
                if href and '/questions/' in href and href not in seen_links:
                    question_links.append(href)
                    seen_links.add(href)
            
            # ìŠ¤í¬ë¡¤í•˜ì—¬ ë” ë§ì€ ì§ˆë¬¸ ë¡œë“œ (ì¼ë¶€ í˜ì´ì§€ì—ì„œëŠ” ë™ì  ë¡œë”©)
            scroll_attempts = 3
            for i in range(scroll_attempts):
                # í˜ì´ì§€ ëê¹Œì§€ ìŠ¤í¬ë¡¤
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                await asyncio.sleep(1)
                
                # ìƒˆë¡œìš´ ì§ˆë¬¸ë“¤ ì°¾ê¸°
                all_questions = self.driver.find_elements(By.CSS_SELECTOR, '.s-post-summary .s-link')
                
                for element in all_questions:
                    href = element.get_attribute('href')
                    if href and '/questions/' in href and href not in seen_links:
                        question_links.append(href)
                        seen_links.add(href)
                        
                        if len(question_links) >= max_questions:
                            break
                
                if len(question_links) >= max_questions:
                    break
            
            return question_links[:max_questions]
            
        except Exception as e:
            logger.error(f"ë¬´í•œ ìŠ¤í¬ë¡¤ ì§ˆë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return question_links

    async def _extract_reddit_posts_from_page(self) -> List[Dict[str, Any]]:
        """í˜„ì¬ Reddit í˜ì´ì§€ì—ì„œ ê²Œì‹œë¬¼ ì •ë³´ ì¶”ì¶œ"""
        posts = []
        
        try:
            # Reddit ê²Œì‹œë¬¼ ìš”ì†Œë“¤ ì°¾ê¸°
            post_elements = self.driver.find_elements(By.CSS_SELECTOR, '[data-testid="post-container"], .Post')
            
            for post_element in post_elements:
                try:
                    # ê²Œì‹œë¬¼ ì œëª©
                    title_element = post_element.find_element(By.CSS_SELECTOR, 'h3, [data-testid="post-content"] h3, .s1b4bwul')
                    title = title_element.text.strip() if title_element else ""
                    
                    # ê²Œì‹œë¬¼ ë§í¬
                    link_element = post_element.find_element(By.CSS_SELECTOR, 'a[data-testid="post-title"], a')
                    post_url = link_element.get_attribute('href') if link_element else ""
                    
                    # ê²Œì‹œë¬¼ ID ì¶”ì¶œ
                    post_id = ""
                    if post_url:
                        import re
                        id_match = re.search(r'/comments/([a-zA-Z0-9]+)/', post_url)
                        if id_match:
                            post_id = id_match.group(1)
                    
                    # ì ìˆ˜ (ì—…ë³´íŠ¸)
                    score = 0
                    try:
                        score_element = post_element.find_element(By.CSS_SELECTOR, '[aria-label*="upvote"], .s1yr86ss')
                        score_text = score_element.text.strip()
                        if score_text.replace('k', '').replace('.', '').isdigit():
                            score = int(float(score_text.replace('k', '')) * (1000 if 'k' in score_text else 1))
                    except:
                        pass
                    
                    if title and post_id:
                        posts.append({
                            'id': post_id,
                            'title': title,
                            'url': post_url,
                            'score': score,
                            'source': 'reddit_infinite_scroll',
                            'collected_at': datetime.now().isoformat()
                        })
                
                except Exception as e:
                    logger.debug(f"ê°œë³„ ê²Œì‹œë¬¼ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                    continue
            
            return posts
            
        except Exception as e:
            logger.error(f"Reddit ê²Œì‹œë¬¼ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return posts

    async def _scroll_down_reddit(self):
        """Reddit í˜ì´ì§€ ë¬´í•œ ìŠ¤í¬ë¡¤"""
        try:
            # ì—¬ëŸ¬ ê°€ì§€ ìŠ¤í¬ë¡¤ ë°©ë²• ì‹œë„
            
            # ë°©ë²• 1: í˜ì´ì§€ ëê¹Œì§€ ìŠ¤í¬ë¡¤
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            await asyncio.sleep(1)
            
            # ë°©ë²• 2: ì¡°ê¸ˆì”© ìŠ¤í¬ë¡¤ (ë” ìì—°ìŠ¤ëŸ¬ìš´ ë¡œë”©)
            for i in range(3):
                self.driver.execute_script("window.scrollBy(0, 800);")
                await asyncio.sleep(0.5)
            
            # ë°©ë²• 3: "Load more" ë²„íŠ¼ì´ ìˆë‹¤ë©´ í´ë¦­
            try:
                load_more_button = self.driver.find_element(By.CSS_SELECTOR, '[data-testid="load-more"], .more')
                if load_more_button.is_displayed():
                    load_more_button.click()
                    await asyncio.sleep(2)
            except:
                pass
            
        except Exception as e:
            logger.debug(f"ìŠ¤í¬ë¡¤ ì‹¤íŒ¨: {e}")

    async def _scrape_question_with_selenium(self, question_url: str) -> Optional[Dict]:
        """Seleniumìœ¼ë¡œ ì§ˆë¬¸ ìƒì„¸ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘"""
        try:
            self.driver.get(question_url)
            await asyncio.sleep(1)
            
            # í˜ì´ì§€ ì†ŒìŠ¤ ê°€ì ¸ì™€ì„œ BeautifulSoupìœ¼ë¡œ íŒŒì‹±
            page_source = self.driver.page_source
            soup = BeautifulSoup(page_source, 'html.parser')
            
            # ê¸°ì¡´ ì›¹ ìŠ¤í¬ë˜í•‘ ë¡œì§ ì¬ì‚¬ìš©
            question_data = self._extract_question_data_selenium(soup, question_url)
            if not question_data:
                return None
            
            answer_data = self._extract_best_answer_data_selenium(soup)
            if not answer_data:
                return None
            
            quality_score = question_data.get('score', 0) + answer_data.get('score', 0)
            
            return {
                'question': question_data,
                'answer': answer_data,
                'quality_score': quality_score,
                'source': 'selenium_scraping',
                'collected_at': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Selenium ì§ˆë¬¸ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ {question_url}: {e}")
            return None

    def _extract_question_data_selenium(self, soup: BeautifulSoup, question_url: str) -> Optional[Dict]:
        """Seleniumìœ¼ë¡œ ì§ˆë¬¸ ë°ì´í„° ì¶”ì¶œ (ê¸°ì¡´ ë¡œì§ê³¼ ë™ì¼)"""
        try:
            import re
            
            # ì§ˆë¬¸ ID ì¶”ì¶œ
            question_id_match = re.search(r'/questions/(\d+)/', question_url)
            question_id = int(question_id_match.group(1)) if question_id_match else None
            
            # ì œëª© ì¶”ì¶œ
            title_element = soup.select_one('h1[itemprop="name"] a, h1 .question-hyperlink')
            title = title_element.get_text().strip() if title_element else ""
            
            # ì§ˆë¬¸ ë³¸ë¬¸ ì¶”ì¶œ
            question_body_element = soup.select_one('.s-prose.js-post-body')
            body_markdown = question_body_element.get_text().strip() if question_body_element else ""
            
            # ì ìˆ˜ ì¶”ì¶œ
            score_element = soup.select_one('.js-vote-count')
            score = 0
            if score_element:
                try:
                    score = int(score_element.get_text().strip())
                except ValueError:
                    score = 0
            
            # ì¡°íšŒìˆ˜ ì¶”ì¶œ
            view_count = 0
            view_element = soup.select_one('[title*="viewed"], .fs-body1')
            if view_element:
                view_text = view_element.get_text()
                view_match = re.search(r'(\d+)', view_text.replace(',', ''))
                if view_match:
                    view_count = int(view_match.group(1))
            
            # íƒœê·¸ ì¶”ì¶œ
            tags = []
            tag_elements = soup.select('.post-tag, .s-tag')
            for tag_element in tag_elements:
                tag_text = tag_element.get_text().strip()
                if tag_text:
                    tags.append(tag_text)
            
            return {
                'question_id': question_id,
                'title': title,
                'body_markdown': body_markdown,
                'score': score,
                'view_count': view_count,
                'tags': tags,
                'is_answered': True,
                'link': question_url
            }
            
        except Exception as e:
            logger.error(f"Selenium ì§ˆë¬¸ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None

    def _extract_best_answer_data_selenium(self, soup: BeautifulSoup) -> Optional[Dict]:
        """Seleniumìœ¼ë¡œ ë‹µë³€ ë°ì´í„° ì¶”ì¶œ (ê¸°ì¡´ ë¡œì§ê³¼ ë™ì¼)"""
        try:
            import re
            
            # ì±„íƒëœ ë‹µë³€ ì°¾ê¸°
            accepted_answer = soup.select_one('.answer.accepted-answer')
            
            if accepted_answer:
                return self._extract_answer_from_element_selenium(accepted_answer, is_accepted=True)
            
            # ì±„íƒëœ ë‹µë³€ì´ ì—†ìœ¼ë©´ ê°€ì¥ ë†’ì€ ì ìˆ˜ ë‹µë³€
            all_answers = soup.select('.answer')
            
            if not all_answers:
                return None
            
            best_answer = None
            best_score = -999
            
            for answer_element in all_answers:
                answer_data = self._extract_answer_from_element_selenium(answer_element, is_accepted=False)
                if answer_data and answer_data.get('score', 0) > best_score:
                    best_score = answer_data.get('score', 0)
                    best_answer = answer_data
            
            return best_answer if best_score >= 0 else None
            
        except Exception as e:
            logger.error(f"Selenium ë‹µë³€ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None

    def _extract_answer_from_element_selenium(self, answer_element, is_accepted: bool = False) -> Optional[Dict]:
        """Seleniumìœ¼ë¡œ ê°œë³„ ë‹µë³€ ì¶”ì¶œ (ê¸°ì¡´ ë¡œì§ê³¼ ë™ì¼)"""
        try:
            import re
            
            # ë‹µë³€ ID ì¶”ì¶œ
            answer_id = None
            id_attr = answer_element.get('id')
            if id_attr:
                id_match = re.search(r'answer-(\d+)', id_attr)
                if id_match:
                    answer_id = int(id_match.group(1))
            
            # ë‹µë³€ ë³¸ë¬¸ ì¶”ì¶œ
            answer_body_element = answer_element.select_one('.s-prose.js-post-body')
            body_markdown = answer_body_element.get_text().strip() if answer_body_element else ""
            
            # ë‹µë³€ ì ìˆ˜ ì¶”ì¶œ
            score_element = answer_element.select_one('.js-vote-count')
            score = 0
            if score_element:
                try:
                    score = int(score_element.get_text().strip())
                except ValueError:
                    score = 0
            
            return {
                'answer_id': answer_id,
                'body_markdown': body_markdown,
                'score': score,
                'is_accepted': is_accepted
            }
            
        except Exception as e:
            logger.error(f"Selenium ê°œë³„ ë‹µë³€ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None

    def get_collection_stats(self) -> Dict[str, Any]:
        """ìˆ˜ì§‘ í†µê³„"""
        return {
            'collected_count': self.collected_count,
            'collection_method': 'infinite_scroll',
            'driver_active': self.driver is not None
        }