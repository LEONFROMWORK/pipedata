#!/usr/bin/env python3
"""
ìˆ˜ì •ëœ Stack Overflow API Data Collector
ì£¼ìš” ìˆ˜ì •ì‚¬í•­:
1. API íŒŒë¼ë¯¸í„° ìˆ˜ì • (accepted=True ì œê±°)
2. ë‹µë³€ ìˆ˜ì§‘ ë¡œì§ ê°œì„ 
3. ì§ˆë¬¸-ë‹µë³€ ë§¤ì¹­ ì•Œê³ ë¦¬ì¦˜ ìˆ˜ì •
4. ì‹¤ì œ API ì‘ë‹µ êµ¬ì¡°ì— ë§ëŠ” ì½”ë“œ ìˆ˜ì •
"""
import asyncio
import logging
import time
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional, Tuple
from urllib.parse import urlencode
import json

import httpx
import backoff

from core.cache import APICache, LocalCache
from core.dedup_tracker import get_global_tracker
from config import Config

logger = logging.getLogger('pipeline.fixed_stackoverflow_collector')

class RateLimitExceeded(Exception):
    """Custom exception for rate limit handling"""
    pass

class FixedStackOverflowCollector:
    """
    ìˆ˜ì •ëœ Stack Overflow API collector
    - ë‹µë³€ì´ ìˆëŠ” ì§ˆë¬¸ ìš°ì„  ìˆ˜ì§‘
    - ì˜¬ë°”ë¥¸ API íŒŒë¼ë¯¸í„° ì‚¬ìš©
    - ê°œì„ ëœ ë‹µë³€ ë§¤ì¹­ ë¡œì§
    """
    
    def __init__(self, cache: APICache):
        self.cache = cache
        self.config = Config.SO_API_CONFIG
        self.rate_config = Config.RATE_LIMITING
        self.dedup_tracker = get_global_tracker()
        
        # API client setup
        self.client = httpx.AsyncClient(
            timeout=30,
            headers={
                'User-Agent': 'Excel-QA-Dataset-Pipeline/1.0'
            }
        )
        
        # Rate limiting tracking
        self.requests_today = 0
        self.last_request_time = 0
        self.daily_quota_reset = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
        
        logger.info("FixedStackOverflowCollector initialized")

    async def collect_excel_questions_fixed(self, from_date: Optional[datetime] = None, 
                                          max_pages: int = 50) -> List[Dict[str, Any]]:
        """
        ìˆ˜ì •ëœ ë©”ì¸ ìˆ˜ì§‘ ë©”ì†Œë“œ
        - ë‹µë³€ì´ ìˆëŠ” ì§ˆë¬¸ë§Œ ìš°ì„  ìˆ˜ì§‘
        - ê°œì„ ëœ ë§¤ì¹­ ë¡œì§
        """
        if from_date is None:
            from_date = datetime.now() - timedelta(days=30)
        
        logger.info(f"ğŸš€ Fixed collection starting from {from_date}")
        
        collected_pairs = []
        page = 1
        has_more = True
        
        while has_more and page <= max_pages and self._check_rate_limit():
            try:
                logger.info(f"ğŸ“„ Processing page {page}")
                
                # 1. ë‹µë³€ì´ ìˆëŠ” ì§ˆë¬¸ë“¤ ìˆ˜ì§‘ (ìˆ˜ì •ëœ íŒŒë¼ë¯¸í„°)
                questions_batch = await self._get_answered_questions_batch(
                    from_date=from_date,
                    page=page
                )
                
                if not questions_batch['items']:
                    logger.info("No more questions available")
                    break
                
                # 2. ì¤‘ë³µ ì²´í¬
                new_questions = self.dedup_tracker.filter_new_stackoverflow_questions(questions_batch['items'])
                
                if not new_questions:
                    logger.info(f"Page {page}: All questions already collected")
                    page += 1
                    continue
                
                logger.info(f"ğŸ“ Found {len(new_questions)} new questions")
                
                # 3. ë‹µë³€ ë°ì´í„° ìˆ˜ì§‘ ë° ë§¤ì¹­ (ê°œì„ ëœ ë¡œì§)
                complete_pairs = await self._collect_complete_qa_pairs(new_questions)
                
                logger.info(f"âœ… Created {len(complete_pairs)} complete Q&A pairs")
                
                # 4. ì¤‘ë³µ ì¶”ì ê¸°ì— ë“±ë¡
                for pair in complete_pairs:
                    question = pair['question']
                    question_id = question.get('question_id')
                    title = question.get('title', '')
                    if question_id:
                        self.dedup_tracker.mark_stackoverflow_collected(
                            question_id, title,
                            quality_score=question.get('score', 0),
                            metadata={'page': page, 'has_answer': bool(pair.get('answer'))}
                        )
                
                collected_pairs.extend(complete_pairs)
                
                has_more = questions_batch.get('has_more', False)
                page += 1
                
                # Rate limiting
                await self._rate_limit_delay()
                
            except RateLimitExceeded:
                logger.warning("Daily rate limit exceeded")
                break
            except Exception as e:
                logger.error(f"Error on page {page}: {e}")
                break
        
        logger.info(f"ğŸ‰ Collection complete: {len(collected_pairs)} total Q&A pairs")
        return collected_pairs

    @backoff.on_exception(
        backoff.expo,
        (httpx.HTTPError, httpx.TimeoutException),
        max_tries=5,
        max_time=300,
        jitter=backoff.full_jitter
    )
    async def _get_answered_questions_batch(self, from_date: datetime, page: int) -> Dict[str, Any]:
        """
        ë‹µë³€ì´ ìˆëŠ” ì§ˆë¬¸ë“¤ ìˆ˜ì§‘ (ìˆ˜ì •ëœ API íŒŒë¼ë¯¸í„°)
        """
        # ìºì‹œ í™•ì¸
        cache_key = f"fixed_questions_page_{page}_{from_date.isoformat()}"
        cached_result = self.cache.get_stackoverflow_response('fixed_questions', {'page': page, 'from_date': from_date.isoformat()})
        
        if cached_result:
            logger.info(f"Using cached questions for page {page}")
            return cached_result
        
        # ìˆ˜ì •ëœ API íŒŒë¼ë¯¸í„° - accepted ì œê±°, answers í¬í•¨ í•„í„° ì‚¬ìš©
        params = {
            'site': 'stackoverflow',
            'order': 'desc', 
            'sort': 'votes',  # íˆ¬í‘œìˆœìœ¼ë¡œ ì •ë ¬ (ë‹µë³€ ìˆì„ ê°€ëŠ¥ì„± ë†’ìŒ)
            'tagged': 'excel-formula',
            'page': page,
            'pagesize': 50,  # ì¤„ì—¬ì„œ í’ˆì§ˆ ë†’ì´ê¸°
            'fromdate': int(from_date.timestamp()),
            'filter': '!nNPvSNdWme',  # ê¸°ë³¸ í•„í„°ì— ë‹µë³€ ì •ë³´ í¬í•¨
            'key': Config.STACKOVERFLOW_API_KEY,
            # accepted=True ì œê±° - ì´ íŒŒë¼ë¯¸í„°ê°€ ë¬¸ì œì˜€ìŒ
        }
        
        url = f"{self.config['base_url']}/questions?" + urlencode(params)
        
        try:
            response = await self.client.get(url)
            response.raise_for_status()
            
            self._update_rate_limit_tracking(response)
            
            result = response.json()
            
            # ìºì‹œ ì €ì¥
            self.cache.cache_stackoverflow_response(
                'fixed_questions',
                {'page': page, 'from_date': from_date.isoformat()},
                result
            )
            
            logger.info(f"âœ… API success: page {page}, {len(result.get('items', []))} questions")
            return result
            
        except httpx.HTTPStatusError as e:
            if e.response.status_code == 429:
                raise RateLimitExceeded("Rate limit exceeded")
            logger.error(f"HTTP error: {e.response.status_code}")
            raise

    async def _collect_complete_qa_pairs(self, questions: List[Dict]) -> List[Dict]:
        """
        ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ë§¤ì¹­í•˜ì—¬ ì™„ì „í•œ Q&A ìŒ ìƒì„± (ê°œì„ ëœ ë¡œì§)
        """
        complete_pairs = []
        
        # 1. ì´ë¯¸ accepted_answer_idê°€ ìˆëŠ” ì§ˆë¬¸ë“¤ ì²˜ë¦¬
        questions_with_accepted = []
        for question in questions:
            if question.get('accepted_answer_id'):
                questions_with_accepted.append(question)
        
        logger.info(f"ğŸ“‹ Questions with accepted answers: {len(questions_with_accepted)}")
        
        # 2. ì±„íƒëœ ë‹µë³€ë“¤ ë°°ì¹˜ ìˆ˜ì§‘
        if questions_with_accepted:
            answer_ids = [q['accepted_answer_id'] for q in questions_with_accepted]
            answers_data = await self._get_answers_batch_fixed(answer_ids)
            
            # 3. ì§ˆë¬¸-ë‹µë³€ ë§¤ì¹­
            answers_by_id = {ans['answer_id']: ans for ans in answers_data.get('items', [])}
            
            for question in questions_with_accepted:
                answer_id = question.get('accepted_answer_id')
                if answer_id in answers_by_id:
                    complete_pairs.append({
                        'question': question,
                        'answer': answers_by_id[answer_id],
                        'quality_score': question.get('score', 0) + answers_by_id[answer_id].get('score', 0)
                    })
        
        # 4. accepted_answer_idê°€ ì—†ëŠ” ì§ˆë¬¸ë“¤ì€ ë‹µë³€ ê²€ìƒ‰
        questions_without_accepted = [q for q in questions if not q.get('accepted_answer_id')]
        
        if questions_without_accepted:
            logger.info(f"ğŸ” Searching answers for {len(questions_without_accepted)} questions")
            
            for question in questions_without_accepted[:10]:  # ì œí•œí•´ì„œ API ì ˆì•½
                try:
                    question_answers = await self._get_question_answers(question['question_id'])
                    
                    if question_answers and len(question_answers.get('items', [])) > 0:
                        # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ë‹µë³€ ì„ íƒ
                        best_answer = max(question_answers['items'], key=lambda x: x.get('score', 0))
                        
                        if best_answer.get('score', 0) >= 1:  # ìµœì†Œ ì ìˆ˜ 1 ì´ìƒ
                            complete_pairs.append({
                                'question': question,
                                'answer': best_answer,
                                'quality_score': question.get('score', 0) + best_answer.get('score', 0)
                            })
                        
                        await asyncio.sleep(0.1)  # API ë³´í˜¸
                        
                except Exception as e:
                    logger.warning(f"Failed to get answers for question {question.get('question_id')}: {e}")
                    continue
        
        logger.info(f"ğŸ¯ Generated {len(complete_pairs)} complete Q&A pairs")
        return complete_pairs

    @backoff.on_exception(
        backoff.expo,
        (httpx.HTTPError, httpx.TimeoutException),
        max_tries=3,
        max_time=60
    )
    async def _get_answers_batch_fixed(self, answer_ids: List[int]) -> Dict[str, Any]:
        """ìˆ˜ì •ëœ ë‹µë³€ ë°°ì¹˜ ìˆ˜ì§‘"""
        ids_str = ';'.join(map(str, answer_ids))
        
        # ìºì‹œ í™•ì¸
        cached_result = self.cache.get_stackoverflow_response('fixed_answers', {'ids': ids_str})
        if cached_result:
            return cached_result
        
        params = {
            'site': 'stackoverflow',
            'order': 'desc',
            'sort': 'votes',
            'filter': 'withbody',  # ë‹µë³€ ë³¸ë¬¸ í¬í•¨
            'key': Config.STACKOVERFLOW_API_KEY
        }
        
        url = f"{self.config['base_url']}/answers/{ids_str}?" + urlencode(params)
        
        response = await self.client.get(url)
        response.raise_for_status()
        
        self._update_rate_limit_tracking(response)
        result = response.json()
        
        # ìºì‹œ ì €ì¥
        self.cache.cache_stackoverflow_response('fixed_answers', {'ids': ids_str}, result)
        
        return result

    @backoff.on_exception(
        backoff.expo,
        (httpx.HTTPError, httpx.TimeoutException),
        max_tries=3,
        max_time=60
    )
    async def _get_question_answers(self, question_id: int) -> Dict[str, Any]:
        """íŠ¹ì • ì§ˆë¬¸ì˜ ëª¨ë“  ë‹µë³€ ìˆ˜ì§‘"""
        params = {
            'site': 'stackoverflow',
            'order': 'desc',
            'sort': 'votes',
            'filter': 'withbody',
            'key': Config.STACKOVERFLOW_API_KEY
        }
        
        url = f"{self.config['base_url']}/questions/{question_id}/answers?" + urlencode(params)
        
        response = await self.client.get(url)
        response.raise_for_status()
        
        self._update_rate_limit_tracking(response)
        
        return response.json()

    def _check_rate_limit(self) -> bool:
        """Rate limit í™•ì¸"""
        now = datetime.now()
        if now >= self.daily_quota_reset + timedelta(days=1):
            self.requests_today = 0
            self.daily_quota_reset = now.replace(hour=0, minute=0, second=0, microsecond=0)
        
        return self.requests_today < self.rate_config['max_requests_per_day']

    def _update_rate_limit_tracking(self, response: httpx.Response) -> None:
        """Rate limit ì¶”ì  ì—…ë°ì´íŠ¸"""
        self.requests_today += 1
        self.last_request_time = time.time()
        
        quota_remaining = response.headers.get('x-ratelimit-remaining')
        if quota_remaining:
            logger.info(f"API quota remaining: {quota_remaining}")

    async def _rate_limit_delay(self) -> None:
        """Rate limiting ì§€ì—°"""
        min_delay = 60 / self.rate_config['requests_per_minute']
        elapsed = time.time() - self.last_request_time
        
        if elapsed < min_delay:
            delay = min_delay - elapsed
            await asyncio.sleep(delay)

    async def close(self) -> None:
        """HTTP client ì •ë¦¬"""
        await self.client.aclose()

    def get_collection_stats(self) -> Dict[str, Any]:
        """ìˆ˜ì§‘ í†µê³„"""
        return {
            'requests_today': self.requests_today,
            'daily_quota_remaining': self.rate_config['max_requests_per_day'] - self.requests_today,
            'last_request': datetime.fromtimestamp(self.last_request_time) if self.last_request_time else None,
            'daily_quota_reset': self.daily_quota_reset,
            'rate_limit_per_minute': self.rate_config['requests_per_minute']
        }