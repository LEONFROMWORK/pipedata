#!/usr/bin/env python3
"""
ì›¹ ìŠ¤í¬ë˜í•‘ ê¸°ë°˜ Stack Overflow ìˆ˜ì§‘ê¸°
- s-pagination í´ë˜ìŠ¤ ê¸°ë°˜ í˜ì´ì§€ ìˆœíšŒ
- ë” ë§ì€ ë°ì´í„° í™•ë³´ë¥¼ ìœ„í•œ ì›¹ ìŠ¤í¬ë˜í•‘
- API ì œí•œ ì—†ì´ ëŒ€ëŸ‰ ë°ì´í„° ìˆ˜ì§‘
"""
import asyncio
import logging
import time
import re
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
from urllib.parse import urljoin, parse_qs, urlparse
import json

import httpx
from bs4 import BeautifulSoup
import backoff

from core.cache import APICache, LocalCache
from core.dedup_tracker import get_global_tracker
from config import Config

logger = logging.getLogger('pipeline.web_scraping_stackoverflow')

class WebScrapingStackOverflowCollector:
    """
    ì›¹ ìŠ¤í¬ë˜í•‘ ê¸°ë°˜ Stack Overflow ìˆ˜ì§‘ê¸°
    - s-pagination í´ë˜ìŠ¤ë¡œ í˜ì´ì§€ ìˆœíšŒ
    - ì§ˆë¬¸ ëª©ë¡ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘
    - ê°œë³„ ì§ˆë¬¸/ë‹µë³€ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘
    """
    
    def __init__(self, cache: APICache):
        self.cache = cache
        self.dedup_tracker = get_global_tracker()
        
        # HTTP í´ë¼ì´ì–¸íŠ¸ ì„¤ì • (ë¸Œë¼ìš°ì €ì²˜ëŸ¼ ë³´ì´ê²Œ)
        self.client = httpx.AsyncClient(
            timeout=30,
            headers={
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
            },
            follow_redirects=True
        )
        
        self.base_url = "https://stackoverflow.com"
        self.collected_count = 0
        self.last_request_time = 0
        
        logger.info("WebScrapingStackOverflowCollector initialized")

    async def collect_excel_questions_web(self, max_pages: int = 50) -> List[Dict[str, Any]]:
        """
        ì›¹ ìŠ¤í¬ë˜í•‘ìœ¼ë¡œ Excel ì§ˆë¬¸ ìˆ˜ì§‘
        - í˜ì´ì§€ë„¤ì´ì…˜ ìë™ ìˆœíšŒ
        - ì§ˆë¬¸ë³„ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
        """
        logger.info(f"ğŸŒ ì›¹ ìŠ¤í¬ë˜í•‘ ìˆ˜ì§‘ ì‹œì‘ (ìµœëŒ€ {max_pages}í˜ì´ì§€)")
        
        collected_qa_pairs = []
        current_page = 1
        
        # Excel ê´€ë ¨ íƒœê·¸ ê²€ìƒ‰ URL
        base_search_url = f"{self.base_url}/questions/tagged/excel-formula"
        
        while current_page <= max_pages:
            try:
                logger.info(f"ğŸ“„ í˜ì´ì§€ {current_page} ì²˜ë¦¬ ì¤‘...")
                
                # í˜ì´ì§€ URL êµ¬ì„±
                if current_page == 1:
                    page_url = base_search_url
                else:
                    page_url = f"{base_search_url}?tab=newest&page={current_page}"
                
                # ì§ˆë¬¸ ëª©ë¡ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘
                question_links = await self._scrape_question_list_page(page_url)
                
                if not question_links:
                    logger.info(f"í˜ì´ì§€ {current_page}ì—ì„œ ì§ˆë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ. ìˆ˜ì§‘ ì¢…ë£Œ.")
                    break
                
                logger.info(f"ğŸ“ í˜ì´ì§€ {current_page}ì—ì„œ {len(question_links)}ê°œ ì§ˆë¬¸ ë°œê²¬")
                
                # ê° ì§ˆë¬¸ì˜ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
                page_qa_pairs = []
                for i, question_link in enumerate(question_links[:10], 1):  # í˜ì´ì§€ë‹¹ ìµœëŒ€ 10ê°œë¡œ ì œí•œ
                    try:
                        logger.info(f"   ì§ˆë¬¸ {i}/{len(question_links[:10])} ì²˜ë¦¬ ì¤‘...")
                        
                        qa_pair = await self._scrape_question_detail(question_link)
                        
                        if qa_pair and qa_pair.get('answer'):
                            # ì¤‘ë³µ ì²´í¬
                            question_id = qa_pair['question'].get('question_id')
                            if question_id:
                                # ê°„ë‹¨í•œ ì¤‘ë³µ ì²´í¬ (ID ê¸°ë°˜)
                                if not any(existing['question'].get('question_id') == question_id 
                                          for existing in collected_qa_pairs):
                                    page_qa_pairs.append(qa_pair)
                                    
                                    # ì¤‘ë³µ ì¶”ì ê¸°ì— ë“±ë¡
                                    self.dedup_tracker.mark_stackoverflow_collected(
                                        question_id, 
                                        qa_pair['question'].get('title', ''),
                                        quality_score=qa_pair.get('quality_score', 0),
                                        metadata={'page': current_page, 'source': 'web_scraping'}
                                    )
                        
                        # ìš”ì²­ ê°„ê²© ì¡°ì ˆ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                        await self._polite_delay()
                        
                    except Exception as e:
                        logger.warning(f"ì§ˆë¬¸ {question_link} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                        continue
                
                collected_qa_pairs.extend(page_qa_pairs)
                logger.info(f"âœ… í˜ì´ì§€ {current_page} ì™„ë£Œ: {len(page_qa_pairs)}ê°œ Q&A ìˆ˜ì§‘")
                
                # ë‹¤ìŒ í˜ì´ì§€ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                has_next = await self._check_next_page_exists(page_url)
                if not has_next:
                    logger.info("ë§ˆì§€ë§‰ í˜ì´ì§€ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.")
                    break
                
                current_page += 1
                
                # í˜ì´ì§€ ê°„ ì§€ì—°
                await asyncio.sleep(2)
                
            except Exception as e:
                logger.error(f"í˜ì´ì§€ {current_page} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                break
        
        logger.info(f"ğŸ‰ ì›¹ ìŠ¤í¬ë˜í•‘ ìˆ˜ì§‘ ì™„ë£Œ: {len(collected_qa_pairs)}ê°œ Q&A ìŒ")
        return collected_qa_pairs

    async def _scrape_question_list_page(self, page_url: str) -> List[str]:
        """ì§ˆë¬¸ ëª©ë¡ í˜ì´ì§€ì—ì„œ ì§ˆë¬¸ ë§í¬ë“¤ ì¶”ì¶œ"""
        try:
            response = await self.client.get(page_url)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ì§ˆë¬¸ ë§í¬ ì¶”ì¶œ (.s-post-summary í´ë˜ìŠ¤ ë‚´ì˜ ì œëª© ë§í¬)
            question_links = []
            
            # Stack Overflowì˜ ì§ˆë¬¸ ì œëª© ë§í¬ ì„ íƒì
            question_elements = soup.select('.s-post-summary .s-link')
            
            for element in question_elements:
                href = element.get('href')
                if href and '/questions/' in href:
                    full_url = urljoin(self.base_url, href)
                    question_links.append(full_url)
            
            # ì¤‘ë³µ ì œê±°
            question_links = list(dict.fromkeys(question_links))
            
            return question_links
            
        except Exception as e:
            logger.error(f"ì§ˆë¬¸ ëª©ë¡ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ {page_url}: {e}")
            return []

    async def _scrape_question_detail(self, question_url: str) -> Optional[Dict]:
        """ê°œë³„ ì§ˆë¬¸ í˜ì´ì§€ì—ì„œ ì§ˆë¬¸ê³¼ ë‹µë³€ ìƒì„¸ ì •ë³´ ì¶”ì¶œ"""
        try:
            response = await self.client.get(question_url)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ì§ˆë¬¸ ì •ë³´ ì¶”ì¶œ
            question_data = self._extract_question_data(soup, question_url)
            if not question_data:
                return None
            
            # ë‹µë³€ ì •ë³´ ì¶”ì¶œ (ì±„íƒëœ ë‹µë³€ ë˜ëŠ” ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ë‹µë³€)
            answer_data = self._extract_best_answer_data(soup)
            
            if not answer_data:
                return None  # ë‹µë³€ì´ ì—†ëŠ” ì§ˆë¬¸ì€ ì œì™¸
            
            # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
            quality_score = question_data.get('score', 0) + answer_data.get('score', 0)
            
            return {
                'question': question_data,
                'answer': answer_data,
                'quality_score': quality_score,
                'source': 'web_scraping',
                'collected_at': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"ì§ˆë¬¸ ìƒì„¸ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ {question_url}: {e}")
            return None

    def _extract_question_data(self, soup: BeautifulSoup, question_url: str) -> Optional[Dict]:
        """ì§ˆë¬¸ ë°ì´í„° ì¶”ì¶œ"""
        try:
            # ì§ˆë¬¸ ID ì¶”ì¶œ (URLì—ì„œ)
            question_id_match = re.search(r'/questions/(\d+)/', question_url)
            question_id = int(question_id_match.group(1)) if question_id_match else None
            
            # ì œëª© ì¶”ì¶œ
            title_element = soup.select_one('h1[itemprop="name"] a, h1 .question-hyperlink')
            title = title_element.get_text().strip() if title_element else ""
            
            # ì§ˆë¬¸ ë³¸ë¬¸ ì¶”ì¶œ
            question_body_element = soup.select_one('.s-prose.js-post-body')
            body_markdown = ""
            if question_body_element:
                # HTMLì„ ë§ˆí¬ë‹¤ìš´ ìŠ¤íƒ€ì¼ë¡œ ë³€í™˜ (ê°„ë‹¨íˆ)
                body_markdown = question_body_element.get_text().strip()
            
            # ì ìˆ˜ ì¶”ì¶œ
            score_element = soup.select_one('.js-vote-count')
            score = 0
            if score_element:
                try:
                    score = int(score_element.get_text().strip())
                except ValueError:
                    score = 0
            
            # ì¡°íšŒìˆ˜ ì¶”ì¶œ
            view_count = 0
            view_element = soup.select_one('[title*="viewed"], .fs-body1')
            if view_element:
                view_text = view_element.get_text()
                view_match = re.search(r'(\d+)', view_text.replace(',', ''))
                if view_match:
                    view_count = int(view_match.group(1))
            
            # íƒœê·¸ ì¶”ì¶œ
            tags = []
            tag_elements = soup.select('.post-tag, .s-tag')
            for tag_element in tag_elements:
                tag_text = tag_element.get_text().strip()
                if tag_text:
                    tags.append(tag_text)
            
            return {
                'question_id': question_id,
                'title': title,
                'body_markdown': body_markdown,
                'score': score,
                'view_count': view_count,
                'tags': tags,
                'is_answered': True,  # ë‹µë³€ì´ ìˆëŠ” ê²ƒë§Œ ì²˜ë¦¬í•˜ë¯€ë¡œ
                'link': question_url
            }
            
        except Exception as e:
            logger.error(f"ì§ˆë¬¸ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None

    def _extract_best_answer_data(self, soup: BeautifulSoup) -> Optional[Dict]:
        """ê°€ì¥ ì¢‹ì€ ë‹µë³€ ë°ì´í„° ì¶”ì¶œ (ì±„íƒëœ ë‹µë³€ ìš°ì„ , ì—†ìœ¼ë©´ ë†’ì€ ì ìˆ˜)"""
        try:
            # 1. ì±„íƒëœ ë‹µë³€ ì°¾ê¸°
            accepted_answer = soup.select_one('.answer.accepted-answer')
            
            if accepted_answer:
                return self._extract_answer_from_element(accepted_answer, is_accepted=True)
            
            # 2. ì±„íƒëœ ë‹µë³€ì´ ì—†ìœ¼ë©´ ëª¨ë“  ë‹µë³€ ì¤‘ ê°€ì¥ ë†’ì€ ì ìˆ˜
            all_answers = soup.select('.answer')
            
            if not all_answers:
                return None
            
            best_answer = None
            best_score = -999
            
            for answer_element in all_answers:
                answer_data = self._extract_answer_from_element(answer_element, is_accepted=False)
                if answer_data and answer_data.get('score', 0) > best_score:
                    best_score = answer_data.get('score', 0)
                    best_answer = answer_data
            
            return best_answer if best_score >= 0 else None  # ìŒìˆ˜ ì ìˆ˜ëŠ” ì œì™¸
            
        except Exception as e:
            logger.error(f"ë‹µë³€ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None

    def _extract_answer_from_element(self, answer_element, is_accepted: bool = False) -> Optional[Dict]:
        """ë‹µë³€ ìš”ì†Œì—ì„œ ë‹µë³€ ë°ì´í„° ì¶”ì¶œ"""
        try:
            # ë‹µë³€ ID ì¶”ì¶œ
            answer_id = None
            id_attr = answer_element.get('id')
            if id_attr:
                id_match = re.search(r'answer-(\d+)', id_attr)
                if id_match:
                    answer_id = int(id_match.group(1))
            
            # ë‹µë³€ ë³¸ë¬¸ ì¶”ì¶œ
            answer_body_element = answer_element.select_one('.s-prose.js-post-body')
            body_markdown = ""
            if answer_body_element:
                body_markdown = answer_body_element.get_text().strip()
            
            # ë‹µë³€ ì ìˆ˜ ì¶”ì¶œ
            score_element = answer_element.select_one('.js-vote-count')
            score = 0
            if score_element:
                try:
                    score = int(score_element.get_text().strip())
                except ValueError:
                    score = 0
            
            return {
                'answer_id': answer_id,
                'body_markdown': body_markdown,
                'score': score,
                'is_accepted': is_accepted
            }
            
        except Exception as e:
            logger.error(f"ê°œë³„ ë‹µë³€ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None

    async def _check_next_page_exists(self, current_page_url: str) -> bool:
        """ë‹¤ìŒ í˜ì´ì§€ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
        try:
            response = await self.client.get(current_page_url)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # s-pagination í´ë˜ìŠ¤ì—ì„œ ë‹¤ìŒ í˜ì´ì§€ ë§í¬ ì°¾ê¸°
            pagination = soup.select_one('.s-pagination')
            if pagination:
                next_link = pagination.select_one('a[rel="next"], a:contains("Next")')
                return next_link is not None
            
            return False
            
        except Exception as e:
            logger.error(f"ë‹¤ìŒ í˜ì´ì§€ í™•ì¸ ì‹¤íŒ¨: {e}")
            return False

    async def _polite_delay(self):
        """ì„œë²„ì— ë¶€ë‹´ì„ ì£¼ì§€ ì•ŠëŠ” ì •ì¤‘í•œ ì§€ì—°"""
        min_delay = 1.0  # ìµœì†Œ 1ì´ˆ ì§€ì—°
        elapsed = time.time() - self.last_request_time
        
        if elapsed < min_delay:
            delay = min_delay - elapsed
            await asyncio.sleep(delay)
        
        self.last_request_time = time.time()

    async def close(self):
        """HTTP í´ë¼ì´ì–¸íŠ¸ ì •ë¦¬"""
        await self.client.aclose()

    def get_collection_stats(self) -> Dict[str, Any]:
        """ìˆ˜ì§‘ í†µê³„"""
        return {
            'collected_count': self.collected_count,
            'last_request_time': datetime.fromtimestamp(self.last_request_time) if self.last_request_time else None,
            'collection_method': 'web_scraping'
        }