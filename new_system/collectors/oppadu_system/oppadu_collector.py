"""
ë…ë¦½ Oppadu ìˆ˜ì§‘ê¸°
Oppadu ì „ìš© ë…ë¦½ ì‹œìŠ¤í…œì„ ì‚¬ìš©í•˜ëŠ” ì™„ì „íˆ ë¶„ë¦¬ëœ ìˆ˜ì§‘ê¸°
"""
import asyncio
import logging
import time
import random
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional, Tuple
from urllib.parse import urljoin, urlparse
import json
import re
import sys
from pathlib import Path

# ìƒìœ„ ë””ë ‰í† ë¦¬ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent.parent))

import aiohttp
import cloudscraper
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.action_chains import ActionChains
from selenium.common.exceptions import TimeoutException, NoSuchElementException

# ë…ë¦½ ì‹œìŠ¤í…œ import
from .oppadu_cache import OppaduCache, OppaduWebCache
from .oppadu_dedup_tracker import get_oppadu_tracker
from .oppadu_config import get_oppadu_config

# ê³µí†µ ìœ í‹¸ë¦¬í‹° import
from shared.utils import generate_unique_id, calculate_quality_score, extract_code_blocks, clean_text
from shared.data_models import QAEntry, CollectionStats

logger = logging.getLogger('oppadu_system.collector')

class OppaduAntiDetection:
    """Oppadu í¬ë¡¤ë§ ë°©ì§€ ìš°íšŒë¥¼ ìœ„í•œ ê³ ê¸‰ ê¸°ìˆ """
    
    def __init__(self, config):
        self.config = config
        web_config = config.get_web_config()
        self.user_agents = web_config['user_agents']
        self.session_headers = self._generate_headers()
        
    def _generate_headers(self) -> Dict[str, str]:
        """í•œêµ­ ì‚¬ìš©ìë¥¼ ëª¨ë°©í•œ í—¤ë” ìƒì„±"""
        return {
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Ch-Ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
            'Sec-Ch-Ua-Mobile': '?0',
            'Sec-Ch-Ua-Platform': '"Windows"',
            'Cache-Control': 'max-age=0'
        }
    
    def rotate_headers(self):
        """í—¤ë” ë¡œí…Œì´ì…˜"""
        self.session_headers['User-Agent'] = random.choice(self.user_agents)
        
    async def human_delay(self, min_delay: float = 2.0, max_delay: float = 5.0):
        """ì¸ê°„ì ì¸ ì§€ì—° ì‹œë®¬ë ˆì´ì…˜"""
        delay = random.uniform(min_delay, max_delay)
        await asyncio.sleep(delay)

class OppaduCollector:
    """ë…ë¦½ Oppadu ìˆ˜ì§‘ê¸°"""
    
    def __init__(self):
        """Oppadu ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”"""
        self.config = get_oppadu_config()
        self.dedup_tracker = get_oppadu_tracker()
        
        # ë…ë¦½ ìºì‹œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        oppadu_cache = OppaduCache(self.config.cache_db_path)
        self.cache = OppaduWebCache(oppadu_cache)
        
        # í¬ë¡¤ë§ ë°©ì§€ ìš°íšŒ ì‹œìŠ¤í…œ
        self.anti_detection = OppaduAntiDetection(self.config)
        
        # CloudScraper ì„¤ì •
        self.scraper = cloudscraper.create_scraper(
            browser={
                'browser': 'chrome',
                'platform': 'windows',
                'desktop': True
            }
        )
        
        # Selenium ë“œë¼ì´ë²„ (í•„ìš”ì‹œ ì‚¬ìš©)
        self.driver = None
        
        # ìˆ˜ì§‘ í†µê³„
        self.stats = {
            'total_processed': 0,
            'total_collected': 0,
            'total_skipped': 0,
            'duplicate_posts': 0,
            'quality_failures': 0,
            'crawling_errors': 0,
            'blocked_requests': 0
        }
        
        logger.info("ë…ë¦½ Oppadu ìˆ˜ì§‘ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def collect_excel_qa_data(self, max_items: int = 100) -> List[QAEntry]:
        """Excel Q&A ë°ì´í„° ìˆ˜ì§‘ (ê°œì„ ëœ í˜ì´ì§€ë„¤ì´ì…˜ ì§€ì›)"""
        logger.info(f"Oppadu Excel Q&A ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ (ìµœëŒ€ {max_items}ê°œ)")
        
        collected_data = []
        collection_config = self.config.get_collection_config()
        
        try:
            # ì²« í˜ì´ì§€ë¶€í„° ì‹œì‘
            page_urls = await self._get_all_page_urls()
            max_pages = min(len(page_urls), collection_config['max_pages'])
            
            logger.info(f"ì´ {len(page_urls)}ê°œ í˜ì´ì§€ ë°œê²¬, ìµœëŒ€ {max_pages}ê°œ í˜ì´ì§€ ì²˜ë¦¬")
            
            for page_num, page_url in enumerate(page_urls[:max_pages], 1):
                if len(collected_data) >= max_items:
                    break
                
                logger.info(f"í˜ì´ì§€ {page_num}/{max_pages} ì²˜ë¦¬ ì¤‘: {page_url}")
                
                # ë‹µë³€ ì™„ë£Œëœ ê²Œì‹œê¸€ ëª©ë¡ ìˆ˜ì§‘
                answered_posts = await self._get_answered_posts_from_url(page_url)
                
                if not answered_posts:
                    logger.info(f"í˜ì´ì§€ {page_num}ì—ì„œ ë” ì´ìƒ ë‹µë³€ ì™„ë£Œëœ ê²Œì‹œê¸€ì´ ì—†ìŒ")
                    continue
                
                logger.info(f"ğŸ¯ {len(answered_posts)}ê°œì˜ ë‹µë³€ ì™„ë£Œ ê²Œì‹œê¸€ ë°œê²¬")
                
                # ê° ê²Œì‹œê¸€ì˜ ìƒì„¸ ë°ì´í„° ìˆ˜ì§‘
                for post_url in answered_posts:
                    if len(collected_data) >= max_items:
                        break
                    
                    try:
                        self.stats['total_processed'] += 1
                        
                        # ì¤‘ë³µ ì²´í¬
                        post_id = self._extract_post_id(post_url)
                        if self.dedup_tracker.is_oppadu_post_collected(post_id):
                            logger.debug(f"ì´ë¯¸ ìˆ˜ì§‘ëœ ê²Œì‹œê¸€ ê±´ë„ˆëœ€: {post_id}")
                            self.stats['duplicate_posts'] += 1
                            continue
                        
                        # ìƒì„¸ ë°ì´í„° ìˆ˜ì§‘
                        qa_entry = await self._scrape_post_detail(post_url)
                        
                        if qa_entry:
                            collected_data.append(qa_entry)
                            self.stats['total_collected'] += 1
                            
                            # ìˆ˜ì§‘ëœ ê²Œì‹œê¸€ ì¶”ì 
                            self.dedup_tracker.mark_oppadu_post_collected(
                                post_id,
                                qa_entry.user_question,
                                post_url,
                                quality_score=qa_entry.metadata.get('quality_score', 0.0),
                                has_answer=bool(qa_entry.assistant_response),
                                metadata={'page': page_num, 'collection_date': datetime.now().isoformat()}
                            )
                            
                            logger.info(f"âœ… ê²Œì‹œê¸€ ìˆ˜ì§‘ ì™„ë£Œ: {qa_entry.user_question[:50]}...")
                        else:
                            self.stats['total_skipped'] += 1
                        
                        # ì¸ê°„ì ì¸ ì§€ì—°
                        crawling_config = self.config.get_crawling_config()
                        await self.anti_detection.human_delay(
                            crawling_config['human_delay_min'],
                            crawling_config['human_delay_max']
                        )
                        
                    except Exception as e:
                        logger.error(f"ê²Œì‹œê¸€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {post_url} - {e}")
                        self.stats['crawling_errors'] += 1
                        continue
                
                # í˜ì´ì§€ ê°„ ì§€ì—°
                crawling_config = self.config.get_crawling_config()
                await self.anti_detection.human_delay(
                    crawling_config['page_delay_min'],
                    crawling_config['page_delay_max']
                )
        
        except Exception as e:
            logger.error(f"Oppadu ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
            self.stats['crawling_errors'] += 1
        
        finally:
            if self.driver:
                self.driver.quit()
        
        logger.info(f"Oppadu ìˆ˜ì§‘ ì™„ë£Œ: {len(collected_data)}ê°œ í•­ëª©")
        return collected_data
    
    async def _get_all_page_urls(self) -> List[str]:
        """ëª¨ë“  í˜ì´ì§€ URL ëª©ë¡ ìˆ˜ì§‘ (class="oppadu-pagination" ì‚¬ìš©)"""
        try:
            # ì²« í˜ì´ì§€ì—ì„œ í˜ì´ì§€ë„¤ì´ì…˜ ì •ë³´ ìˆ˜ì§‘
            first_page_url = self.config.get_web_config()['community_url']
            
            # 1ì°¨ ì‹œë„: CloudScraper
            response = await self._fetch_with_cloudscraper(first_page_url)
            if not response:
                # 2ì°¨ ì‹œë„: Selenium
                response = await self._fetch_with_selenium_for_pagination(first_page_url)
            
            if not response:
                logger.warning("ì²« í˜ì´ì§€ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ")
                return [first_page_url]
            
            # í˜ì´ì§€ë„¤ì´ì…˜ íŒŒì‹±
            page_urls = self._parse_pagination(response)
            
            if not page_urls:
                # í˜ì´ì§€ë„¤ì´ì…˜ì´ ì—†ìœ¼ë©´ ì²« í˜ì´ì§€ë§Œ ë°˜í™˜
                return [first_page_url]
            
            logger.info(f"ì´ {len(page_urls)}ê°œ í˜ì´ì§€ URL ë°œê²¬")
            return page_urls
            
        except Exception as e:
            logger.error(f"í˜ì´ì§€ URL ëª©ë¡ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return [self.config.get_web_config()['community_url']]
    
    async def _get_answered_posts_from_url(self, page_url: str) -> List[str]:
        """íŠ¹ì • í˜ì´ì§€ URLì—ì„œ ë‹µë³€ ì™„ë£Œëœ ê²Œì‹œê¸€ URL ëª©ë¡ ìˆ˜ì§‘"""
        try:
            # 1ì°¨ ì‹œë„: CloudScraper
            response = await self._fetch_with_cloudscraper(page_url)
            if response:
                return self._parse_answered_posts(response)
            
            # 2ì°¨ ì‹œë„: Selenium (JavaScript ë Œë”ë§ í•„ìš”í•œ ê²½ìš°)
            return await self._fetch_with_selenium_for_posts(page_url)
            
        except Exception as e:
            logger.error(f"ë‹µë³€ ì™„ë£Œ ê²Œì‹œê¸€ ëª©ë¡ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []
    
    async def _fetch_with_cloudscraper(self, url: str) -> Optional[str]:
        """CloudScraperë¥¼ ì‚¬ìš©í•œ í˜ì´ì§€ ìˆ˜ì§‘"""
        try:
            # ìºì‹œ í™•ì¸
            cached_content = self.cache.get_cached_page(url)
            if cached_content:
                return cached_content
            
            # í—¤ë” ë¡œí…Œì´ì…˜
            self.anti_detection.rotate_headers()
            self.scraper.headers.update(self.anti_detection.session_headers)
            
            # Referer ì„¤ì • (ìì—°ìŠ¤ëŸ¬ìš´ íƒìƒ‰ ì‹œë®¬ë ˆì´ì…˜)
            if 'page=' in url:
                web_config = self.config.get_web_config()
                self.scraper.headers['Referer'] = web_config['community_url']
            
            response = self.scraper.get(url, timeout=30)
            response.raise_for_status()
            
            # ìºì‹œ ì €ì¥
            self.cache.cache_page(url, response.text)
            
            logger.debug(f"CloudScraper ì„±ê³µ: {url}")
            return response.text
            
        except Exception as e:
            logger.warning(f"CloudScraper ì‹¤íŒ¨: {e}")
            self.stats['blocked_requests'] += 1
            return None
    
    def _parse_pagination(self, html_content: str) -> List[str]:
        """HTMLì—ì„œ í˜ì´ì§€ë„¤ì´ì…˜ URL ëª©ë¡ ì¶”ì¶œ (class="oppadu-pagination")"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # class="oppadu-pagination" ì»¨í…Œì´ë„ˆ ì°¾ê¸°
            pagination = soup.find(class_='oppadu-pagination')
            if not pagination:
                logger.warning("oppadu-pagination ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return []
            
            page_urls = []
            web_config = self.config.get_web_config()
            
            # class="page-number" ê° í˜ì´ì§€ ë§í¬ ì¶”ì¶œ
            page_links = pagination.find_all(class_='page-number')
            
            for link in page_links:
                href = link.get('href')
                if href:
                    if href.startswith('?'):
                        page_url = web_config['community_url'] + href
                    elif href.startswith('/'):
                        page_url = web_config['base_url'] + href
                    else:
                        page_url = urljoin(web_config['community_url'], href)
                    
                    page_urls.append(page_url)
            
            # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
            unique_urls = list(dict.fromkeys(page_urls))  # ìˆœì„œ ìœ ì§€í•˜ë©´ì„œ ì¤‘ë³µ ì œê±°
            
            logger.debug(f"í˜ì´ì§€ë„¤ì´ì…˜ì—ì„œ {len(unique_urls)}ê°œ í˜ì´ì§€ URL ì¶”ì¶œ")
            return unique_urls
            
        except Exception as e:
            logger.error(f"í˜ì´ì§€ë„¤ì´ì…˜ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
            return []
    
    async def _fetch_with_selenium_for_pagination(self, url: str) -> Optional[str]:
        """Seleniumì„ ì‚¬ìš©í•œ í˜ì´ì§€ë„¤ì´ì…˜ ìˆ˜ì§‘"""
        try:
            if not self.driver:
                self._init_selenium_driver()
            
            self.driver.get(url)
            
            # í˜ì´ì§€ë„¤ì´ì…˜ ë¡œë”© ëŒ€ê¸°
            WebDriverWait(self.driver, 10).until(
                EC.any_of(
                    EC.presence_of_element_located((By.CLASS_NAME, "oppadu-pagination")),
                    EC.presence_of_element_located((By.CLASS_NAME, "post-list-modern"))
                )
            )
            
            await asyncio.sleep(2)
            page_source = self.driver.page_source
            
            # ìºì‹œ ì €ì¥
            self.cache.cache_page(url, page_source)
            
            return page_source
            
        except Exception as e:
            logger.error(f"Selenium í˜ì´ì§€ë„¤ì´ì…˜ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return None
    
    async def _fetch_with_selenium_for_posts(self, url: str) -> List[str]:
        """Seleniumì„ ì‚¬ìš©í•œ ê²Œì‹œê¸€ ëª©ë¡ ìˆ˜ì§‘"""
        try:
            if not self.driver:
                self._init_selenium_driver()
            
            self.driver.get(url)
            
            # ê²Œì‹œê¸€ ëª©ë¡ ë¡œë”© ëŒ€ê¸°
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.CLASS_NAME, "post-list-modern"))
            )
            
            # ìŠ¤í¬ë¡¤ ë‹¤ìš´ (ë ˆì´ì§€ ë¡œë”© ëŒ€ì‘)
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            await asyncio.sleep(2)
            
            page_source = self.driver.page_source
            
            # ìºì‹œ ì €ì¥
            self.cache.cache_page(url, page_source)
            
            return self._parse_answered_posts(page_source)
            
        except Exception as e:
            logger.error(f"Selenium ê²Œì‹œê¸€ ëª©ë¡ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            self.stats['blocked_requests'] += 1
            return []
    
    def _init_selenium_driver(self):
        """Selenium ë“œë¼ì´ë²„ ì´ˆê¸°í™”"""
        try:
            selenium_config = self.config.get_selenium_config()
            
            options = uc.ChromeOptions()
            for option in selenium_config['chrome_options']:
                options.add_argument(option)
            
            options.add_argument(f'--window-size={selenium_config["window_size"]}')
            options.add_argument('--user-agent=' + random.choice(self.anti_detection.user_agents))
            
            if selenium_config['headless_mode']:
                options.add_argument('--headless')
            
            # í•œêµ­ ì–¸ì–´ ì„¤ì •
            options.add_experimental_option('prefs', {
                'intl.accept_languages': 'ko-KR,ko,en'
            })
            
            self.driver = uc.Chrome(options=options)
            
        except Exception as e:
            logger.error(f"Selenium ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def _parse_answered_posts(self, html_content: str) -> List[str]:
        """HTMLì—ì„œ ë‹µë³€ ì™„ë£Œëœ ê²Œì‹œê¸€ URL ì¶”ì¶œ (ì‚¬ìš©ì ì œê³µ ì •í™•í•œ êµ¬ì¡°)"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # class="post-list-modern" ì»¨í…Œì´ë„ˆ ì°¾ê¸°
            post_list = soup.find(class_='post-list-modern')
            if not post_list:
                logger.warning("post-list-modern ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return []
            
            answered_posts = []
            
            # ê²Œì‹œë¬¼ ëª©ë¡ì—ì„œ ê° ê²Œì‹œë¬¼ í•­ëª© ì°¾ê¸°
            post_items = post_list.find_all('div', recursive=True)
            logger.debug(f"ì´ {len(post_items)}ê°œì˜ ê²Œì‹œê¸€ í•­ëª© ê²€ì‚¬")
            
            for item in post_items:
                # class="answer-complete-badge"ê°€ ìˆëŠ”ì§€ í™•ì¸
                answer_badge = item.find(class_='answer-complete-badge')
                if answer_badge:
                    # ê²Œì‹œê¸€ ë§í¬ ì¶”ì¶œ (ê°€ì¥ ê°€ê¹Œìš´ ë§í¬ ì°¾ê¸°)
                    link_element = (
                        item.find('a', href=True) or 
                        item.find_parent().find('a', href=True) if item.find_parent() else None
                    )
                    
                    if link_element:
                        href = link_element['href']
                        
                        # ì˜¬ë°”ë¥¸ URL êµ¬ì„±
                        web_config = self.config.get_web_config()
                        if href.startswith('?'):
                            post_url = web_config['community_url'] + href
                        elif href.startswith('/'):
                            post_url = web_config['base_url'] + href
                        else:
                            post_url = urljoin(web_config['community_url'], href)
                        
                        answered_posts.append(post_url)
                        logger.debug(f"ë‹µë³€ ì™„ë£Œ ê²Œì‹œê¸€ ë°œê²¬: {post_url}")
            
            logger.info(f"ë‹µë³€ ì™„ë£Œëœ ê²Œì‹œê¸€ {len(answered_posts)}ê°œ ë°œê²¬")
            return answered_posts
            
        except Exception as e:
            logger.error(f"HTML íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
            return []
    
    async def _scrape_post_detail(self, post_url: str) -> Optional[QAEntry]:
        """ê²Œì‹œê¸€ ìƒì„¸ í˜ì´ì§€ì—ì„œ ë°ì´í„° ì¶”ì¶œ"""
        try:
            # ìƒì„¸ í˜ì´ì§€ ìˆ˜ì§‘
            html_content = await self._fetch_with_cloudscraper(post_url)
            if not html_content and self.driver:
                self.driver.get(post_url)
                await asyncio.sleep(3)
                html_content = self.driver.page_source
            
            if not html_content:
                logger.error(f"ìƒì„¸ í˜ì´ì§€ ìˆ˜ì§‘ ì‹¤íŒ¨: {post_url}")
                return None
            
            return self._parse_post_detail(html_content, post_url)
            
        except Exception as e:
            logger.error(f"ê²Œì‹œê¸€ ìƒì„¸ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def _parse_post_detail(self, html_content: str, post_url: str) -> Optional[QAEntry]:
        """ê²Œì‹œê¸€ ìƒì„¸ í˜ì´ì§€ HTML íŒŒì‹± (ì‚¬ìš©ì ì œê³µ ì •í™•í•œ êµ¬ì¡°)"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ì œëª© ì¶”ì¶œ
            title_element = soup.find('h1') or soup.find(class_='post-title')
            title = title_element.get_text(strip=True) if title_element else ""
            
            # ì§ˆë¬¸ ë‚´ìš© ì¶”ì¶œ (class="post-content")
            post_content = soup.find(class_='post-content')
            question_text = ""
            question_images = []
            
            if post_content:
                # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                question_text = clean_text(post_content.get_text(strip=True, separator=' '))
                
                # ì´ë¯¸ì§€ ì¶”ì¶œ
                question_images = self._extract_images_from_element(post_content, post_url)
            
            # ë‹µë³€ ë‚´ìš© ì¶”ì¶œ (id="comment-list" ë‚´ class="selected-answer")
            answer_text = ""
            answer_images = []
            
            comment_list = soup.find(id='comment-list')
            if comment_list:
                selected_answer = comment_list.find(class_='selected-answer')
                if selected_answer:
                    # ë‹µë³€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    answer_text = clean_text(selected_answer.get_text(strip=True, separator=' '))
                    
                    # ë‹µë³€ ì´ë¯¸ì§€ ì¶”ì¶œ
                    answer_images = self._extract_images_from_element(selected_answer, post_url)
            
            # ìœ íš¨ì„± ê²€ì‚¬
            if not self._is_valid_post_content(title, question_text, answer_text):
                return None
            
            # ì´ë¯¸ì§€ í…ìŠ¤íŠ¸ í†µí•©
            all_question_text = question_text
            all_answer_text = answer_text
            
            if question_images:
                image_texts = [img.get('extracted_text', '') for img in question_images if img.get('extracted_text')]
                if image_texts:
                    all_question_text += ' [ì´ë¯¸ì§€ ë‚´ìš©: ' + ' '.join(image_texts) + ']'
            
            if answer_images:
                image_texts = [img.get('extracted_text', '') for img in answer_images if img.get('extracted_text')]
                if image_texts:
                    all_answer_text += ' [ì´ë¯¸ì§€ ë‚´ìš©: ' + ' '.join(image_texts) + ']'
            
            # ë©”íƒ€ë°ì´í„° ìƒì„±
            metadata = {
                'difficulty': self._estimate_difficulty(title, all_question_text),
                'functions': self._extract_excel_functions(all_answer_text),
                'quality_score': self._calculate_oppadu_quality_score(title, all_question_text, all_answer_text),
                'source': 'oppadu',
                'is_solved': bool(answer_text),
                'oppadu_metadata': {
                    'post_id': self._extract_post_id(post_url),
                    'url': post_url,
                    'title': title,
                    'has_answer': bool(answer_text),
                    'collection_date': datetime.now().isoformat(),
                    'question_images': question_images,
                    'answer_images': answer_images
                }
            }
            
            # í’ˆì§ˆ ì ìˆ˜ ê²€ì‚¬
            quality_config = self.config.get_quality_config()
            if metadata['quality_score'] < quality_config['min_quality_score']:
                self.stats['quality_failures'] += 1
                return None
            
            # ì½”ë“œ ë¸”ë¡ ì¶”ì¶œ
            code_blocks = extract_code_blocks(f"{all_question_text} {all_answer_text}")
            
            return QAEntry(
                id=generate_unique_id('oppadu_qa'),
                user_question=title,
                user_context=all_question_text,
                assistant_response=all_answer_text,
                code_blocks=code_blocks,
                metadata=metadata
            )
            
        except Exception as e:
            logger.error(f"ê²Œì‹œê¸€ ìƒì„¸ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def _extract_images_from_element(self, element, base_url: str) -> List[Dict[str, Any]]:
        """HTML ìš”ì†Œì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ ë° ì²˜ë¦¬"""
        try:
            images = []
            img_tags = element.find_all('img')
            
            for img in img_tags:
                src = img.get('src')
                if src:
                    # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                    if src.startswith('/'):
                        web_config = self.config.get_web_config()
                        image_url = web_config['base_url'] + src
                    elif src.startswith('http'):
                        image_url = src
                    else:
                        image_url = urljoin(base_url, src)
                    
                    # ì´ë¯¸ì§€ ì •ë³´ ìƒì„±
                    image_info = {
                        'url': image_url,
                        'alt_text': img.get('alt', ''),
                        'title': img.get('title', ''),
                        'width': img.get('width', ''),
                        'height': img.get('height', ''),
                        'extracted_text': ''  # OCR ì²˜ë¦¬ëŠ” í–¥í›„ êµ¬í˜„
                    }
                    
                    images.append(image_info)
            
            return images
            
        except Exception as e:
            logger.error(f"ì´ë¯¸ì§€ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            return []
    
    def _is_valid_post_content(self, title: str, question: str, answer: str) -> bool:
        """ê²Œì‹œë¬¼ ë‚´ìš© ìœ íš¨ì„± ê²€ì‚¬"""
        collection_config = self.config.get_collection_config()
        
        # ì œëª© ê¸¸ì´ ê²€ì‚¬
        if len(title) < collection_config['min_title_length']:
            return False
        
        # ì§ˆë¬¸ ê¸¸ì´ ê²€ì‚¬
        if len(question) < collection_config['min_content_length']:
            return False
        
        # ë‹µë³€ì´ ìˆëŠ”ì§€ ê²€ì‚¬
        if collection_config['only_answered_posts'] and not answer:
            return False
        
        # í‚¤ì›Œë“œ ê²€ì‚¬
        combined_text = f"{title} {question} {answer}".lower()
        
        # í•„ìˆ˜ í‚¤ì›Œë“œ ê²€ì‚¬
        if collection_config['required_keywords']:
            if not any(keyword in combined_text for keyword in collection_config['required_keywords']):
                return False
        
        # ì œì™¸ í‚¤ì›Œë“œ ê²€ì‚¬
        if collection_config['excluded_keywords']:
            if any(keyword in combined_text for keyword in collection_config['excluded_keywords']):
                return False
        
        return True
    
    def _estimate_difficulty(self, title: str, content: str) -> str:
        """ë‚œì´ë„ ì¶”ì •"""
        combined_text = f"{title} {content}".lower()
        
        # ê³ ê¸‰ í‚¤ì›Œë“œ (í•œêµ­ì–´)
        advanced_keywords = ['vba', 'ë§¤í¬ë¡œ', 'í”¼ë²—', 'ë°°ì—´', 'ìˆ˜ì‹', 'í•¨ìˆ˜', 'ì¡°ê±´ë¶€ì„œì‹']
        advanced_count = sum(1 for keyword in advanced_keywords if keyword in combined_text)
        
        if advanced_count >= 2:
            return 'advanced'
        elif advanced_count >= 1:
            return 'intermediate'
        else:
            return 'beginner'
    
    def _extract_excel_functions(self, text: str) -> List[str]:
        """Excel í•¨ìˆ˜ ì¶”ì¶œ (í•œêµ­ì–´ í¬í•¨)"""
        functions = []
        
        # ì˜ì–´ í•¨ìˆ˜ íŒ¨í„´
        function_pattern = r'([A-Z][A-Z0-9_]*)\s*\('
        matches = re.findall(function_pattern, text.upper())
        functions.extend(matches)
        
        # í•œêµ­ì–´ í•¨ìˆ˜ëª… ë§¤í•‘
        korean_functions = {
            'í•©ê³„': 'SUM',
            'í‰ê· ': 'AVERAGE',
            'ê°œìˆ˜': 'COUNT',
            'ìµœëŒ€': 'MAX',
            'ìµœì†Œ': 'MIN',
            'ì¡°ê±´': 'IF',
            'ì°¾ê¸°': 'VLOOKUP',
            'ì—°ê²°': 'CONCATENATE'
        }
        
        for korean, english in korean_functions.items():
            if korean in text:
                functions.append(english)
        
        return list(set(functions))
    
    def _calculate_oppadu_quality_score(self, title: str, question: str, answer: str) -> float:
        """Oppadu í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        quality_config = self.config.get_quality_config()
        
        score = 5.0  # ê¸°ë³¸ ì ìˆ˜
        
        # ì œëª© ê¸¸ì´ ì ìˆ˜
        if len(title) >= 20:
            score += 0.5
        
        # ì§ˆë¬¸ ê¸¸ì´ ì ìˆ˜
        question_length = len(question)
        if question_length >= 100:
            score += 1.0
        elif question_length >= 50:
            score += 0.5
        
        # ë‹µë³€ ê¸¸ì´ ì ìˆ˜
        answer_length = len(answer)
        if answer_length >= 100:
            score += 1.5
        elif answer_length >= 50:
            score += 1.0
        
        # í•œêµ­ì–´ ì½˜í…ì¸  ê°€ì¤‘ì¹˜
        korean_content_weight = quality_config['korean_content_weight']
        score *= korean_content_weight
        
        # ì½”ë“œë‚˜ ìˆ˜ì‹ í¬í•¨ ì‹œ ê°€ì‚°ì 
        if self._contains_excel_content(f"{question} {answer}"):
            score += 1.0
        
        return min(score, 10.0)  # ìµœëŒ€ 10ì 
    
    def _contains_excel_content(self, text: str) -> bool:
        """Excel ê´€ë ¨ ë‚´ìš© í¬í•¨ ì—¬ë¶€ í™•ì¸"""
        excel_indicators = [
            '=', 'SUM', 'IF', 'VLOOKUP', 'INDEX', 'MATCH',
            'í•©ê³„', 'í‰ê· ', 'ê°œìˆ˜', 'ì¡°ê±´', 'ì°¾ê¸°', 'ìˆ˜ì‹', 'í•¨ìˆ˜',
            'ì…€', 'ì›Œí¬ì‹œíŠ¸', 'ì—‘ì…€', 'í”¼ë²—'
        ]
        
        text_upper = text.upper()
        return any(indicator.upper() in text_upper for indicator in excel_indicators)
    
    def _extract_post_id(self, url: str) -> str:
        """URLì—ì„œ ê²Œì‹œê¸€ ID ì¶”ì¶œ"""
        try:
            # uid íŒŒë¼ë¯¸í„° ì¶”ì¶œ
            if 'uid=' in url:
                uid_match = re.search(r'uid=(\d+)', url)
                if uid_match:
                    return uid_match.group(1)
            
            # URL ê²½ë¡œì—ì„œ ID ì¶”ì¶œ
            parts = url.split('/')
            for part in reversed(parts):
                if part.isdigit():
                    return part
            
            # ìˆ«ì IDê°€ ì—†ìœ¼ë©´ URL í•´ì‹œ ì‚¬ìš©
            return str(hash(url))
        except:
            return str(hash(url))
    
    def get_collection_stats(self) -> CollectionStats:
        """ìˆ˜ì§‘ í†µê³„ ë°˜í™˜"""
        return CollectionStats(
            source='oppadu',
            total_collected=self.stats['total_collected'],
            total_skipped=self.stats['total_skipped'],
            collection_time_seconds=0.0,  # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì‹œê°„ ì¸¡ì •
            quality_score_avg=0.0,  # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” í‰ê·  ê³„ì‚°
            errors_count=self.stats['crawling_errors']
        )
    
    def get_detailed_stats(self) -> Dict[str, Any]:
        """ìƒì„¸ í†µê³„ ë°˜í™˜"""
        return {
            **self.stats,
            'dedup_stats': self.dedup_tracker.get_oppadu_stats(),
            'cache_stats': self.cache.cache.get_stats()
        }