#!/usr/bin/env python3
"""
API í•œë„ ë° ìˆ˜ì§‘ ì¤‘ë‹¨ ì›ì¸ ë¶„ì„ ë„êµ¬
"""
import asyncio
import logging
from datetime import datetime, timedelta
from pathlib import Path
import sys
import json

# Add project root to path
sys.path.append(str(Path(__file__).parent))

from collectors.stackoverflow_collector import StackOverflowCollector
from collectors.reddit_collector import RedditCollector
from core.cache import APICache, LocalCache
from core.dedup_tracker import get_global_tracker

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

async def analyze_stackoverflow_limits():
    """Stack Overflow API í•œë„ ë¶„ì„"""
    
    logger.info("ğŸ“Š Stack Overflow API í•œë„ ë¶„ì„")
    logger.info("=" * 70)
    
    # Cache ì´ˆê¸°í™”
    local_cache = LocalCache(db_path=Path("/tmp/so_limit_analysis.db"))
    cache = APICache(local_cache)
    
    so_collector = StackOverflowCollector(cache)
    
    # í˜„ì¬ rate limiting ìƒíƒœ í™•ì¸
    logger.info(f"   ğŸ“ˆ í˜„ì¬ ìš”ì²­ ìˆ˜: {so_collector.requests_today}")
    logger.info(f"   ğŸ“… ì¼ì¼ í• ë‹¹ëŸ‰ ë¦¬ì…‹: {so_collector.daily_quota_reset}")
    logger.info(f"   â° ë§ˆì§€ë§‰ ìš”ì²­ ì‹œê°„: {so_collector.last_request_time}")
    
    # API í•œë„ ì„¤ì • í™•ì¸
    rate_config = so_collector.rate_config
    logger.info(f"   ğŸ”„ ë¶„ë‹¹ ìš”ì²­ ì œí•œ: {rate_config['requests_per_minute']}")
    logger.info(f"   ğŸ“Š ì¼ì¼ ìš”ì²­ ì œí•œ: {rate_config['max_requests_per_day']}")
    
    # ì‹¤ì œ API í…ŒìŠ¤íŠ¸ (1í˜ì´ì§€ë§Œ)
    try:
        from_date = datetime.now() - timedelta(days=1)  # ìµœê·¼ 1ì¼ë§Œ
        
        logger.info(f"   ğŸ§ª API í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        test_data = await so_collector.collect_excel_questions(
            from_date=from_date,
            max_pages=1  # 1í˜ì´ì§€ë§Œ í…ŒìŠ¤íŠ¸
        )
        
        logger.info(f"   âœ… API í…ŒìŠ¤íŠ¸ ì„±ê³µ: {len(test_data)}ê°œ ìˆ˜ì§‘")
        logger.info(f"   ğŸ“ˆ í…ŒìŠ¤íŠ¸ í›„ ìš”ì²­ ìˆ˜: {so_collector.requests_today}")
        
        return {
            'api_working': True,
            'requests_today': so_collector.requests_today,
            'test_collection_count': len(test_data),
            'daily_limit': rate_config['max_requests_per_day'],
            'per_minute_limit': rate_config['requests_per_minute']
        }
        
    except Exception as e:
        logger.error(f"   âŒ API í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        # Rate limit ì—ëŸ¬ì¸ì§€ í™•ì¸
        if "rate limit" in str(e).lower() or "quota" in str(e).lower():
            logger.warning("   ğŸš« Rate limitì— ë„ë‹¬í•œ ê²ƒìœ¼ë¡œ ë³´ì„")
            return {
                'api_working': False,
                'error_type': 'rate_limit',
                'error_message': str(e),
                'requests_today': so_collector.requests_today
            }
        else:
            return {
                'api_working': False,
                'error_type': 'other',
                'error_message': str(e)
            }

async def analyze_reddit_limits():
    """Reddit API í•œë„ ë¶„ì„"""
    
    logger.info("\nğŸŸ  Reddit API í•œë„ ë¶„ì„")
    logger.info("=" * 70)
    
    # Cache ì´ˆê¸°í™”
    local_cache = LocalCache(db_path=Path("/tmp/reddit_limit_analysis.db"))
    cache = APICache(local_cache)
    
    reddit_collector = RedditCollector(cache)
    
    # Reddit API ì •ë³´
    logger.info(f"   ğŸ¤– User Agent: {reddit_collector.reddit.config.user_agent}")
    logger.info(f"   ğŸ“Š í˜„ì¬ ìš”ì²­ ìˆ˜: {reddit_collector.requests_today}")
    logger.info(f"   â° ë§ˆì§€ë§‰ ìš”ì²­ ì‹œê°„: {reddit_collector.last_request_time}")
    
    # Reddit API í…ŒìŠ¤íŠ¸
    try:
        logger.info(f"   ğŸ§ª Reddit API í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        # ê°„ë‹¨í•œ subreddit í™•ì¸
        subreddit = reddit_collector.reddit.subreddit('excel')
        logger.info(f"   ğŸ“ Subreddit: {subreddit.display_name}")
        logger.info(f"   ğŸ‘¥ êµ¬ë…ì: {subreddit.subscribers:,}")
        
        # ìµœê·¼ 'solved' í¬ìŠ¤íŠ¸ ëª‡ ê°œë§Œ í™•ì¸
        solved_count = 0
        total_checked = 0
        
        for submission in subreddit.new(limit=50):  # 50ê°œë§Œ í™•ì¸
            total_checked += 1
            flair = submission.link_flair_text or "No Flair"
            
            if flair.lower() == 'solved':
                solved_count += 1
                if solved_count >= 5:  # 5ê°œë§Œ ì°¾ìœ¼ë©´ ì¤‘ë‹¨
                    break
        
        logger.info(f"   âœ… Reddit API í…ŒìŠ¤íŠ¸ ì„±ê³µ")
        logger.info(f"   ğŸ“Š í™•ì¸í•œ í¬ìŠ¤íŠ¸: {total_checked}ê°œ")
        logger.info(f"   ğŸ¯ 'solved' í¬ìŠ¤íŠ¸: {solved_count}ê°œ")
        
        return {
            'api_working': True,
            'subscribers': subreddit.subscribers,
            'solved_posts_found': solved_count,
            'posts_checked': total_checked,
            'solved_ratio': solved_count / total_checked if total_checked > 0 else 0
        }
        
    except Exception as e:
        logger.error(f"   âŒ Reddit API í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        # Rate limit ì—ëŸ¬ì¸ì§€ í™•ì¸
        if "rate limit" in str(e).lower() or "429" in str(e):
            logger.warning("   ğŸš« Reddit Rate limitì— ë„ë‹¬í•œ ê²ƒìœ¼ë¡œ ë³´ì„")
            return {
                'api_working': False,
                'error_type': 'rate_limit',
                'error_message': str(e)
            }
        else:
            return {
                'api_working': False,
                'error_type': 'other',
                'error_message': str(e)
            }

async def analyze_collection_patterns():
    """ìˆ˜ì§‘ íŒ¨í„´ ë¶„ì„"""
    
    logger.info("\nğŸ“ˆ ìˆ˜ì§‘ íŒ¨í„´ ë¶„ì„")
    logger.info("=" * 70)
    
    # ì¤‘ë³µ ì¶”ì ê¸°ì—ì„œ í†µê³„ í™•ì¸
    tracker = get_global_tracker()
    stats = tracker.get_collection_stats(days=1)  # ìµœê·¼ 1ì¼
    
    logger.info("   ğŸ“Š ìµœê·¼ ìˆ˜ì§‘ í†µê³„:")
    if stats:
        so_stats = stats.get('stackoverflow', {})
        reddit_stats = stats.get('reddit', {})
        
        logger.info(f"      Stack Overflow: {so_stats.get('total_collected', 0)}ê°œ")
        logger.info(f"      Reddit: {reddit_stats.get('total_collected', 0)}ê°œ")
        
        if so_stats.get('last_collected'):
            logger.info(f"      ë§ˆì§€ë§‰ SO ìˆ˜ì§‘: {so_stats['last_collected']}")
        if reddit_stats.get('last_collected'):
            logger.info(f"      ë§ˆì§€ë§‰ Reddit ìˆ˜ì§‘: {reddit_stats['last_collected']}")
    else:
        logger.info("      í†µê³„ ë°ì´í„° ì—†ìŒ")
    
    # í˜„ì¬ ìˆ˜ì§‘ëœ íŒŒì¼ í™•ì¸
    output_file = Path("/Users/kevin/bigdata/data/output/year=2025/month=07/day=18/combined_20250718.jsonl")
    
    if output_file.exists():
        with open(output_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        logger.info(f"   ğŸ“„ í˜„ì¬ ì¶œë ¥ íŒŒì¼: {len(lines)}ê°œ í•­ëª©")
        
        # ì†ŒìŠ¤ë³„ ë¶„ì„
        stackoverflow_count = 0
        reddit_count = 0
        
        for line in lines:
            try:
                data = json.loads(line.strip())
                source = data.get('metadata', {}).get('source', 'unknown')
                if source == 'stackoverflow':
                    stackoverflow_count += 1
                elif source == 'reddit':
                    reddit_count += 1
            except:
                continue
        
        logger.info(f"      - Stack Overflow: {stackoverflow_count}ê°œ")
        logger.info(f"      - Reddit: {reddit_count}ê°œ")
        
        return {
            'total_items': len(lines),
            'stackoverflow_items': stackoverflow_count,
            'reddit_items': reddit_count
        }
    else:
        logger.info("   ğŸ“„ ì¶œë ¥ íŒŒì¼ ì—†ìŒ")
        return {'total_items': 0}

async def suggest_api_solutions():
    """API í•œë„ í•´ê²°ì±… ì œì•ˆ"""
    
    logger.info("\nğŸ’¡ API í•œë„ í•´ê²°ì±…")
    logger.info("=" * 70)
    
    logger.info("1. ğŸ“Š Stack Overflow API ìµœì í™”:")
    logger.info("   â€¢ í˜ì´ì§€ í¬ê¸° ê°ì†Œ: 100 â†’ 50ê°œ")
    logger.info("   â€¢ ìˆ˜ì§‘ ê°„ê²© ì¦ê°€: 2ì´ˆ â†’ 5ì´ˆ")
    logger.info("   â€¢ ì¼ì¼ í• ë‹¹ëŸ‰ ëª¨ë‹ˆí„°ë§ ê°•í™”")
    
    logger.info("\n2. ğŸŸ  Reddit API ìµœì í™”:")
    logger.info("   â€¢ PRAW ëŒ€ì‹  Async PRAW ì‚¬ìš© ê³ ë ¤")
    logger.info("   â€¢ ìš”ì²­ ê°„ê²© ì¦ê°€")
    logger.info("   â€¢ User-Agent ìµœì í™”")
    
    logger.info("\n3. ğŸ”„ ìˆ˜ì§‘ ì „ëµ ë³€ê²½:")
    logger.info("   â€¢ ë°°ì¹˜ ìˆ˜ì§‘ â†’ ì ì§„ì  ìˆ˜ì§‘")
    logger.info("   â€¢ ì‹œê°„ëŒ€ë³„ ë¶„ì‚° ìˆ˜ì§‘")
    logger.info("   â€¢ ìºì‹œ í™œìš© ê·¹ëŒ€í™”")
    
    logger.info("\n4. âš™ï¸  ì„¤ì • ì¡°ì •:")
    logger.info("   â€¢ max_pages: 50 â†’ 20")
    logger.info("   â€¢ target_count: 100 â†’ 50")
    logger.info("   â€¢ rate_limit_delay: 2ì´ˆ â†’ 5ì´ˆ")

async def main():
    """ë©”ì¸ ë¶„ì„ ë£¨í‹´"""
    logger.info("ğŸ” API í•œë„ ë° ìˆ˜ì§‘ ì¤‘ë‹¨ ì›ì¸ ë¶„ì„")
    logger.info("=" * 80)
    
    # Stack Overflow ë¶„ì„
    so_analysis = await analyze_stackoverflow_limits()
    
    # Reddit ë¶„ì„
    reddit_analysis = await analyze_reddit_limits()
    
    # ìˆ˜ì§‘ íŒ¨í„´ ë¶„ì„
    collection_analysis = await analyze_collection_patterns()
    
    # í•´ê²°ì±… ì œì•ˆ
    await suggest_api_solutions()
    
    # ìµœì¢… ì§„ë‹¨
    logger.info("\n" + "ğŸ¯" * 30)
    logger.info("ì§„ë‹¨ ê²°ê³¼:")
    
    if not so_analysis.get('api_working', True):
        logger.warning("âŒ Stack Overflow API ë¬¸ì œ ê°ì§€")
        if so_analysis.get('error_type') == 'rate_limit':
            logger.warning("   ì›ì¸: Rate limit ì´ˆê³¼")
        else:
            logger.warning(f"   ì›ì¸: {so_analysis.get('error_message', 'Unknown')}")
    else:
        logger.info("âœ… Stack Overflow API ì •ìƒ")
    
    if not reddit_analysis.get('api_working', True):
        logger.warning("âŒ Reddit API ë¬¸ì œ ê°ì§€")
        if reddit_analysis.get('error_type') == 'rate_limit':
            logger.warning("   ì›ì¸: Rate limit ì´ˆê³¼")
        else:
            logger.warning(f"   ì›ì¸: {reddit_analysis.get('error_message', 'Unknown')}")
    else:
        logger.info("âœ… Reddit API ì •ìƒ")
    
    total_items = collection_analysis.get('total_items', 0)
    if total_items < 20:
        logger.warning(f"âš ï¸  ìˆ˜ì§‘ëŸ‰ ë¶€ì¡±: {total_items}ê°œë§Œ ìˆ˜ì§‘ë¨")
        logger.info("   ê¶Œì¥ì‚¬í•­: API í•œë„ ì„¤ì • ì¡°ì • í•„ìš”")
    else:
        logger.info(f"âœ… ìˆ˜ì§‘ëŸ‰ ì ì ˆ: {total_items}ê°œ")

if __name__ == "__main__":
    asyncio.run(main())